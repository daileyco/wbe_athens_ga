---
title: "Biostatistical Consulting  \n Wastewater-based Epidemiology of COVID-19 in Athens,
  GA, USA 2020-2021"
author: 
  - Cody Dailey [^epibios] & Megan Lott [^ehsc]
  - Erin Lipp ^2^ & Stephen Rathbun ^1^

link-citations: yes
date: '`r format(Sys.Date(), "%d %B %Y")`'
output: 
  bookdown::word_document2:
    
    toc: true
    toc_depth: 2
    number_sections: true
    
bibliography: "../00-references/references.bib"
biblio-style: apalike
csl: "../00-references/epidemiology.csl"
nocite: "@*"

editor_options: 
  chunk_output_type: console
---

[^epibios]:Department of Epidemiology and Biostatistics, University of Georgia, Athens, GA, USA
[^ehsc]:Department of Environmental Health, University of Georgia, Athens, GA, USA

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, fig.width = 6, fig.height = 6*9/16, out.width = "100%", out.height = "90%", fig.topcaption = TRUE)
```


\newpage
# Preface {-}

This document will serve as a report among collaborators working on the analysis of wastewater surveillance data as part of a biostatistical consulting course (BIOS8200) instructed by Dr. Stephen Rathbun. The wastewater surveillance project was designed and implemented by Dr. Erin Lipp and her environmental health doctoral students, Megan Lott and William Norfolk, and other members of the Lipp lab. Megan Lott acts as the primary "client" for correspondence with lead "consultant" Cody Dailey, under the supervision of Dr. Stephen Rathbun and with meaningful discussion among other course consultants: Nicholas Mallis, Amanda Skarlupka, Morgan Taylor, and Adrianna Westbrook.
  
The report is structured to highlight the analytic workflow from raw data management through analysis techniques. Particular attention is given to analytic options and decision-making to expand on reproducibility. This report does *not* mimic the structure found in scientific manuscripts. Rather and analogously, it will be more detailed and comprehensive *methods* and *results* sections with commentary and coding descriptions. 
  


\newpage
## Common Abbreviations {-}
SARS-CoV-2 = severe acute respiratory syndrome coronavirus 2 (a virus)  
COVID-19 = coronavirus disease 2019 (a disease)  
RT-qPCR = reverse transcription quantitative polymerase chain reaction  
LOD = limit of detection    
LOQ = limit of quantification    

  
  
  
  
  
  
  
  
  
  
\newpage
# Background
## SARS-CoV-2 and COVID-19 Pandemic
  
Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the etiological agent of coronavirus disease 2019 (COVID-19). This virus stormed the modern world with a global pandemic that has imposed a gobsmacking burden on society. Direct impacts of deaths, disease, and disability have been met with indirect effects in societal upheaval and economic stress. 
  
Governments, public health professionals, and academics alike were neither expecting of nor prepared for a global emergency of this magnitude. Consequently, there was an obvious dearth of public health and medical capacity. Important to our context, a valid and reliable surveillance system and methodology was missing. We initially had no sound approaches to track the extent of the pandemic, that is, how many and who were infected.     
    
Even as diagnostic tests were developed and anthropocentric surveillance of cases yielded situation reports, there remained obscurity in the underlying pandemic processes. At best, infections are only partially observed (and reported) and potential biases from diagnostic accuracy and human behavior loom over our understanding of outbreaks as they happen. Retrospective serological studies can address some of the lapses in surveillance but do little to address concerns during an ongoing outbreak. 

Dr. Erin Lipp and peers devised ways to monitor the pandemic from another perspective. 
  
## Wastewater-based Epidemiological Surveillance
  
When infected with pathogenic organisms, people will often and unknowingly release or shed the microbes into their environment. This is fundamental to understanding the transmission of communicable infectious disease (e.g., aerosol spread from a cough or sneeze). However, pathogen shedding is not limited to the more dominant / obvious modes of transmission. 
  
Fecal shedding is a well-known and relatively common characteristic of viral infections and serves as the base rationale for Dr. Lipp's research into SARS-CoV-2 and COVID-19 surveillance in wastewater. **Compared to traditional case reporting systems, the detection and quantification of SARS-CoV-2 ribonucleic acid (RNA) in wastewater offers a more timely and comprehensive assessment of the extent of an outbreak.** Ultimately in this analysis, we aim to investigate the veracity of this statement. 
  
## Objectives
  
With this analysis, we hope to characterize the capabilities of wastewater-based epidemiological surveillance for SARS-CoV-2. In doing so, we outline the following objectives:
  
1. Explore viral load quantification methodologies
2. Address sources of bias in COVID-19 case reports
3. Develop a predictive framework using wastewater sampling to inform COVID-19 surveillance

## Document Structure / Flow
  
The rest of this document will be organized in the following way (with terrible puns throughout). First in Section \@ref(overview), I give a brief overview of the study design with respect to the collection of wastewater samples and the subsequent analysis. In Section \@ref(rawdata), I describe the data to be used in the analysis. Section \@ref(missingness) outlines the sampling frequencies with resulting observations, quantifies the extent of data missingness, or rather null results, from the wastewater samples' analyses, and explores limits of detection and quantification. Section \@ref(replacement) covers the calibrations of reaction efficiencies, conversions from assay results to viral loads, and missing data replacements using estimated limits. Section \@ref(deconvolution) switches focus to COVID-19 incidence and estimation of a deconvoluted incidence curve from a time series of case counts by dates of report. Finally, Section \@ref(correlation) explores the associations between wastewater assays and COVID-19 case counts and works towards a prediction framework for using wastewater-based epidemiology to inform surveillance. Then, Section \@ref(conclusions) discusses the entirety of the project, results, implications, and future directions. 

All analyses were carried out using R (`r gsub("R ", "", version$version.string)`, `r version$nickname`; @R-base) and RStudio (Windows 10 Desktop Version 1.4.1106, Tiger Daylily; @rstudio). This document was prepared using the `bookdown` [@R-bookdown] and `rmarkdown` [@R-rmarkdown] packages within the RStudio IDE. 

















\newpage
# From Feces to Species {#overview}
  
  
```{r, echo=F, results='hide', message=F, warning=F, error=F}
my.packages <- c("dplyr", "readr", "readxl", "magrittr", "knitr", "openxlsx", "flextable", "tidyr")

# knitr::write_bib(c(.packages(), my.packages, "zoo", "dLagM", 'rmarkdown', 'bookdown', 'incidental'), file = './consult/00-references/software/packages.bib')

knitr::write_bib(c(.packages(), my.packages, "zoo", "dLagM", 'rmarkdown', 'bookdown', 'incidental'), file = './consult/00-references/software/packages.bib')

sapply(my.packages, library, character.only=T)
rm(my.packages)
```


## Overview of Wastewater Sampling, Processing, and Analysis with Resulting Data

As part of the wastewater surveillance, three Athens-Clarke County (ACC) water reclamation facilities yield water samples. The sampling frequency varies across the study. Initially, samples were taken weekly, but later the sampling frequency was twice weekly (on Mondays and Wednesdays). However, there remained some irregularity in the sampling frequency (further discussed in Section \@ref(missingness)). 
      
Overall, the *three* wastewater reclamation facilities yield approximately *three* 24-hour composite water samples for a single sampling time. The three biological replicates each serve as a source for RT-qPCR experiments. The experiments use amplification primers for *two* viral sequence targets, N1 and N2, and for each primer *three* replicate experiments are conducted. So, for any given sampling time, there should be approximately 3x3x2x3=54 RT-qPCR results.
      
Initially, two water samples for each facility were taken at each sampling time, but the study design shifted to collecting three samples for most of the study duration; these redundant samples are referred to as biological replicates.     
      
Each wastewater sample was processed and analyzed via a reverse transcription quantitative polymerase chain reaction (RT-qPCR) -based laboratory workflow required for the enumeration of SARS-CoV-2 in the wastewater samples. Briefly, viral genetic material (i.e., ribonucleic acid (RNA)) was extracted from the wastewater samples and used as a template to generate complementary deoxyribonucleic acid (DNA) which was then serially amplified to determine the concentration of viral genetic material from the original sample. This RT-qPCR workflow is done both in duplicate for two separate genetic sequence targets/primers (N1 and N2) and, for each sequence target, in triplicate (at least) for additional redundancies; these triplicate experiments are referred to as technical replicates.     
      
More comprehensive documentation on data and methodology is found at the University of Georgia (UGA) Center for the Ecology of Infectious Disease (CEID) [COVID-19 Portal: Wastewater Surveillance for SARS-CoV-2 in Athens, GA](https://www.covid19.uga.edu/wastewater-athens.html) and at the [Lipp Laboratory Protocol for Wastewater Surveillance of SARS-CoV-2](https://docs.google.com/document/d/1vvRGrvR5iF3P6d40tKPjUpOZBBKGz_OZAr9Z5xJiCKw/edit). 
    


























\newpage  
# Raw (sewage) Data {#rawdata}

This section outlines the datasets used in these analyses. 
  
## RT-qPCR Generated Data
  
Megan Lott conducts the RT-qPCR procedures and aggregates raw data output using Microsoft Excel workbooks. These data contain unique identifiers for biological and technical replicates, dates of water sampling and RT-qPCR runs, and the cycle thresholds (ct; i.e., the number of PCR cycles that were needed to detect the fluorescent markers incorporated into the amplified DNA).

Data from RT-qPCR runs using sequence primers for N1 and N2 were imported into R as follows:
  
  
```{r data, echo=T, message=F, warning=F, error=F}
n1 <- read_csv("./data/raw_data/n1_all_cleaned2.csv")
n2 <- read_csv("./data/raw_data/n2_all_cleaned2.csv")
```
  
    
## Reaction Calibration Data
  
In addition to the RT-qPCR runs of the water samples with unknown viral loads, positive controls are run as a means by which to calibrate the conversion equations for the RT-qPCRs. That is, water samples are "spiked" with known concentrations of viral sequence targets (N1 or N2) or a related sequence (orthologous or paralogous?) such as Bovine Coronavirus genes (not included in the current analyses). These data are needed to estimate reaction efficiency and in the calculations that convert RT-qPCR output Ct values to estimations of viral load (e.g., number of viral genetic copies per volume of water sample)
  
The following two lines of code were used to read in the quality-control data required for the standard curves:
  
```{r data2, echo=T, message=F, warning=F, error=F}
qc <- read_csv("./data/raw_data/QC/all_curves.csv")
qc2 <- read_xlsx("./data/raw_data/QC/sarscov2_rna_control.xlsx")
```
    
   
## Wastewater Reclamation Facilities
  
The wastewater reclamation facilities that serve as sources for samples provide data on total influent water / sewage volume (i.e., how much water flows through the facility) and total suspended solids. These characteristics may impact both the water sample and the downstream analysis.

The following line of code was used to import the facility data required in the conversion of viral load concentrations to an absolute count of viral copies in the wastewater:

```{r data3, echo=T, message=F, warning=F, error=F}
plant <- read_csv("./data/raw_data/plant_data.csv")
```



## COVID-19 Surveillance Reports
  
The purpose of these efforts to quantify the viral load in wastewater is to complement the underlying SARS-CoV-2 infections or COVID-19 epidemic curve. As such, we will also incorporate the COVID-19 case reports from the Georgia Department of Public Health (GaDPH). The data are available as two datasets of COVID-19 case reports. These two datasets are distinguished subtly based on how any particular case report is tied to a specific date. The first dataset contains simple COVID-19 case frequencies for the dates at which the cases were reported. The second dataset contains COVID-19 case frequencies corresponding to the date of symptom onset; however, if a date of symptom onset is not available for a record, then the date is replaced with the date of sample specimen collection. 
  
Although the symptom onset dataset is imperfect, it is likely a useful addition to the analysis at hand. Viral shedding in feces occurs post symptom onset, but whether it occurs before symptom onset is unclear; however, some have noted similar viral loads and shedding in SARS-CoV-2 infections, regardless of symptomaticity. Still, it may be necessary to explore relations in the data with respect to the timing of cases in the population and viral loads in wastewater. 

In addition, we have data on diagnostic test frequencies and positivity that may shed additional light onto the hidden processes. These data are also given for two date schema, report and specimen collection dates. 


The following lines of code read in these data, subset to the records for Athens-Clarke County, and extract the variables pertinent to our analyses:

```{r data4, echo=T, message=F, warning=F, error=F}
covid <- read_csv("./consult/01-data/ga_covid_data/epicurve_symptom_date.csv") %>% 
            filter(county=="Clarke") %>% 
            select(symptom.date=`symptom date`, 
                   cases, moving_avg_cases)

covid.report <- read_csv("./consult/01-data/ga_covid_data/epicurve_rpt_date.csv") %>% 
            filter(county=="Clarke") %>% 
            select(report_date, 
                   cases, 
                   moving_avg_cases)

covid.testing <- read_csv("./consult/01-data/ga_covid_data/pcr_positives_col.csv") %>% 
            filter(county=="Clarke") %>% 
            select(collection_date = collection_dt, 
                   pcr_tests = `ALL PCR tests performed`, 
                   pcr_pos = `All PCR positive tests`)

``` 


Appendix \@ref(descriptA) contains a more detailed description of the raw data used in the analyses, the contents of the data, and, briefly, their management. 





















\newpage
# Plunging into Data Interrogation {#missingness}
  
In this section, I describe the sampling frequencies employed and the resulting number of records from wastewater processing, explore the extent and patterns of *missingness* within the wastewater samples data, and approach estimations of detection and quantification limits for the RT-qPCR analyses. 
  
```{r manage, echo=F, message=F, warning=F, error=F}
.wbe_old <- bind_rows(n1, n2) %>% 
    mutate(
        run_date=as.Date(run_date, format = "%d-%b-%y"), 
        sample_date=as.Date(sample_date, format = "%d-%b-%y"), 
        facility=substr(sample_id, 1,2), 
        biological_replicate=substr(sample_id, nchar(sample_id), nchar(sample_id)), 
        ct=as.numeric(ifelse(ct=="Undetermined", NA, ct))
    ) %>% 
    arrange(sample_date, facility, target, biological_replicate)


wbe <- bind_rows(n1, n2) %>% 
          mutate(
            run_date=as.Date(run_date, format = "%d-%b-%y"), 
            sample_date=as.Date(sample_date, format = "%d-%b-%y"), 
            facility=substr(sample_id, 1,2), 
            biological_replicate=substr(sample_id, nchar(sample_id), nchar(sample_id)), 
            ct=as.numeric(ifelse(ct=="Undetermined", NA, ct))
            ) %>% 
          arrange(sample_date, facility, target, biological_replicate) %>% 
          select(sample_date, facility, target, biological_replicate, target, collection_num, run_date, run_num, ct)

rm(n1, n2)

qc <- bind_rows(
  qc %>% 
    select(-log_quant) %>% 
    mutate(df="qc1"), 
  qc2 %>% 
    mutate(quantity = 1e8*3/25*2/20, df="qc2") %>% 
    rename(ct=ct_value)
  )
rm(qc2)

plant %<>% mutate(date = as.Date(date, format = "%m/%d/%Y"), influent_flow_L = influent_flow_mg*1e6*231*(0.0254^3)*1000) %>% select(date, wrf, influent_flow_L, influent_tss_mg_l)


.covid_old <- covid

covid <- full_join(
            covid%>%
              select(cases.symptom.onset=cases, date=symptom.date), 
            covid.report%>%
              select(cases.reported=cases, date=report_date), 
            by = "date"
            ) %>% 
         full_join(
           covid.testing%>%
             rename(date=collection_date), 
           by="date"
           ) %>%
         select(date, cases.symptom.onset, cases.reported, pcr_tests, pcr_pos)

rm(covid.report, covid.testing)
```


## Sampling Frequencies

As mentioned in the study design overview, the sampling frequency varied across the study period. Figure \@ref(fig:sf) shows the sampling frequency as the number of days since the previous sample. Most of the observations in the dataset are weekly; however, there are many from the more recent Monday-Wednesday sampling design. There are some irregularities in the sampling frequency, such as in mid-November and mid-January, where there are observations sampled a single day apart. 

```{r, echo = F, message=F, warning=F, error=F}

sf <- wbe %>% 
        group_by(sample_date, facility, biological_replicate, target) %>% 
        summarise(n = n()) %>% 
        ungroup() %>% 
        pivot_wider(names_from = target, values_from = n) %>% 
        mutate(n = N1+N2) %>% 
        group_by(sample_date, facility) %>% 
        summarise(n.bio = n(), n.tech = sum(n, na.rm = T), N1 = sum(N1, na.rm = T), N2 = sum(N2, na.rm = T)) %>% 
        ungroup() 

sf.bio.facility <- sf %>% select(1:3) %>% pivot_wider(names_from = facility, values_from = n.bio) %>% mutate_at(2:4, ~ifelse(is.na(.), 0, .)) %>% mutate(sd2 = lag(sample_date, 1)) %>% mutate(tsls = sample_date - sd2)
```





```{r sf, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*1.75, fig.cap = "Sampling Frequency of Wastewater: Number of Days Between Samples"}
# png(filename = "./consult/03-output/for-pres/sampling_frequency.png", width = 16, height = 9, units = "in", res = 300, pointsize = 16)
par(mar = c(3.1, 7.1, 4.1, 2.1))

plot(sf.bio.facility$sample_date, sf.bio.facility$tsls, type = "l", pch = 16, axes = F, xlab = '', ylab = '')

points(sf.bio.facility$sample_date, sf.bio.facility$tsls, pch = ifelse(weekdays(sf.bio.facility$sample_date)=="Thursday", "R", substr(weekdays(sf.bio.facility$sample_date), 1, 1)), lwd = 4)

all.dates <- unique(sf.bio.facility$sample_date)
axis(1, at = all.dates[which(c(3, all.dates[2:length(all.dates)]-all.dates[1:length(all.dates)-1])>2)], labels = F)
axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])<=2)], labels = F, tck = -0.01)
text(x = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], y = par('usr')[3], labels = format(all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], "%b %d"), srt = 45, adj = c(1.5, 1.5), xpd = T, cex = 0.5)

axis(2, at = seq(1,7, by=1), cex.axis = 0.5, las = 1)


axis(2, at = seq(1,7, by=1), cex.axis = 0.5, las = 1, tick = F, labels = paste("n =", c(2, 9, 0, 1, 7, 2, 21)), line = 5, hadj = 0)

axis(2, at = c(2, 5), labels = F, tick = T, line = -par('pin')[1]/par('cin')[1]*order(sf.bio.facility$sample_date)[which(sf.bio.facility$sample_date==sf.bio.facility$sample_date[which(sf.bio.facility$tsls!=7)][1]-sf.bio.facility$tsls[which(sf.bio.facility$tsls!=7)][1])] / order(sf.bio.facility$sample_date)[which.max(sf.bio.facility$sample_date)], tck = 0.02, lty = 3)
axis(2, at = 3.5, labels = "Indicates\nMonday-Wednesday\nSampling", tick = T, line = -par('pin')[1]/par('cin')[1]*order(sf.bio.facility$sample_date)[which(sf.bio.facility$sample_date==sf.bio.facility$sample_date[which(sf.bio.facility$tsls!=7)][1]-sf.bio.facility$tsls[which(sf.bio.facility$tsls!=7)][1])] / order(sf.bio.facility$sample_date)[which.max(sf.bio.facility$sample_date)], tck = -0.02, las = 1, cex.axis = 0.7, font = 4)

axis(3, at = c(min(sf.bio.facility$sample_date)+7, as.Date("2020-10-27")), labels = F, tick = T, line = 1, lty = 3, tck = 0.02)
axis(3, at = mean(c(min(sf.bio.facility$sample_date)+7, sf.bio.facility$sample_date[which(sf.bio.facility$tsls!=7)][1]-sf.bio.facility$tsls[which(sf.bio.facility$tsls!=7)][1])), labels = "Indicates Weekly Sampling", tick = T, line = 1, cex.axis = 0.7, font = 4)


title(xlab = "Sampling Date", ylab = "Days Since Previous Sample", cex.lab = 0.6, line=2)

title(ylab = "Frequency", cex.lab = 0.6, line = 6.5)

legend("bottomleft", pch = c("M", "T", "W", "R"), legend = c("Monday", "Tuesday", "Wednesday", "Thursday"), cex = 0.7, pt.lwd = rep(4,4))

# dev.off()


```


Figure \@ref(fig:nsamples) shows the number of 24-hour composite wastewater samples taken from each reclamation facility across the study period. Despite a few irregularities, the data show a transition from 2 samples taken to 3 samples taken in early October. There is a single date, `r format(sf.bio.facility$sample_date[which(sf.bio.facility$CC==0)], "%d %B %Y")`, where no samples were taken from the Cedar Creek facility. 



```{r nsamples, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap="Number of 24-hour Composite Wastewater Samples from Reclamation Facilities across the Study"}

# png(filename = "./consult/03-output/for-pres/observations_profile.png", width = 16, height = 9, units = "in", res = 300, pointsize = 16)
par(mar = c(3.1, 3.1, 1.1, 1.1))
plot(sf.bio.facility$sample_date, sf.bio.facility$CC, ylim = c(0, 4.3), pch = 0, axes = F, xlab = '', ylab = '')
points(sf.bio.facility$sample_date, sf.bio.facility$MI+0.15, pch = 1)
points(sf.bio.facility$sample_date, sf.bio.facility$NO-0.15, pch = 2)
all.dates <- unique(sf.bio.facility$sample_date)
axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], labels = F)
axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])<=2)], labels = F, tck = -0.01)
text(x = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], y = par('usr')[3], labels = format(all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], "%b %d"), srt = 45, adj = c(1.5, 1.5), xpd = T, cex = 0.5)

axis(2, at = seq(0,4, by=1), cex.axis = 0.5, las = 1)

title(xlab = "Sampling Date", ylab = "Number of Wastewater Samples", cex.lab = 0.6, line=2)
legend("bottomright", pch = 0:2, legend = c("CC", "MI", "NO"), cex = 0.7)
box()
# dev.off()
```








## "Missingness" Evaluation
  
The RT-qPCR workflow can sometimes produce false negative results. That is, despite having a known concentration of some genetic material in a given sample, it is possible that the RT-qPCR technique can fail to detect any genetic material in the sample. A false negative may occur when the initial concentration of the genetic material in the sample is too small and, despite serial amplifications, this small concentration is never detected. This scenario refers to the concept of a limit of detection; below this limit, the RT-qPCR technique is insensitive and unable to detect the presence of substrate. As such, it is possible for a given sample to return a null result when processed. From a data perspective, we may consider this as *missing* data, however, from a statistical perspective, a more accurate description would be aligned with the concepts of censoring or truncation. It is important to consider that, for those observations with null or "missing" results, all we know is that the viral load falls somewhere between zero and the limit of detection (given no spurious results). Therefore, it is essential to investigate the extent of this *missingness* within the wastewater samples' PCR results. 

Due to the hierarchical structure in the data, we can explore the incomplete data records at various levels of replication. 
  

```{r, echo=F, message=F, warning=F, error=F}
wbe.summary.tr <- wbe %>% 
                    group_by(sample_date, facility, target, biological_replicate) %>% 
                    summarise(
                      n=n(), 
                      n.miss=sum(is.na(ct)), 
                      ct.mean=mean(ct,na.rm=T), 
                      ct.sd=sd(ct,na.rm=T)
                      ) %>% 
                    mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                    ungroup()

wbe.summary.br <- wbe.summary.tr %>%
                    group_by(sample_date, facility, target) %>% 
                    summarise(
                      n.bio = n(),
                      n.bio.non.miss = sum(!is.na(ct.mean)),
                      n.bio.miss = sum(is.na(ct.mean)),
                      n.total = sum(n), 
                      n.total.miss = sum(n.miss), 
                      bio.ct.mean = mean(ct.mean, na.rm = T), 
                      bio.ct.sd = sd(ct.mean, na.rm=T), 
                      tech.ct.dists = paste(paste0(biological_replicate, " = ", round(ct.mean,2), " (sd=", round(ct.sd,2), ", n=", n-n.miss, ")"), collapse = "; ")
                      ) %>%
                      mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                      ungroup()

wbe.missing.profile <- wbe.summary.br %>%
                        mutate(p.missing = n.total.miss / n.total) %>%
                        select(sample_date, facility, target, p.missing)

wbe.missing.profile.overall <- wbe.summary.br %>%
                                group_by(sample_date) %>%
                                summarise(p.missing = sum(n.total.miss) / sum(n.total))
wbe.missing.profile.target <- wbe.summary.br %>%
                                group_by(sample_date, target) %>%
                                summarise(p.missing = sum(n.total.miss) / sum(n.total))
wbe.missing.profile.facility <- wbe.summary.br %>%
                                group_by(sample_date, facility) %>%
                                summarise(p.missing = sum(n.total.miss) / sum(n.total))

```


Figures \@ref(fig:all-miss), \@ref(fig:target-miss), and \@ref(fig:facility-miss) show the missing data profiles of RT-qPCR results for all experimental replicates, stratified by viral sequence target, and stratified by wastewater reclamation facility, respectively. Note that the tick marks on the x-axis represent unique sampling times of wastewater. 


```{r all-miss, echo=F, message=F, warning=F, error=F, fig.cap="Overall Profile of Undetermined RT-qPCR Results"}
# layout(matrix(1), widths = lcm(5*2.54), heights = lcm(5*9/16*2.54), respect = T)
# png(filename = "./consult/03-output/for-pres/overall_missing.png", width = 16*3/5, height = 9*3/5, units = "in", res = 300, pointsize = 16)
par(mar = c(3.1, 3.1, 0, 0), oma = c(0,0,0,0))
plot(wbe.missing.profile.overall$sample_date, wbe.missing.profile.overall$p.missing, type = "o", pch = 16, axes = F, xaxs = 'r', yaxs = 'r', xlab = "", ylab = "", ylim = c(0,1))
all.dates <- unique(wbe.missing.profile.overall$sample_date)
axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], labels = F)
axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])<=2)], labels = F, tck = -0.01)
text(x = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], y = par('usr')[3], labels = format(all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], "%b %d"), srt = 45, adj = c(1.5, 1.5), xpd = T, cex = 0.5)
axis(2, at = seq(0,1, by=0.1), cex.axis = 0.5, las = 1)
axis(2, at = seq(0,1, by=0.05), labels = F, tck = -0.01)
title(xlab = "Sampling Date", ylab = "Proportion of Undetermined RT-qPCR Results", cex.lab = 0.6, line=2)
box()
# dev.off()
```





```{r target-miss, echo=F, message=F, warning=F, error=F, fig.cap="Profile of Undetermined RT-qPCR Results by Viral Sequence Target"}
# png(filename = "./consult/03-output/for-pres/target_missing.png", width = 16*2/5, height = 9*2/5, units = "in", res = 300, pointsize = 16)
par(mar = c(3.1, 3.1, 0, 0), oma = c(0,0,0,0))
plot(wbe.missing.profile.target$sample_date[which(wbe.missing.profile.target$target=="N1")], wbe.missing.profile.target$p.missing[which(wbe.missing.profile.target$target=="N1")], type = "o", pch = 16, axes = F, xaxs = 'r', yaxs = 'r', xlab = "", ylab = "", ylim = c(0,1))
lines(wbe.missing.profile.target$sample_date[which(wbe.missing.profile.target$target=="N2")], wbe.missing.profile.target$p.missing[which(wbe.missing.profile.target$target=="N2")], lty = 2)
points(wbe.missing.profile.target$sample_date[which(wbe.missing.profile.target$target=="N2")], wbe.missing.profile.target$p.missing[which(wbe.missing.profile.target$target=="N2")], pch = 1)

axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], labels = F)
axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])<=2)], labels = F, tck = -0.01)
text(x = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], y = par('usr')[3], labels = format(all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], "%b %d"), srt = 45, adj = c(1.5, 1.5), xpd = T, cex = 0.5)
axis(2, at = seq(0,1, by=0.1), cex.axis = 0.5, las = 1)
axis(2, at = seq(0,1, by=0.05), labels = F, tck = -0.01)
title(xlab = "Sampling Date", ylab = "Proportion of Undetermined RT-qPCR Results", cex.lab = 0.6, line=2)
box()
legend("bottomleft", lty = c(1,2), pch = c(16,1), legend = c("N1", "N2"), cex = 0.7)
# dev.off()
```


```{r facility-miss, echo=F, message=F, warning=F, error=F, fig.cap="Profile of Undetermined RT-qPCR Results by Wastewater Reclamation Facility"}
# png(filename = "./consult/03-output/for-pres/facility_missing.png", width = 16*2/5, height = 9*2/5, units = "in", res = 300, pointsize = 16)
par(mar = c(3.1, 3.1, 0, 0), oma = c(0,0,0,0))
plot(wbe.missing.profile.facility$sample_date[which(wbe.missing.profile.facility$facility=="CC")], wbe.missing.profile.facility$p.missing[which(wbe.missing.profile.facility$facility=="CC")], type = "o", pch = 0, axes = F, xaxs = 'r', yaxs = 'r', xlab = "", ylab = "", ylim = c(0,1), cex = 0.7)
lines(wbe.missing.profile.facility$sample_date[which(wbe.missing.profile.facility$facility=="NO")], wbe.missing.profile.facility$p.missing[which(wbe.missing.profile.facility$facility=="NO")], lty = 5)
points(wbe.missing.profile.facility$sample_date[which(wbe.missing.profile.facility$facility=="NO")], wbe.missing.profile.facility$p.missing[which(wbe.missing.profile.facility$facility=="NO")], pch = 2, cex = 0.7)
lines(wbe.missing.profile.facility$sample_date[which(wbe.missing.profile.facility$facility=="MI")], wbe.missing.profile.facility$p.missing[which(wbe.missing.profile.facility$facility=="MI")], lty = 3)
points(wbe.missing.profile.facility$sample_date[which(wbe.missing.profile.facility$facility=="MI")], wbe.missing.profile.facility$p.missing[which(wbe.missing.profile.facility$facility=="MI")], pch = 8, cex = 0.7)


axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], labels = F)
axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])<=2)], labels = F, tck = -0.01)
text(x = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], y = par('usr')[3], labels = format(all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], "%b %d"), srt = 45, adj = c(1.5, 1.5), xpd = T, cex = 0.5)
axis(2, at = seq(0,1, by=0.1), cex.axis = 0.5, las = 1)
axis(2, at = seq(0,1, by=0.05), labels = F, tck = -0.01)
title(xlab = "Sampling Date", ylab = "Proportion of Undetermined RT-qPCR Results", cex.lab = 0.6, line=2)
box()
legend("bottomleft", lty = c(1,5,3), pch = c(0,2,8), legend = c("CC", "NO", "MI"), cex = 0.7)
# dev.off()
```

```{r, echo=F, message=F, warning=F, error=F}

wbe.missing.profile.overall.samples <- wbe.summary.br %>%
    group_by(sample_date) %>%
    summarise(n.samples.non.miss = sum(n.total) - sum(n.total.miss))

```



Missing data are quite extensive. Of the total `r nrow(wbe)` records, `r paste0(sum(is.na(wbe$ct)), " (", round(sum(is.na(wbe$ct))/nrow(wbe)*100, 1), "%)")` had undetermined values for the cycle threshold, i.e., effectively missing or censored. On the other hand, Figure \@ref(fig:n-non-miss) shows the total number of samples that RT-qPCR detected as positive. There are an average of `r mean(wbe.missing.profile.overall.samples$n.samples.non.miss)` positive samples for each day across the study; over 80% of the sampling dates have more than 6 positive results. [^caveat]
  
[^caveat]:The number of experiments ran / technical replicates was not strictly uniform across the study. 

```{r n-non-miss, echo=F, message=F, warning=F, error=F, fig.cap="Overall Profile of Positive Samples from RT-qPCR"}
# layout(matrix(1), widths = lcm(5*2.54), heights = lcm(5*9/16*2.54), respect = T)
par(mar = c(3.1, 3.1, 0, 0), oma = c(0,0,0,0))
plot(wbe.missing.profile.overall.samples$sample_date, wbe.missing.profile.overall.samples$n.samples.non.miss, type = "o", pch = 16, axes = F, xaxs = 'r', yaxs = 'r', xlab = "", ylab = "", ylim = c(0, max(wbe.missing.profile.overall.samples$n.samples.non.miss)*1.1))
all.dates <- unique(wbe.missing.profile.overall$sample_date)
axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], labels = F)
axis(1, at = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])<=2)], labels = F, tck = -0.01)
text(x = all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], y = par('usr')[3], labels = format(all.dates[which(c(3, all.dates[2:43]-all.dates[1:42])>2)], "%b %d"), srt = 45, adj = c(1.5, 1.5), xpd = T, cex = 0.5)
axis(2, at = seq(0,max(wbe.missing.profile.overall.samples$n.samples.non.miss)*1.1, by=5), cex.axis = 0.5, las = 1)
axis(2, at = seq(0,max(wbe.missing.profile.overall.samples$n.samples.non.miss)*1.1, by=1), labels = F, tck = -0.01)
title(xlab = "Sampling Date", ylab = "Number of Samples with Detectable Viral Loads", cex.lab = 0.6, line=2)
box()
```


  
  
Given the hierarchical structure of the data, it is useful to explore the undetermined values at different levels. Table \@ref(tab:prop-detect) and Figure \@ref(fig:bar-prop-detect) show the percentages / proportions of undetermined values stratified by wastewater reclamation facility and viral sequence target; these data are aggregated across the study duration. The three columns of percentages (with frequencies in parentheses) are given to show the three levels of organization within the data.   
  
```{r, prop-detect, echo = F, message=F, warning=F, error=F}
prop.detect <- wbe.summary.br%>%
                      group_by(facility, target)%>%
                      summarise(
                        n.miss.day=sum(n.bio.non.miss==0),
                        n.days=n(),
                        n.bio.non.miss=sum(n.bio.non.miss), 
                        n.bio.miss=sum(n.bio.miss), 
                        n.bio=sum(n.bio), 
                        n.tech.miss=sum(n.total.miss), 
                        n.tech=sum(n.total)
                        )%>%
                      mutate(
                        prop.m.day=n.miss.day/n.days,
                        prop.m.bio=n.bio.miss/n.bio, 
                        prop.m.tech=n.tech.miss/n.tech
                        )

prop.detect.tab <- prop.detect %>%
  mutate(
    prop.day = paste0(round(prop.m.day*100,1), " (", n.miss.day, " / ", n.days, ")"),
    prop.bio = paste0(round(prop.m.bio*100,1), " (", n.bio.miss, " / ", n.bio, ")"), 
    prop.tech = paste0(round(prop.m.tech*100,1), " (", n.tech.miss, " / ", n.tech, ")"))%>%
  select(facility, target, prop.day, prop.bio, prop.tech)

names(prop.detect.tab) <- c("Wastewater Reclamation Facility", "Viral Sequence Target", "Percentage of Sampling Dates in which All Results were Undetermined", "Percentage of Biological Replicates in which All Results were Undetermined", "Percentage of Technical Replicates with Undetermined Results")

flextable(prop.detect.tab) %>% 
  font(part = "all", fontname = "Arial") %>%
  fontsize(part = "header", size = 9) %>%
  fontsize(part = "body", size = 8) %>%
  border_remove() %>%
  border(part = "header", border.bottom = fp_border_default(color = "black", width = 2)) %>%
  width(j=1:2, width = 1) %>%
  width(j=3:5, width = 1.5) %>%
  merge_v(j=1) %>%
  valign(j=1, valign = "top") %>%
  bold(part = "header") %>% 
  set_caption(caption = "Extent of Undetermined RT-qPCR Values at Various Levels of the Data Hierarchy")
```

  
The first column of percentages labeled "Percentage of Sampling Dates in which All Results were Undetermined" in Table \@ref(tab:prop-detect) shows the frequencies of undetermined RT-qPCR values with respect to the date of sampling. To explain further, of the `r length(unique(wbe$sample_date))` distinct sampling dates in the data[^other-missing], there are some sampling dates where ***every*** RT-qPCR experiment for that day's samples yielded undetermined results. For example, the samples from Cedar Creek on 30 June 2020 all yielded undetermined results for all biologic and technical replicates for both viral sequence targets, N1 and N2, in the RT-qPCR experiments.
    
    
[^other-missing]:There are some implicitly missing data within the datasets with respect to sampling dates, facilities, and sequence targets. That is, there are some instances where no RT-qPCR results exist in the data for a given sampling date, facility, and viral sequence target. For example, there are no results Cedar Creek on 14 July 2020 for either viral sequence target, N1 or N2.  
  
```{r, echo = F, message=F, warning=F, error=F}
n.bio.reps <- wbe %>% 
                group_by(sample_date, facility, biological_replicate) %>% 
                summarise(n=1) %>% 
                group_by(facility) %>% 
                summarise(n=n())


# table(wbe$biological_replicate, wbe$target, wbe$sample_date, wbe$facility)

## potential data entry error???
# table(wbe$biological_replicate[which(wbe$sample_date%in%c(as.Date("2020-11-18"), as.Date("2020-11-19")) & wbe$facility=="CC")], wbe$target[which(wbe$sample_date%in%c(as.Date("2020-11-18"), as.Date("2020-11-19")) & wbe$facility=="CC")], wbe$sample_date[which(wbe$sample_date%in%c(as.Date("2020-11-18"), as.Date("2020-11-19")) & wbe$facility=="CC")])
```
  
The next column, labeled "Percentage of Biological Replicates in which All Results were Undetermined" goes to the next smaller level in the data hierarchy: the biological replicates. There are a total of `r n.bio.reps$n[which(n.bio.reps$facility=="CC")]`, `r n.bio.reps$n[which(n.bio.reps$facility=="MI")]`, and `r n.bio.reps$n[which(n.bio.reps$facility=="NO")]` biological replicates from the Cedar Creek, Middle Oconee, and North Oconee Wastewater Reclamation Facilities, respectively.[^potential-errors] So, the percentages in this column refer to the frequencies in which *all* the technical replicate experiments among the biological replicates yielded undetermined results.   
  
The last column in Table \@ref(tab:prop-detect) shows the percentage of all technical replicates (i.e., the smallest experimental unit) that yielded an undetermined result from the RT-qPCR. Unlike the other columns, this one is a simple frequency calculation of the raw data. In fact, if you sum the denominators of each row in the last column, then you will get `r nrow(wbe)` which is the total number of rows / experimental units in the RT-qPCR dataset. 
  

[^potential-errors]:Upon closer examination to the data, I may have uncovered a potential data entry error. For the Cedar Creek samples, there are uncrossed biological replicates with respect to viral sequence targets. For example, there are 3 technical replicates for each of 3 biological replicates for Cedar Creek on 18 November 2020; however, these records only exist for the N1 sequence target. Similarly, there are 9 records for Cedar Creek on 19 November 2020, but these are only for the N2 sequence target. I suspect these experimental units are from the same biological samples collected on the same day and that the 18 v 19 date is a simple entry error. There may be other entry errors, though. Closer inspection of the x-axes for Figures \@ref(fig:all-miss), \@ref(fig:target-miss), and \@ref(fig:facility-miss) may shed some light on the matter. 


(ref:barplot-caption) Barplot of Proportions Undetermined. Here, D, B, & T refer to sampling dates, biological replicates, and technical replicates, respectively. N1 & N2 refer to the viral sequence targets. CC, MI, & NO refer to the wastewater reclamation facilities Cedar Creek, Middle Oconee, and North Oconee, respectively.



```{r bar-prop-detect, echo = F, message=F, warning=F, error=F, fig.cap = '(ref:barplot-caption)'}
par(mar=c(5.1,4.1,4.1,2.1))
col.indices <- c((0:1295*6+1)[0:215*6+1][0:35*6+1][0:5*6+1], (0:1295*6+1)[0:215*6+1][0:35*6+1][0:5*6+1]+216, (0:1295*6+1)[0:215*6+1][0:35*6+1][0:5*6+1]+432)
col.indices <- col.indices[order(col.indices)]

bar.data <- t(prop.detect[,10:12])

barplot(matrix(1, nrow=3, ncol=6), beside = T, col = viridis::viridis(max(col.indices), alpha = 0.25)[col.indices], cex.axis = 0.7, las = 1)
barplot(bar.data, beside = T, add = T, col = viridis::viridis(max(col.indices))[col.indices], legend=F, axes = F)

text(x=seq(1,{18+5})[-seq(4,{18+5}, by = 4)]+0.5, y=0, labels = c("D", "B", "T"), xpd=T, adj = c(0.5, 1), cex=0.5)
text(x=seq(1,{18+5})[-seq(4,{18+5}, by = 4)][seq(2,23,by=3)]+0.5, y=0, labels = rep(c("N1", "N2"), 3), xpd=T, adj = c(0.5, 3), cex=0.6)
axis(1, at = c(2.5,6.5), tick = T, labels = F, line = 2, tck = 0.02)
axis(1, at = c(10.5,14.5), tick = T, labels = F, line = 2, tck = 0.02)
axis(1, at = c(18.5,22.5), tick = T, labels = F, line = 2, tck = 0.02)
text(x=seq(4,{18+5}, by=8)+0.5, y=0, labels = c("CC", "MI", "NO"), xpd=T, adj = c(0.5, 6), cex=0.7)
title(ylab = "Proportion Undetermined Values")
```


To further explore the missingness and observed differences with respect to the viral sequence target and wastewater reclamation facility, logistic regression models were fit predicting the missing-data indicator from viral sequence target and wastewater reclamation facility.[^binomial] Table \@ref(tab:fit-tab) shows the results of these logistic regression models. 
  
[^binomial]:The logistic regression models used the cbind() designation of a binomial for the response variable. The models were fit on the data used to generate Table \@ref(tab:prop-detect). For example, the frequency of undetermined results aggregated to the sampling date for Cedar Creek and the N1 sequence target is 9 out of 41 total sampling dates. So, the model used the "missing" values and "non-missing" values to form the response, e.g., `cbind(9, 32)` for Cedar Creek and the N1 sequence target. Similarly, for all combinations of wastewater reclamation facility and viral sequence target, the variables for the frequency of missing and non-missing observations were combined using cbind() which served as the response variable in the models. This grouped binomial designation is equivalent in estimation as the more common 0 / 1 coding in logistic regression. 
  
```{r, echo=F, message=F, warning=F, error=F}

fit.day <- glm(cbind(n.miss.day, n.days-n.miss.day)~target+facility, data=prop.detect, family="binomial")
fit.day.aov <- car::Anova(fit.day, type = 3)
fit.day.mean.ests <- predict(fit.day, type = "response", se.fit = T)
fit.day.mean.ests <- paste0(round(fit.day.mean.ests$fit,3), " (", round(fit.day.mean.ests$fit - fit.day.mean.ests$se.fit*qnorm(0.975),3), ", ", round(fit.day.mean.ests$fit + fit.day.mean.ests$se.fit*qnorm(0.975), 3), ")")



fit.bio <- glm(cbind(n.bio.miss, n.bio.non.miss)~target+facility, data=prop.detect, family="binomial")
fit.bio.aov <- car::Anova(fit.bio, type = 3)
fit.bio.mean.ests <- predict(fit.bio, type = "response", se.fit = T)
fit.bio.mean.ests <- paste0(round(fit.bio.mean.ests$fit,3), " (",round(fit.bio.mean.ests$fit - fit.bio.mean.ests$se.fit*qnorm(0.975),3), ", ", round(fit.bio.mean.ests$fit + fit.bio.mean.ests$se.fit*qnorm(0.975), 3), ")")


fit.tech <- glm(cbind(n.tech.miss, n.tech-n.tech.miss)~target+facility, data=prop.detect, family="binomial")
fit.tech.aov <- car::Anova(fit.tech, type = 3)
fit.tech.mean.ests <- predict(fit.tech, type = "response", se.fit = T)
fit.tech.mean.ests <- paste0(round(fit.tech.mean.ests$fit,3), " (", round(fit.tech.mean.ests$fit - fit.tech.mean.ests$se.fit*qnorm(0.975),3), ", ", round(fit.tech.mean.ests$fit + fit.tech.mean.ests$se.fit*qnorm(0.975), 3), ")")

fit.tab <- cbind(prop.detect.tab[,1:2], "P of Sampling bio with All Undetermined Results"=fit.bio.mean.ests, "P of Biological Replicates with All Undetermined Results"=fit.bio.mean.ests, "P of Technical Replicates with Undetermined Results"=fit.tech.mean.ests)


fit.aovs <- rbind(cbind(Model = "Days", Effect=rownames(fit.day.aov), round(fit.day.aov, 3)), cbind(Model = "Bios", Effect=rownames(fit.bio.aov), round(fit.bio.aov, 3)), cbind(Model = "Techs", Effect=rownames(fit.tech.aov), round(fit.tech.aov,3)))
fit.aovs %<>% add_row(.before=1) %>% mutate_all(as.character)
fit.aovs[1,] <- as.list(names(fit.aovs))


fit.tab <- add_row(fit.tab %>% ungroup(), .before = 1) %>% mutate_all(as.character)
fit.tab[1,] <- as.list(names(fit.tab))
fit.tab <- add_row(fit.tab)
# nrow(fit.tab)
i=1
repeat{
  fit.tab <- add_row(fit.tab)
  fit.tab[nrow(fit.tab),] <- as.list(fit.aovs[i,])
  if(nrow(fit.tab)==6+nrow(fit.aovs)+2){break}else{i=i+1}

}


fit.day.coefs <- cbind(Model = "Days", Parameter=c("N2 v N1", "MI v CC", "NO v CC"), "OR Estimate"=paste0(round(exp(coef(fit.day)[-1]), 3), " (", round(exp(confint(fit.day)[-1,1]),3), ", ", round(exp(confint(fit.day)[-1,2]),3), ")"))
fit.bio.coefs <- cbind(Model = "Bios", Parameter=c("N2 v N1", "MI v CC", "NO v CC"), "OR Estimate"=paste0(round(exp(coef(fit.bio)[-1]), 3), " (", round(exp(confint(fit.bio)[-1,1]),3), ", ", round(exp(confint(fit.bio)[-1,2]),3), ")"))
fit.tech.coefs <- cbind(Model = "Techs", Parameter=c("N2 v N1", "MI v CC", "NO v CC"), "OR Estimate"=paste0(round(exp(coef(fit.tech)[-1]), 3), " (", round(exp(confint(fit.tech)[-1,1]),3), ", ", round(exp(confint(fit.tech)[-1,2]),3), ")"))


fit.coefs <- rbind(fit.day.coefs, fit.bio.coefs, fit.tech.coefs)
fit.coefs <- fit.coefs %>% as.data.frame() %>% add_row(.before = 1)
fit.coefs[1,] <- as.list(names(fit.coefs))
fit.coefs <- cbind(fit.coefs, NA, NA)

fit.tab <- add_row(fit.tab)
# nrow(fit.tab)
i=1
repeat{
  fit.tab <- add_row(fit.tab)
  fit.tab[nrow(fit.tab),] <- as.list(fit.coefs[i,])
  if(nrow(fit.tab)==16+nrow(fit.coefs)){break}else{i=i+1}

}
```

```{r fit-tab, echo = F, message=F, warning=F, error=F}

fit.tab <- cbind(new = NA, fit.tab)

fit.tab$new[c(1, which(is.na(fit.tab$`Wastewater Reclamation Facility`))+1)] <- c("A", "B", "C")


flextable(fit.tab) %>%
  delete_part(part = "header") %>%
  border_remove() %>%
  
  border(part = "body", i = which(fit.tab$`Wastewater Reclamation Facility`=="Model")-1, border.top = fp_border_default(color="black", width = 0.5)) %>% 
  border(part = "body", i = c(1,which(fit.tab$`Wastewater Reclamation Facility`=="Model")), border.bottom = fp_border_default(color="black", width = 2)) %>%
  font(part = "all", fontname = "Arial") %>% 
  fontsize(part = "header", size = 9) %>%
  fontsize(part = "body", size = 8) %>% 
  fontsize(part = "body", i = c(1,which(fit.tab$`Wastewater Reclamation Facility`=="Model")), size = 9) %>%
  
  bold(part = "body", i = c(1, which(fit.tab$`Wastewater Reclamation Facility`=="Model")), j = 1) %>% 
  fontsize(part = "body", i = c(1, which(fit.tab$`Wastewater Reclamation Facility`=="Model")), j = 1, size = 12) %>% 
  
  bold(part = "header") %>% 
  bold(part = "body", i = c(1,which(fit.tab$`Wastewater Reclamation Facility`=="Model"))) %>%
  merge_v(j=2) %>% 
  width(j=2:3, width = 1) %>%
  width(j=4:6, width = 1.5) %>%
  align(part = "header", align = "center") %>%
  align(part = "body", j=4:6, align = "right") %>% 
  align(part = "body", i = c(1,which(fit.tab$`Wastewater Reclamation Facility`=="Model")), align = "center") %>% 
  valign(part = "body", j=2, valign = "top") %>%
  bold(part = "body", j=6, i = c(13,15)) %>% 
  bold(part = "body", j=4, i = c(22,25)) %>% 
  italic(part = "body", j=4, i = 26) %>%
  set_caption(caption = "(ref:logmodels)") %>% 
  compose(part="body", j=4, i=1, value = as_paragraph("P\U0302 of Sampling Days in which All Results were Undetermined")) %>% 
  compose(part="body", j=5, i=1, value = as_paragraph("P\U0302 of Biological Replicates in which All Results were Undetermined")) %>%
  compose(part="body", j=6, i=1, value = as_paragraph("P\U0302 of Technical Replicates with All Undetermined Results"))
```


(ref:logmodels) Logistic Regression for Undetermined RT-qPCR Results on Viral Sequence Target and Wastewater Reclamation Facility. A gives probability estimates from the logistic regression models for the missing frequencies; B shows the results of analyses of deviance; and C gives estimated odds ratios.

    
Interestingly, the facility was determined to have had some effect on the frequencies of undetermined results for the data at the biological replicate level ($\chi^2$ = `r round(fit.bio.aov[2,1], 1)`, df = 2, p = `r round(fit.bio.aov[2,3], 3)`) and the technical replicate level ($\chi^2$ = `r round(fit.tech.aov[2,1], 1)`, df = 2, p = `r round(fit.tech.aov[2,3], 3)`) after controlling for sequence target. No significant differences were observed for the data aggregated at the sampling date level ($\chi^2$ = `r round(fit.day.aov[2,1], 1)`, df = 2, p = `r round(fit.day.aov[2,3], 3)`), but this is not surprising as the artificial reduction in sample size likely reduced the power of the analysis. The viral sequence target does not exhibit evidence of having any impact on "missing" data (p = `r round(fit.day.aov[1,3], 3)`, `r round(fit.bio.aov[1,3], 3)`, and `r round(fit.tech.aov[1,3], 3)` for analyses of sampling dates, biological replicates, and technical replicates, respectively); that is, the frequencies of undetermined experimental values are indistinguishable between N1 and N2 at each level while controlling for facility. The differences among facilities appears to be mostly the result of lower frequencies (lower estimated odds) of undetermined values for the Middle Oconee wastewater reclamation facility when compared with Cedar Creek for both analyses at the biological and technical replicate levels. Notably, the estimated odds ratio comparing undetermined values in North Oconee to Cedar Creek is *marginally* significant for the analyses of technical replicates (i.e., its confidence interval just barely includes the null value so it is inconclusive whether North Oconee's missingness differs from that of Cedar Creek). It seems that Cedar Creek samples are more likely to yield undetermined results from the RT-qPCR experiments.
  
Regardless of the ***patterns*** of missingness within the data, it may be important to explore approaches to use any information of a missing / undetermined result. To accomplish this, we explore limits of detection in the next subsection.   
  




## Exploring Limits of Detection and Quantification from the Data Perspective
  
The limit of detection refers to a threshold where a positive sample (i.e., one that **does** contain targeted substrate) is indistinguishable from a negative sample (i.e., one that **does not** contain any targeted substrate) given some technique. Therefore, some of the undetermined values within the data could be truly positive, but, since the substrate is at such a low concentration, the experimental approach fails to detect anything (i.e., a false negative).[^other-miss-source]

[^other-miss-source]:It may be important to acknowledge that these experimental methods in themselves are not without flaw nor devoid of other potential sources of measurement error. So, even given a sample with substrate concentrations above the limit of detection, it is still possible and not unlikely that an experiment could have an undetermined / negative result (i.e., spurious). 

Theoretically, it may be possible to calculate a limit of detection through considerations of the experimental procedure alone.[^lower-bound] The RT-qPCR workflow begins with the water / sewage sample from which 280 microliters, $\mu L$, is used in RNA extraction. Following, the RNA is eluted into 60 $\mu L$ of a buffer solution. Three $\mu L$ of the 60 $\mu L$ of RNA-in-buffer is added to a reverse transcription reaction of 25 $\mu L$. Finally, 2 $\mu L$ of the reverse transcription reaction product is transferred to 20 $\mu L$ wells. These 25 $\mu L$ product of the reverse transcription reaction is distributed in ~2 $\mu L$ quantities for three technical replicates for each viral sequence target; the remainder is used in controls.[^description-flaws]   
  
Altogether, these quantities are important for understanding the dilution scheme imposed by the lab procedures which can then be used in calculating the theoretical limit of detection (Equation \@ref(eq:dilution)). In words, if there was a ***single*** viral sequence copy within our sample, then we can track the copy through the dilution to calculate the concentration that is used in the PCR amplifications: 

\begin{equation}
  Theoretical~LOD = \frac{1~viral~copy}{60\mu L} \times \frac{3\mu L}{25\mu L} \times \frac{2\mu L}{20\mu L} = 0.0002~copies~ \mu L^{-1} ~of~reaction.
  (\#eq:dilution)
\end{equation}
  
Although a useful quantity / concentration to know, this limit of detection is rather optimistic. So, in lieu of an experimental approach, we investigated some data analysis approaches to quantify the limit. 
    
[^lower-bound]:This theoretical limit of detection may be more aligned with an absolute lower bound for procedure as it would assume perfectly efficient lab techniques and reactions. 
[^description-flaws]:I am not confident in my verbose description of the lab procedure nor essential verbage.
  
    
    
### Observed Distributions of Cycle Thresholds and Normality
    
The cycle threshold values for detected samples range from approximately `r round(min(wbe$ct, na.rm=T),2)` to `r round(max(wbe$ct, na.rm=T),2)`; a histogram of these data is shown in Figure \@ref(fig:ct-hist). There appears to be a stacking of observations around Ct = 37. This potential "boundary" in the data distribution may indicate both limits of detection (37+) and quantification (37-). 

```{r ct-hist, echo = F, message = F, warning=F, error=F, fig.cap="Histogram of Cycle Thresholds"}
# png(filename = "./consult/03-output/for-pres/ct_hist.png", width = 16*3/5, height = 9*3/5, units = "in", res = 300, pointsize = 16)
par(mar = c(2.1, 2.1, 1.1, 0))

hist(wbe$ct, breaks = 100, axes = F, xlab = "", ylab = "", main = "")
axis(1, at = 32:39, labels = T, tick = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = "Cycle Threshold", line = 1)
axis(2, at = seq(0,40,by=10), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Frequency", line = 1.25)
# title(main = "Histogram of Cycle Thresholds", line = 0.25)
# dev.off()
```
  
Figure \@ref(fig:qq-plots-ct) shows the normal quantile plots for the cycle thresholds of the two viral sequence targets, N1 and N2, separately. If the data were approximately normally distributed, we would expect a roughly straight line (shown by the gray diagonal line). However, there appears to be some deviations from normality at the upper tail for higher Ct values. From this normal quantile plots, we can identify two points of interest. The first point of interest would be the initial inflection point (scanning from left to right or low Ct values to higher Ct values) where the data deviate from a normal distribution tail, i.e., begin stacking in the histogram. The second point of interest would approximate to the point of truncation seen in Figure \@ref(fig:ct-hist) and in Figure \@ref(fig:qq-plots-ct) as the right-hand side of the flatter portion of the "curve" of data points.[^programmatic-estimation]

[^programmatic-estimation]:To estimate the ct values associated with these points of interest, I programmatically scanned the qqplot and the slopes for each pair of points (scanning right to left with respect to the qqplots [artifact of first writing program for copy number scanning]). When the slopes were successively below 1, this indicated the first point of interest (i.e., the limit of detection). Following, when the slopes then approximated 1 again, this indicated the second point of interest (i.e., the limit of quantification).
  

```{r qq-plots-ct, echo=F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap="Normal Quantile-Quantile Plots of Observed Cycle Thresholds for Viral Sequence Targets N1 and N2"}

qqnorm.ct.n1 <- qqnorm(wbe$ct[which(wbe$target=="N1")], plot.it = F) %>% as.data.frame()
qqnorm.ct.n2 <- qqnorm(wbe$ct[which(wbe$target=="N2")], plot.it = F) %>% as.data.frame()


qqnorm.Explorer.ct <- function(qqnorm.ct){
        qqnorm.ct <- qqnorm.ct[which(complete.cases(qqnorm.ct)),]
        qqnorm.ct <- qqnorm.ct[order(qqnorm.ct$x),]
        qqnorm.ct <- cbind(qqnorm.ct, rbind(NA, qqnorm.ct[-nrow(qqnorm.ct),])) %>% setNames(., nm = c("x", "y", "x-1", "y-1"))
        qqnorm.ct %<>% mutate(rise = y-`y-1`, run = x-`x-1`) %>% mutate(slope = rise / run)
        
        qqnorm.ct$lod <- NA
        qqnorm.ct$loq <- NA
        
        prev.slope <- 1
        lod.found <- 0
        for(i in nrow(qqnorm.ct):2){
          if(lod.found==0){
            if(qqnorm.ct$slope[i]<1 & prev.slope <1){
              qqnorm.ct$lod[i] <- 1
              lod.found <- 1
            }else{
              prev.slope <- qqnorm.ct$slope[i]
            }
          }
          if(lod.found==1){
            if(qqnorm.ct$slope[i]>1){
              qqnorm.ct$loq[i] <- 1
              break
            }else{
              prev.slope <- qqnorm.ct$slope[i]
            }
          }
        }
        
        
        lod.ct <- qqnorm.ct$y[which(qqnorm.ct$lod==1)]
        loq.ct <- qqnorm.ct$y[which(qqnorm.ct$loq==1)]

        return(list(qqnorm.dataset = qqnorm.ct, lod = lod.ct, loq = loq.ct))
}



qqnorm.ct.n1 <- qqnorm.Explorer.ct(qqnorm.ct.n1)
qqnorm.ct.n2 <- qqnorm.Explorer.ct(qqnorm.ct.n2)
        

# png(filename = "./consult/03-output/for-pres/limits_ct.png", width = 16*2/5, height = 9*2/5*2, units = "in", res = 300, pointsize = 16)
# png(filename = "./consult/03-output/limits_ct.png", height = 9, width = 8, units = "in", res = 300)
# layout(matrix(c(1,2), nrow = 2), widths = c(lcm(6*2.54)), heights = c(lcm(6*9/16*2.54), lcm(6*9/16*2.54)))
# par(mar = c(2.1, 2.1, 1.1, 0), fig = c(0,1,0.5,1))
# par(mar = c(2.1, 2.1, 1.1, 0), fig = c(0,1,0,0.5), new = T)
# 
# dev.off()


par(mfcol = c(2,1), mar = c(2.1, 2.1, 1.1, 0))

# layout.show(2)
qqnorm(wbe$ct[which(wbe$target=="N1")],  axes = F, ylab = "", xlab = "", main = "")
qqline(wbe$ct[which(wbe$target=="N1")], col = "gainsboro")
axis(1, at = -3:3, tick = T, labels = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = "Theoretical Quantiles", line = 1)

axis(2, at = 32:39, tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Observed Cycle Thresholds", line = 1.25)

abline(h = qqnorm.ct.n1$lod)
text(par('usr')[1], par('usr')[4], labels = paste("LOD =", round(qqnorm.ct.n1$lod,3)), adj = c(-0.05,1.2))
abline(h = qqnorm.ct.n1$loq, lty = 3)
text(par('usr')[1], par('usr')[4], labels = paste("LOQ =", round(qqnorm.ct.n1$loq,3)), adj = c(-0.05,2.4))
legend("bottomright", lty = c(1,3), legend = c("LOD", "LOQ"))
# title(main = "Normal Q-Q Plot for N1 Cycle Threshold", line = 0.25)
box()


text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "A", adj = c(0,1), xpd = T, cex = 1, font = 2)




qqnorm(wbe$ct[which(wbe$target=="N2")],  axes = F, ylab = "", xlab = "", main = "")
qqline(wbe$ct[which(wbe$target=="N2")], col = "gainsboro")
axis(1, at = -3:3, tick = T, labels = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = "Theoretical Quantiles", line = 1)

axis(2, at = 33:39, tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Observed Cycle Thresholds", line = 1.25)


abline(h = qqnorm.ct.n2$lod)
text(par('usr')[1], par('usr')[4], labels = paste("LOD =", round(qqnorm.ct.n2$lod,3)), adj = c(-0.05,1.2))
abline(h = qqnorm.ct.n2$loq, lty = 3)
text(par('usr')[1], par('usr')[4], labels = paste("LOQ =", round(qqnorm.ct.n2$loq,3)), adj = c(-0.05,2.4))
legend("bottomright", lty = c(1,3), legend = c("LOD", "LOQ"))
# title(main = "Normal Q-Q Plot for N2 Cycle Threshold", line = 0.25)
box()
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "B", adj = c(0,1), xpd = T, cex = 1, font = 2)

# dev.off()
# dev.off()
```

So, with careful consideration, we can rationalize the meaning of these points. The point of truncation within the histogram and the larger point of interest determined from the normal quantile plots may serve as a proxy for the limit of detection. There are few values more extreme than this cutoff point and it seemingly acts as a boundary in the Ct values distribution.   
  
Also, the point of inflection where the data deviate from normality may serve as a proxy for a limit of quantification. Similar to a limit of detection, a limit of quantification represents a lower boundary of concentrations / quantities beyond which the experimental procedure performs poorly. The limit of quantification is defined for higher concentrations / larger quantities of substrate when compared to the limit of detection. This means that the experimental procedure would be able to detect a sample as positive as it is above the detection limit. However, for those samples with concentrations above the limit of detection yet below the limit of quantification, the experiment fails to accurately estimate and distinguish the concentrations of substrate.   
  
Table \@ref(tab:limits-table) explores where the cycle threshold data fall with respect to these newly defined limits. A more detailed table can be found in Appendix \@ref(morelod).

```{r limits-table, echo=F, message=F, warning=F, error=F}
wbe.summary.lod <- wbe %>% 
                        mutate(
                          ct.b.lod = ifelse(target=="N1", ct>qqnorm.ct.n1$lod, ct>qqnorm.ct.n2$lod),
                          ct.loq.lod = ifelse(target=="N1", 
                                              ct>qqnorm.ct.n1$loq & ct<=qqnorm.ct.n1$lod, 
                                              ct>qqnorm.ct.n2$loq & ct<=qqnorm.ct.n2$lod), 
                          ct.good = ifelse(target =="N1", 
                                           ct<=qqnorm.ct.n1$loq, 
                                           ct<=qqnorm.ct.n2$loq)
                        ) %>%
                        group_by(sample_date, facility, target, biological_replicate) %>% 
                        summarise(
                          n=n(), 
                          n.miss = sum(is.na(ct)), 
                          n.b.lod = sum(ct.b.lod, na.rm = T),
                          n.loq.lod = sum(ct.loq.lod, na.rm = T), 
                          n.good = sum(ct.good, na.rm = T)
                        ) %>% 
                        mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                        ungroup() %>%
  
                        group_by(sample_date, facility, target) %>% 
                        summarise(
                          n.bio = n(),
                          n.bio.miss = sum(n==n.miss),
                          n.bio.b.lod = sum(n==n.miss+n.b.lod), 
                          n.bio.loq.lod = sum(n==n.miss+n.b.lod+n.loq.lod),
                          n.bio.good = sum(n!=n.miss+n.b.lod+n.loq.lod),

                          n.total = sum(n), 
                          n.total.miss = sum(n.miss),
                          n.total.b.lod = sum(n.b.lod), 
                          n.total.loq.lod = sum(n.loq.lod), 
                          n.total.good = sum(n.good)
                      ) %>%
                        mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                        ungroup() %>%

                        group_by(facility, target) %>%
                        summarise(
                          n.days = n(), 
                          n.days.miss = sum(n.bio == n.bio.miss), 
                          n.days.b.lod = sum(n.bio == n.bio.b.lod), 
                          n.days.loq.lod = sum(n.bio == n.bio.loq.lod), 
                          n.days.good = sum(n.bio != n.bio.loq.lod),
                          
                          n.bio = sum(n.bio), 
                          n.bio.miss = sum(n.bio.miss), 
                          n.bio.b.lod = sum(n.bio.b.lod), 
                          n.bio.loq.lod = sum(n.bio.loq.lod), 
                          n.bio.good = sum(n.bio.good),
                          
                          n.total = sum(n.total), 
                          n.total.miss = sum(n.total.miss), 
                          n.total.b.lod = sum(n.total.b.lod), 
                          n.total.loq.lod = sum(n.total.loq.lod), 
                          n.total.good = sum(n.total.good)
                        ) %>%
                        mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                        ungroup()

wbe.lod.tr <- wbe.summary.lod %>% 
  mutate(
    total.miss = paste0(n.total.miss, " (", round(n.total.miss/n.total*100,1), ")"), 
    total.b.lod = paste0(n.total.b.lod, " (", round(n.total.b.lod/n.total*100, 2), ")"), 
    total.loq.lod = paste0(n.total.loq.lod, " (", round(n.total.loq.lod / n.total*100, 1), ")"), 
    total.good = paste0(n.total.good, " (", round(n.total.good/n.total*100,1), ")"), 
    n.total = as.character(n.total)) %>% 
  select(facility, target, n.total, total.miss, total.b.lod, total.loq.lod, total.good)



# 
# wbe.lod.bio <- wbe.summary.lod %>% 
#   mutate(
#     bio.miss = paste0(round(n.bio.miss/n.bio*100,1), " (", n.bio.miss, " / ", n.bio, ")"), 
#     bio.b.lod = paste0(round(n.bio.b.lod/n.bio*100, 1), " (", n.bio.b.lod, " / ", n.bio, ")"), 
#     bio.loq.lod = paste0(round(n.bio.loq.lod / (n.bio)*100, 1), " (", n.bio.loq.lod, " / ", n.bio, ")"), 
#     bio.good = paste0(round(n.bio.good/(n.bio)*100,1), " (", n.bio.good, " / ", n.bio, ")")) %>% 
#   select(facility, target, bio.miss, bio.b.lod, bio.loq.lod, bio.good)
# 
# 
# 
# 
# 
# wbe.lod.days <- wbe.summary.lod %>% 
#   mutate(
#     days.miss = paste0(round(n.days.miss/n.days*100,1), " (", n.days.miss, " / ", n.days, ")"), 
#     days.b.lod = paste0(round(n.days.b.lod/n.days*100, 1), " (", n.days.b.lod, " / ", n.days, ")"), 
#     days.loq.lod = paste0(round(n.days.loq.lod / (n.days)*100, 1), " (", n.days.loq.lod, " / ", n.days, ")"), 
#     days.good = paste0(round(n.days.good/(n.days)*100,1), " (", n.days.good, " / ", n.days, ")")) %>% 
#   select(facility, target, days.miss, days.b.lod, days.loq.lod, days.good)
# 


wbe.lod.tr %<>% ungroup() %>% add_row(.before = 1) %>% add_row(.before = 1)
wbe.lod.tr[2,] <- as.list(names(wbe.lod.tr))

# wbe.lod.bio %<>% ungroup() %>% add_row(.before = 1) %>% add_row(.before = 1)
# wbe.lod.bio[2,] <- as.list(names(wbe.lod.bio))
# names(wbe.lod.bio) <- names(wbe.lod.tr)
# 
# wbe.lod.days %<>% ungroup() %>% add_row(.before = 1) %>% add_row(.before = 1)
# wbe.lod.days[2,] <- as.list(names(wbe.lod.days))
# names(wbe.lod.days) <- names(wbe.lod.tr)
# 
# lod.tab <- rbind(wbe.lod.tr%>%add_row(), wbe.lod.bio%>%add_row(), wbe.lod.days)



flextable(wbe.lod.tr) %>%
  delete_part(part = "header") %>%
  border_remove() %>%
  border(part = "body", i = 2, border.bottom = fp_border_default(color = "black", width = 2)) %>% 
  
  font(part = "all", fontname = "Arial") %>%
  fontsize(part = "body", size = 8) %>%
  fontsize(part = "body", i = 2, size = 9) %>%
  bold(part = "body", i = 2) %>% 
  
  merge_v(j = 1) %>% 
  
  width(j=1:2, width = 1) %>%
  width(j=3, width = 0.5) %>%
  width(j=4:7, width = 1.25) %>%
  
  align(part = "body", j = 3:7, align = "right") %>%
  align(part = "body", i = 2, align = "center") %>%
  valign(part = "body", j = 1, valign = "top") %>% 
  
  set_caption(caption = "Frequencies and Percentages of Technical Replicates whose Cycle Thresholds are Undetermined, Above the LOD, Between the LOD and LOQ, and Below the LOQ") %>% 
  
  compose(part="body", j=1, i=2, value = as_paragraph("Wastewater Reclamation Facility")) %>% 
  compose(part="body", j=2, i=2, value = as_paragraph("Viral Sequence Target")) %>%
  compose(part="body", j=3, i=2, value = as_paragraph("n")) %>%
  compose(part="body", j=4, i=2, value = as_paragraph("Undetermined")) %>%
  compose(part="body", j=5, i=2, value = as_paragraph("Above LOD")) %>%
  compose(part="body", j=6, i=2, value = as_paragraph("Between LOD and LOQ")) %>%
  compose(part="body", j=7, i=2, value = as_paragraph("Below LOQ"))

```

Table \@ref(tab:limits-table) has some overlap in presented data with Table \@ref(tab:prop-detect); specifically, the frequencies for undetermined results. However, the additional columns show further breakdowns of the cycle threshold values with respect to the LOD and LOQ.   
  
Only `r sum(wbe.summary.lod$n.total.b.lod)` technical replicates with Ct values were above the newly specified limit of detection (Table \@ref(tab:limits-table)).[^limit-interpret-ct]  
  
[^limit-interpret-ct]:Normally, values *below* the limit of detection would be of undetectable. However, since we are referring to the cycle thresholds, the values *above* the limit of detection are of interest. These higher Ct values correspond to lower concentrations of substrate or genetic material within the sample. 

So, after identifying these potential limitation boundaries within the data, we have a value that may be useful in managing the "missingness" / undetermined results: the limit of detection. 
  
  





























\newpage
# Waste (Data) Management {#replacement}
  
  
## Standard Curves and Reaction Efficiency

Output of RT-qPCR is the cycle threshold (Ct) or the number of amplification cycles at which fluorescence intensity is detectable. This value in its own is not directly related to the absolute concentration / quantity of genetic material within the original sample; rather, it is a semi-quantitative value that corresponds to the relative concentrations following serial amplification. In order to convert these cycle threshold values to estimates of the absolute concentration of genetic material within the original sample, we need to calibrate conversion values to represent the implemented RT-qPCR reaction.


### Algebraic Foundations of Amplification
  
As an aside (and more to walk myself through the understanding), the following equations from Kralik and Ricchi (2017) link the original sample concentration ($N_0$) to the concentration after *n* rounds of amplification ($N_n$): 


\begin{equation}
  N_n = N_0 \times (1+E)^n.
  (\#eq:doubling)
\end{equation}


The parameter *E* corresponds to the efficiency of the reaction: for a perfectly efficient reaction, $E=1$ which simplifies the term $(1+E)^n = 2^n$ to reflect the doubling of genetic material for each of *n* rounds of amplification (Equation \@ref(eq:doubling-2)); that is,  

\begin{equation}
  N_n/N_0 = 2^n.
  (\#eq:doubling-2)
\end{equation}

Typically, the samples with known concentrations in ten-fold serial dilutions are used to estimate calibration or standard curves for the reactions; The following represents a simplification of Equation \@ref(eq:doubling-2) using two ten-fold dilutions:

\begin{equation}
  10 = 2^n.
  (\#eq:ten-fold)
\end{equation}

Solving Equation \@ref(eq:ten-fold) yields 

\begin{equation}
  log_2(10) = n \approx 3.322,
  (\#eq:ten-fold-solved)
\end{equation}

showing that under a perfectly efficient doubling reaction, the difference in the number of amplification rounds between any two ten-fold dilutions should be approximately 3.322.    
    
This known value can thusly be used to calculate the reaction's efficiency 

\begin{equation}
  E = 10^{-(\frac{1}{n})}-1.
  (\#eq:efficiency)
\end{equation}

Altogether, this framework can be used to convert RT-qPCR output Ct values to an absolute concentration of genetic material. 



### Fitting Standard Curves

The standard curves are fit to the data using linear regression of cycle threshold on base 10 logarithm of the known quantity of substrate: 

\begin{equation}
  Ct = \beta_0 + \beta_1 log_{10}(quantity) + \epsilon.
  (\#eq:sc-fit)
\end{equation}


```{r standard-curves, echo = F, message=F, warning=F, error=F}
# summary(qc)
# plot(ct~log10(quantity), data=qc[which(qc$target=="N1" & qc$df=="qc1"),])
# lm(ct~log10(quantity), data=qc[which(qc$target=="N1" & qc$df=="qc1"),])
# 
# plot(ct~log10(quantity), data=qc[which(qc$target=="N2" & qc$df=="qc1"),])
# lm(ct~log10(quantity), data=qc[which(qc$target=="N2" & qc$df=="qc1"),])

qc.means <- qc %>% filter(df=="qc1") %>% group_by(target, quantity) %>% summarise(n= n(), mean.ct = mean(ct), sd.ct = sd(ct))

sc.mean.n1 <- lm(mean.ct~log10(quantity), data=qc.means[which(qc.means$target=="N1"),])
sc.mean.n2 <- lm(mean.ct~log10(quantity), data=qc.means[which(qc.means$target=="N2"),])

# i=1
# repeat{
#   cn <- unique(qc$collection_num)[i]
#   
#   cat("\n \n \n")
#   print(cn)
#   
#   cat("\n \n \n N1")
#   try(print(summary(lm(ct~log10(quantity), data=qc[which(qc$target=="N1" & qc$collection_num==cn), ]))))
#   
#   cat("\n \n \n N2")
#   try(print(summary(lm(ct~log10(quantity), data=qc[which(qc$target=="N2" & qc$collection_num==cn), ]))))
#   
#   continue <- readline("Continue?")
#   
#   if(continue==0){break}
#   i = i+1
# }

n1.cn <- 13
n2.cn <- 24



qc.by.target <- split(qc %>% filter(df=="qc1"), f = qc$target[which(qc$df=="qc1")])

qc.by.target.by.collection.number <- lapply(qc.by.target, function(x){split(x, f = x$collection_num)})

qc.standard.curves <- lapply(qc.by.target.by.collection.number, function(x){lapply(x, function(y){lm(ct~log10(quantity), data=y)})})



qc.sc.ests <- lapply(qc.standard.curves, function(x){lapply(x, function(y){c(coef(y), r2=summary(y)$r.squared)})})

qc.sc.ests <- lapply(qc.sc.ests, function(x){bind_rows(x, .id = "CN")}) %>% bind_rows(.id="target")

qc.sc.ests$Efficiency <- 10^(-1/qc.sc.ests$`log10(quantity)`)-1


```


Figure \@ref(fig:sc-plots) shows fit lines for each repetition of the standard curve calibration experiments, highlighting the mean of all fits (grey) and the single results chosen to represent the reaction (black line for fit line, black circles for data points) for both viral sequence targets, N1 (A) and N2 (B). Appendix \@ref(morestandardcurves) has a more extensive description of each of the standard curve fits from the many experimental repetitions (denoted with CN=collection number).



```{r sc-plots, echo=F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap = "RT-qPCR Standard Curves for Viral Sequence Targets, N1 & N2"}

# png(filename = "./consult/03-output/for-pres/standard_curves.png", width = 16*2/5, height = 9*2/5*2, units = "in", res = 300, pointsize = 16)
par(mfcol = c(2,1), mar = c(2.1, 2.1, 1.1, 0))


plot(0,type='n',axes=F, ylim = c(20,40), xlim = c(0,5), xlab = "", ylab = "", main = "")
axis(1, at = 0:5, tick = T, labels = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = "log10(quantity)", line = 1)

axis(2, at = seq(20,40,by=5), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Cycle Threshold", line = 1.25)


abline(sc.mean.n1, lwd = 10, col = "darkgrey")


for(i in 1:length(qc.standard.curves$N1)){
  abline(qc.standard.curves$N1[[i]], lty = 3)
}
abline(qc.standard.curves$N1$`13`, lwd = 4)
points(qc.by.target.by.collection.number$N1$`13`$ct~log10(qc.by.target.by.collection.number$N1$`13`$quantity), cex = 2)
text(par('usr')[2],par('usr')[4],labels = paste("13: Ct =",round(coef(qc.standard.curves$N1$`13`)[1],3), round(coef(qc.standard.curves$N1$`13`)[2],3), "*log10(quantity)"), adj = c(1, 1.2))
text(par('usr')[2],par('usr')[4],labels = paste("Mean: Ct =",round(coef(sc.mean.n1)[1],3), round(coef(sc.mean.n1)[2],3), "*log10(quantity)"), adj = c(1, 2.4))

# title(main = "Standard Curves for N1")
box()
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "A", adj = c(0,1), xpd = T, cex = 1, font = 2)




plot(0,type='n',axes=F, ylim = c(20,40), xlim = c(0,5), xlab = "", ylab = "", main = "")
axis(1, at = 0:5, tick = T, labels = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = "log10(quantity)", line = 1)

axis(2, at = seq(20,40, by=5), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Cyle Threshold", line = 1.25)




abline(sc.mean.n2, lwd = 10, col = "darkgrey")

for(i in 1:length(qc.standard.curves$N2)){
  abline(qc.standard.curves$N2[[i]], lty = 3)
}
abline(qc.standard.curves$N2$`24`, lwd = 4)
points(qc.by.target.by.collection.number$N2$`24`$ct~log10(qc.by.target.by.collection.number$N2$`24`$quantity), cex = 2)
text(par('usr')[2],par('usr')[4],labels = paste("24: Ct =",round(coef(qc.standard.curves$N2$`24`)[1],3), round(coef(qc.standard.curves$N2$`24`)[2],3), "*log10(quantity)"), adj = c(1, 1.2))
text(par('usr')[2],par('usr')[4],labels = paste("Mean: Ct =",round(coef(sc.mean.n2)[1],3), round(coef(sc.mean.n2)[2],3), "*log10(quantity)"), adj = c(1, 2.4))
# title(main = "Standard Curves for N2")
box()
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "B", adj = c(0,1), xpd = T, cex = 1, font = 2)

# dev.off()
```


### Calculating Sample Concentrations using Calibrated Standard Curves and Reaction Dilutions

Now with the fit lines to the standard curve data, we can use the values from the intercepts and the slopes to convert cycle threshold values to the original sample concentration:  

\begin{equation}
  Quantity = 10^{-\frac{Ct - \beta_0}{\beta_1}},
  (\#eq:ct-conversion)
\end{equation}


a rearrangement of Equation \@ref(eq:sc-fit)). Substituting our fit estimates for the intercept and slopes, we have 


\begin{equation}
  \begin{split}
      &Quantity_{N1} = 10^{-\frac{Ct - 34.008}{-3.3890}}, \\
      &Quantity_{N2} = 10^{-\frac{Ct - 32.416}{-3.3084}},
  \end{split}
  (\#eq:ct-seqs)
\end{equation}

for the viral sequence targets, N1 and N2, respectively. The concentrations yielded from Equation \@ref(eq:ct-seqs) would be in units $copies~ per ~\mu L~of~reaction$. We need to take into consideration the dilution scheme (Equation \@ref(eq:dilution)) to have the units be simply $copies~per~\mu L$. 




## Data Adjustment using Limits of Detection and Quantification

It is relatively common to use the limit of detection or some fraction thereof to replace undetermined results within the data. For the purposes of this analysis we will use a generic approach to replace the missing results (or those that fall below the estimated LOD) with half of the LOD value (i.e., LOD / 2). Similarly, we will replace those values that fall between the LOD and LOQ with half of the LOQ value (i.e., LOQ / 2). Figure \@ref(fig:b-and-a) shows the histograms of the natural logarithm of estimated copies per microliter before (panels A and B) and after (panels C and D) replacing the values as previously described. Similar to Figure \@ref(fig:ct-hist), the histograms in panels A and B of Figure \@ref(fig:b-and-a) show that the data deviate from a normal distribution, even after the normalizing natural logarithm transformation. Also similar, we can observe the points of interest within the distribution that indicate limits of detection and quantification (highlighted with the vertical bars). Panels C and D of Figure \@ref(fig:b-and-a) show these same histograms after the data were adjusted using the LODs and LOQs. Note that, due to the extensive missingness, the replaced values dominate the distributions.

```{r, echo = F, message=F, warning=F, error=F}
n1.int <- qc.sc.ests$`(Intercept)`[which(qc.sc.ests$target=="N1" & qc.sc.ests$CN==n1.cn)]
n1.slope <- qc.sc.ests$`log10(quantity)`[which(qc.sc.ests$target=="N1" & qc.sc.ests$CN==n1.cn)]
n2.int <- qc.sc.ests$`(Intercept)`[which(qc.sc.ests$target=="N2" & qc.sc.ests$CN==n2.cn)]
n2.slope <- qc.sc.ests$`log10(quantity)`[which(qc.sc.ests$target=="N2" & qc.sc.ests$CN==n2.cn)]

wbe %<>% 
  mutate(copies_per_uL_rxn = ifelse(target=="N1", 
                                           10^(-(n1.int-ct)/n1.slope), 
                                           10^(-(n2.int-ct)/n2.slope)
                                           )
                ) %>%
  mutate(copies_per_uL = copies_per_uL_rxn*20/2*25/3*60)      

n1.lod.copies_per_uL <- 10^(-(n1.int-qqnorm.ct.n1$lod)/n1.slope) * 20/2*25/3*60
n1.loq.copies_per_uL <- 10^(-(n1.int-qqnorm.ct.n1$loq)/n1.slope) * 20/2*25/3*60
n2.lod.copies_per_uL <- 10^(-(n2.int-qqnorm.ct.n2$lod)/n2.slope) * 20/2*25/3*60
n2.loq.copies_per_uL <- 10^(-(n2.int-qqnorm.ct.n2$loq)/n2.slope) * 20/2*25/3*60



# Scenario 1 -- raw data
## leave missings out, calculate correlations with positive results only
## leave all values between LOD and LOQ as is







# Scenario 2 -- simple replacement
## use LOD/2 to replace missings
## use LOQ/2 to replace values between LOD and LOQ


wbe2 <- wbe %>% 
          mutate(copies_per_uL = ifelse(target == "N1", 
                                        ifelse(
                                          copies_per_uL < n1.lod.copies_per_uL | is.na(copies_per_uL), 
                                          n1.lod.copies_per_uL/2, 
                                          ifelse(
                                            copies_per_uL < n1.loq.copies_per_uL, 
                                            n1.loq.copies_per_uL/2,
                                            copies_per_uL
                                          )),
                                        ifelse(
                                          copies_per_uL < n2.lod.copies_per_uL | is.na(copies_per_uL), 
                                          n2.lod.copies_per_uL/2,
                                          ifelse(
                                            copies_per_uL < n2.loq.copies_per_uL, 
                                            n2.loq.copies_per_uL/2,
                                            copies_per_uL
                                          )
                                        )
                                  )
                 )

```


(ref:copy-hists) Histograms of the Natural Logarithm of Sample Viral Sequence Copy Concentrations. Panels A and B show the histograms for N1 and N2, respectively, before any adjustments were made. Panels C and D show the histograms for N1 and N2, respectively, after values which were either undetermined or fell below the LOD were replaced with half of the LOD and, similarly, values which fell between the LOD and LOQ were replaced with half of the LOQ.


```{r b-and-a, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap = "(ref:copy-hists)"}
# png(filename = "./consult/03-output/for-pres/replacing_with_limits.png", width = 16*3/5, height = 9*3/5*2, units = "in", res = 300, pointsize = 16)
par(mfrow = c(2,2), mar = c(2.1,2.1,1.1,0.1))


hist(log(wbe$copies_per_uL[which(wbe$target=="N1")]), breaks = 50, axes = F, xlab = "", ylab = "", main = "", xlim = c(4,10), ylim = c(0,40))
abline(v=log(n1.lod.copies_per_uL), lwd = 2)
abline(v=log(n1.loq.copies_per_uL), lwd = 0.5)
abline(v=log(n1.lod.copies_per_uL/2), lwd = 2, lty = 3)
abline(v=log(n1.loq.copies_per_uL/2), lwd = 0.5, lty = 3)
# abline(v=log(n1.lod.copies_per_uL/sqrt(2)), lwd = 2, lty = 5)
# abline(v=log(n1.loq.copies_per_uL/sqrt(2)), lwd = 0.5, lty = 5)
axis(1, at = 4:10, labels = T, tick = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = expression(paste("log(Copies per ", mu, "L)")), line = 1)
axis(2, at = seq(0,40,by=10), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Frequency", line = 1.25)
legend("topright", lwd = c(2,0.5,2,0.5), lty = c(1,1,3,3), legend = c(expression(LOD[~N1]), expression(LOQ[~N1]), expression(frac(1,2) %*% LOD[~N1]),expression(frac(1,2) %*% LOQ[~N1])), bty = "n", y.intersp = 1.25, cex = 0.7)
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "A", adj = c(0,1), xpd = T, cex = 1, font = 2)




hist(log(wbe$copies_per_uL[which(wbe$target=="N2")]), breaks = 50, axes = F, xlab = "", ylab = "", main = "", xlim = c(4,10), ylim = c(0,40))
abline(v=log(n2.lod.copies_per_uL), lwd = 2)
abline(v=log(n2.loq.copies_per_uL), lwd = 0.5)
abline(v=log(n2.lod.copies_per_uL/2), lwd = 2, lty = 3)
abline(v=log(n2.loq.copies_per_uL/2), lwd = 0.5, lty = 3)
# abline(v=log(n2.lod.copies_per_uL/sqrt(2)), lwd = 2, lty = 5)
# abline(v=log(n2.loq.copies_per_uL/sqrt(2)), lwd = 0.5, lty = 5)
axis(1, at = 4:10, labels = T, tick = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = expression(paste("log(Copies per ", mu, "L)")), line = 1)
axis(2, at = seq(0,40,by=10), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Frequency", line = 1.25)
legend("topright", lwd = c(2,0.5,2,0.5), lty = c(1,1,3,3), legend = c(expression(LOD[~N2]), expression(LOQ[~N2]), expression(frac(1,2) %*% LOD[~N2]),expression(frac(1,2) %*% LOQ[~N2])), bty = "n", y.intersp = 1.25, cex = 0.7)
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "B", adj = c(0,1), xpd = T, cex = 1, font = 2)






hist(log(wbe2$copies_per_uL[which(wbe2$target=="N1")]), breaks = 50, axes = F, xlab = "", ylab = "", main = "", xlim = c(4,10))
abline(v=log(n1.lod.copies_per_uL), lwd = 2)
abline(v=log(n1.loq.copies_per_uL), lwd = 0.5)
abline(v=log(n1.lod.copies_per_uL/2), lwd = 2, lty = 3)
abline(v=log(n1.loq.copies_per_uL/2), lwd = 0.5, lty = 3)
# abline(v=log(n1.lod.copies_per_uL/sqrt(2)), lwd = 2, lty = 5)
# abline(v=log(n1.loq.copies_per_uL/sqrt(2)), lwd = 0.5, lty = 5)
axis(1, at = 4:10, labels = T, tick = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = expression(paste("log(Copies per ", mu, "L)")), line = 1)
axis(2, at = seq(0,800,by=100), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Frequency", line = 1.25)
legend("topright", lwd = c(2,0.5,2,0.5), lty = c(1,1,3,3), legend = c(expression(LOD[~N1]), expression(LOQ[~N1]), expression(frac(1,2) %*% LOD[~N1]),expression(frac(1,2) %*% LOQ[~N1])), bty = "n", y.intersp = 1.25, cex = 0.7)
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "C", adj = c(0,1), xpd = T, cex = 1, font = 2)




hist(log(wbe2$copies_per_uL[which(wbe2$target=="N2")]), breaks = 50, axes = F, xlab = "", ylab = "", main = "", xlim = c(4,10))
abline(v=log(n2.lod.copies_per_uL), lwd = 2)
abline(v=log(n2.loq.copies_per_uL), lwd = 0.5)
abline(v=log(n2.lod.copies_per_uL/2), lwd = 2, lty = 3)
abline(v=log(n2.loq.copies_per_uL/2), lwd = 0.5, lty = 3)
# abline(v=log(n2.lod.copies_per_uL/sqrt(2)), lwd = 2, lty = 5)
# abline(v=log(n2.loq.copies_per_uL/sqrt(2)), lwd = 0.5, lty = 5)
axis(1, at = 4:10, labels = T, tick = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = expression(paste("log(Copies per ", mu, "L)")), line = 1)
axis(2, at = seq(0,800,by=100), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Frequency", line = 1.25)
legend("topright", lwd = c(2,0.5,2,0.5), lty = c(1,1,3,3), legend = c(expression(LOD[~N2]), expression(LOQ[~N2]), expression(frac(1,2) %*% LOD[~N2]),expression(frac(1,2) %*% LOQ[~N2])), bty = "n", y.intersp = 1.25, cex = 0.7)
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "D", adj = c(0,1), xpd = T, cex = 1, font = 2)

# dev.off()

```




























\newpage
# Wiping away Convolution {#deconvolution}

There are certain inherent flaws within most, if not all, infectious disease surveillance systems. Perhaps the most well-known is the discrepancy between the times a case is reported through the surveillance system and the times that person is potentially infectious and transmitting. These discrepancies leave public health professionals with an imperfect understanding of the true epidemic curve. 

The data available for COVID-19 cases for Georgia attempt to overcome this issue. In addition to the more widely available time series of cases by date of report, the Georgia Department of Public Health has also included time series of cases by the date of symptom onset. This subtle difference has a meaningful rationale as it would better demarcate the times at which the cases were potentially infectious and contributing to transmission. Similarly, this would give us a better glimpse at the times at which cases were shedding into their environment. However, the data documentation indicates that the dates correspond to the date of symptom onset only when that information is available and, otherwise, refer to dates of positive specimen collection. A date of positive specimen collection may still be preferable to report date, but it is likely there still exists some discrepancy with that of the true transmissible period.

Figure \@ref(fig:epi-curve) shows the epidemic curve for Athens-Clarke County with cases by dates of report and symptom onset, and PCR positive tests by date of specimen collection. These data are complete from `r format(min(covid$date), "%d %B %Y")` to `r format(max(covid$date), "%d %B %Y")` and have records for cases totaling `r sum(covid$cases.reported)`, `r sum(covid$cases.symptom.onset)`, and `r sum(covid$pcr_pos)`, respectively for cases by report date, symptom onset date, and date of specimen collection.    
    
The difference in the curves is slight, but left shifts are notable. That is, the curve for cases by report date lags behind the curve for PCR positive cases by date of specimen collection which, in turn, lags behind the curve for cases by symptom onset date. 


```{r epi-curve1, eval=F, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap = "Epidemic Curve of COVID-19 in Athens-Clarke County, GA, USA"}

date.labels <- seq(min(covid$date), max(covid$date), by="months")

at.points <- seq(0.5, nrow(covid)-0.5, by=1)[which(covid$date%in%date.labels)]

at.points2 <- seq(0.5, nrow(covid)-0.5, by=1)[which(covid$date%in%c(min(wbe$sample_date), max(wbe$sample_date)))]

layout(matrix(c(1,2,1,3), nrow = 2), widths = c(1,1), heights = c(3,1))

par(mar=c(2.1,3.1,2.1,1.1))
# barplot(cases.reported~date, data=covid)
barplot(cases.reported~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, col = "gainsboro", border = "gainsboro")
axis(1, at=at.points, tick=T, labels = paste(format(date.labels, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -1)
axis(2, labels = T, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(ylab = "Cases", line = 2)
axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7)
axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)


barplot(cases.symptom.onset~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, add = T, col = viridis::viridis(100, alpha = 0.75)[55], border = viridis::viridis(100, alpha = 0.75)[55])

legend("topleft", fill = c("gainsboro", viridis::viridis(100, alpha = 0.75)[55]), legend = c("Reported", "Symptom Onset"))
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "A", adj = c(0,1), xpd = T, cex = 1, font = 2)


par(mar = c(2.1, 1.1, 1.1, 1.1))
barplot(cases.reported~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, col = "black", ylim = c(0, 300))
axis(1, at=at.points, tick=T, labels = paste(format(date.labels, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -2, tck = -0.02)
axis(2, labels = F, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(xlab = "Date of Report", line = 0.75)
# axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7)
# axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "B", adj = c(0,1), xpd = T, cex = 1, font = 2)



barplot(cases.symptom.onset~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, col = viridis::viridis(100, alpha = 0.75)[55], border = viridis::viridis(100, alpha = 0.75)[55], ylim = c(0, 300))
axis(1, at=at.points, tick=T, labels = paste(format(date.labels, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -2, tck = -0.02)
axis(2, labels = F, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(xlab = "Date of Symptom Onset", line = 0.75)
# axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7, hadj = 1)
# axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "C", adj = c(0,1), xpd = T, cex = 1, font = 2)


```






```{r epi-curve2, eval = F, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap = "Epidemic Curve of COVID-19 in Athens-Clarke County, GA, USA"}

date.labels <- seq(min(covid$date), max(covid$date), by="months")

at.points <- seq(0.5, nrow(covid)-0.5, by=1)[which(covid$date%in%date.labels)]

at.points2 <- seq(0.5, nrow(covid)-0.5, by=1)[which(covid$date%in%c(min(wbe$sample_date), max(wbe$sample_date)))]

par(mar=c(2.1,3.6,2.1,1.1), mfrow = c(2,1))
# # barplot(cases.reported~date, data=covid)
# barplot(cases.reported~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, col = "gainsboro", border = "gainsboro")
# axis(1, at=at.points, tick=T, labels = paste(format(date.labels, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -1)
# axis(2, labels = T, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
# title(ylab = "Cases", line = 2)
# axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7)
# axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)
# 
# 
# barplot(cases.symptom.onset~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, add = T, col = viridis::viridis(100, alpha = 0.75)[55], border = viridis::viridis(100, alpha = 0.75)[55])
# 
# legend("topleft", fill = c("gainsboro", viridis::viridis(100, alpha = 0.75)[55]), legend = c("Reported", "Symptom Onset"))
# text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
#      par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
#      labels = "A", adj = c(0,1), xpd = T, cex = 1, font = 2)


# par(mar = c(2.1, 1.1, 1.1, 1.1))
barplot(cases.reported~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, col = "black", ylim = c(0, 300))
axis(1, at=at.points, tick=T, labels = paste(format(date.labels, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -2, tck = -0.02)
axis(2, labels = F, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(xlab = "Date of Report", line = 1)
# axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7)
# axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)
# text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
#      par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
#      labels = "B", adj = c(0,1), xpd = T, cex = 1, font = 2)


axis(2, labels = T, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(ylab = "Cases", line = 2)
axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7)
axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)

abline(v = at.points, lty = 3, col = "gainsboro", xpd = T)

text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "A", adj = c(0,1), xpd = T, cex = 1, font = 2)



barplot(cases.symptom.onset~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, col = viridis::viridis(100, alpha = 1)[55], border = viridis::viridis(100, alpha = 1)[55], ylim = c(0, 300))
axis(1, at=at.points, tick=T, labels = paste(format(date.labels, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -2, tck = -0.02)
axis(2, labels = F, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(xlab = "Date of Symptom Onset", line = 1)
# axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7, hadj = 1)
# axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "B", adj = c(0,1), xpd = T, cex = 1, font = 2)
axis(2, labels = T, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(ylab = "Cases", line = 2)

abline(v = at.points, lty = 3, col = "gainsboro", xpd = T)
# legend("topleft", fill = c("gainsboro", viridis::viridis(100, alpha = 0.75)[55]), legend = c("Reported", "Symptom Onset"))
```







```{r epi-curve, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap = "Epidemic Curve of COVID-19 in Athens-Clarke County, GA, USA"}

date.labels <- seq(min(covid$date), max(covid$date), by="months")

at.points <- seq(0.5, nrow(covid)-0.5, by=1)[which(covid$date%in%date.labels)]

at.points2 <- seq(0.5, nrow(covid)-0.5, by=1)[which(covid$date%in%c(min(wbe$sample_date), max(wbe$sample_date)))]




# png(filename = "./consult/03-output/for-pres/epi-curves.png", width = 16, height = 9, units = "in", res = 300, pointsize = 16)
par(mar=c(2.1,3.6,2.1,1.1), mfrow = c(3,1))






barplot(cases.symptom.onset~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, col = viridis::viridis(100, alpha = 1)[1], border = viridis::viridis(100, alpha = 1)[1], ylim = c(0, 300))
axis(1, at=at.points, tick=T, labels = paste(format(date.labels, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -2, tck = -0.02)
axis(2, labels = F, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(xlab = "Date of Symptom Onset", line = 1)
# axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7, hadj = 1)
# axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "A", adj = c(0,1), xpd = T, cex = 1, font = 2)
axis(2, labels = T, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(ylab = "Cases", line = 2)

abline(v = at.points, lty = 3, col = "grey60", xpd = T)
# legend("topleft", fill = c("gainsboro", viridis::viridis(100, alpha = 0.75)[55]), legend = c("Reported", "Symptom Onset"))

axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7)
axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)



legend("topleft", fill = c(viridis::viridis(100, alpha = 1)[1], viridis::viridis(100, alpha = 1)[50], viridis::viridis(100, alpha = 1)[100]), border = c(viridis::viridis(100, alpha = 1)[1], viridis::viridis(100, alpha = 1)[50], viridis::viridis(100, alpha = 1)[100]), legend = c("Symptom Onset", "Specimen Collection", "Reported"))













barplot(pcr_pos~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, col = viridis::viridis(100, alpha = 1)[50], border = viridis::viridis(100, alpha = 1)[50], ylim = c(0, 300))
axis(1, at=at.points, tick=T, labels = paste(format(date.labels, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -2, tck = -0.02)
axis(2, labels = F, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(xlab = "Date of Specimen Collection", line = 1)
# axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7, hadj = 1)
# axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "B", adj = c(0,1), xpd = T, cex = 1, font = 2)
axis(2, labels = T, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(ylab = "Cases", line = 2)

abline(v = at.points, lty = 3, col = "grey60", xpd = T)
# legend("topleft", fill = c("gainsboro", viridis::viridis(100, alpha = 0.75)[55]), legend = c("Reported", "Symptom Onset"))







# par(mar = c(2.1, 1.1, 1.1, 1.1))
barplot(cases.reported~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, col = viridis::viridis(100, alpha = 1)[100], border = viridis::viridis(100, alpha = 1)[100], ylim = c(0, 300))
axis(1, at=at.points, tick=T, labels = paste(format(date.labels, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -2, tck = -0.02)
axis(2, labels = F, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(xlab = "Date of Report", line = 1)


axis(2, labels = T, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(ylab = "Cases", line = 2)

abline(v = at.points, lty = 3, col = "grey60", xpd = T)




text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "C", adj = c(0,1), xpd = T, cex = 1, font = 2)





# dev.off()
```






















## Comparing Deconvoluted Case Counts by Report Date to Case Counts By Symptom Onset Date 

The case counts by report date are essentially a *convolution* of the underlying true epidemic curve and the distribution of times between true and reported infection dates. If we have an understanding of the distribution of these delays (i.e., how likely a delay of *n* amount of time is), then we can use a method of deconvolution to attempt to remove the delays from the data. The `incidental` package of R [@R-incidental] uses an empirical Bayes estimation method to accomplish this. Furthermore, the package also has a given delay distribution for COVID-19 (Figure \@ref(fig:delay-dist)). This delay distribution was estimated using information on the distribution of the incubation period from [Lauer et al, 2020](https://www.acc.org/latest-in-cardiology/journal-scans/2020/05/11/15/18/the-incubation-period-of-coronavirus-disease) and reporting delays after symptom onset from [Florida case listings](https://open-fdoh.hub.arcgis.com/datasets/florida-covid19-case-line-data). I was unable to find any associated peer-reviewed publications using this delay distribution. 

```{r delay-dist, echo = F, message=F, warning=F, error=F, fig.cap = "Incidental Package Delay Distribution for COVID-19"}

# png(filename = "./consult/03-output/for-pres/delay_dist.png", width = 16*2/5, height = 9*2/5, units = "in", res = 300, pointsize = 16)
par(mar = c(4.1, 4.1, 1.1, 1.1), mfrow = c(1,1))
plot(incidental::covid_delay_dist$days, incidental::covid_delay_dist$case, type = "l", xlab = "Days of Delay", ylab = "Probability")
# lines(incidental::covid_delay_dist$days, incidental::covid_delay_dist$hospitalization, lty = 5)

arrows(x0 = which.max(incidental::covid_delay_dist$case), x1 = which.max(incidental::covid_delay_dist$case), y0 = par('usr')[4], y1 = incidental::covid_delay_dist$case[which.max(incidental::covid_delay_dist$case)]+0.0005, length = par('pin')[2]*0.01)


# text(x = par('usr')[2]-0.24*par('usr')[2], y = par('usr')[4]-0.2*par('usr')[4], labels = paste(c("Days", incidental::covid_delay_dist$days[which(incidental::covid_delay_dist$case!=0)]), collapse = "\n"), cex = 0.5, adj = c(0, 1))
# 
# text(x = par('usr')[2]-0.12*par('usr')[2], y = par('usr')[4]-0.2*par('usr')[4], labels = paste(c("Probability (%)", round(100*incidental::covid_delay_dist$case[which(incidental::covid_delay_dist$case!=0)], 3)), collapse = "\n"), cex = 0.5, adj = c(1, 1))
# 
# text(x = par('usr')[2], y = par('usr')[4]-0.2*par('usr')[4], labels = paste(c("Cumulative P (%)", round(100*cumsum(incidental::covid_delay_dist$case[which(incidental::covid_delay_dist$case!=0)]), 3)), collapse = "\n"), cex = 0.5, adj = c(1, 1))


lines(incidental::covid_delay_dist$days, cumsum(incidental::covid_delay_dist$case)*par('usr')[4], lwd = 2, lty = 5)

legend("topright", lty = c(1,5), lwd = c(1,2), legend = c("PMF", "CDF"))

# dev.off()
```


The estimation procedure for the deconvoluted incidence curve includes the fitting of splines to the convoluted data using Poisson basis functions for a regularized Poisson likelihood function. Therefore, it is necessary to specify the number of knots for the splines fits and a hyperparameter, $\lambda$, for the regularization. A vector of potential values is explored for both the numbers of knots and the hyperparameter and the algorithm used by the `incidental` package selects the best performing vales based on AIC for spline knots and validation likelihood for lambda. Additional information on the `incidental` package may be found on its [CRAN webpage](https://CRAN.R-project.org/package=incidental) or within the built-in help documentation `?incidental`.

```{r, echo=F, message=F, warning=F, error=F}
# dof_grid_try = seq(30, 50, 1)
# lam_grid_try = 10^(seq(-0.5, -10, length.out = 30))
# 
# 
# decon.report <- incidental::fit_incidence(covid$cases.reported, 
#                                           incidental::covid_delay_dist$case, 
#                                           dof_grid = dof_grid_try, 
#                                           lam_grid = lam_grid_try) 

# save(decon.report, file = "./consult/01-data/decon_report.rds")
load("./consult/01-data/decon_report.rds")
# decon.symptom <- incidental::fit_incidence(covid$cases.symptom.onset, 
#                                            incidental::covid_delay_dist$case, 
#                                            dof_grid = dof_grid_try, 
#                                            lam_grid = lam_grid_try)


covid$cases.reported.7dma <- zoo::rollmean(covid$cases.reported, k = 7, fill = NA, align = "right")
covid$cases.symptom.onset.7dma <- zoo::rollmean(covid$cases.symptom.onset, k = 7, fill = NA, align = "right")
covid$pcr.pos.7dma <- zoo::rollmean(covid$pcr_pos, k = 7, fill = NA, align = "right")


covid$cases.reported.17dsum <- zoo::rollsum(covid$cases.reported, k = 17, fill = NA, align = "right")
covid$cases.symptom.onset.17dsum <- zoo::rollsum(covid$cases.symptom.onset, k = 17, fill = NA, align = "right")
covid$pcr.pos.17dsum <- zoo::rollsum(covid$pcr_pos, k = 17, fill = NA, align = "right")

covid$cases.reported.con <- decon.report$Chat
covid$cases.reported.decon <- decon.report$Ihat

covid$cases.reported.decon.17dsum <- zoo::rollsum(covid$cases.reported.decon, k = 17, fill = NA, align = "right")
```

Figure \@ref(fig:epi-curve-decon) shows the estimated incidence curves from the deconvolution. The deconvoluted incidence curve does seem fairly well matched with the epidemic curve for symptom onset. The deconvoluted incidence curve apprears to be shifted to the left of the symptom onset data, as expected. Peak levels in the deconvoluted curve are observed to fall below the peaks for the reported case counts. This could be attributed in part to the correlation between the mean and variance in case counts under the Poisson distribution. Moreover, the spline model will tend to smooth the data and, hence, lead to lower peak levels.  Each dataset of case counts time series and the deconvolution estimates will be explored in its association with the RT-qPCR data. 

```{r epi-curve-decon, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap = "Comparison of Epidemic Curves from Dates of Report and Symptom Onset to a Deconvoluted Incidence Curve"}

# png(filename = "./consult/03-output/for-pres/deconvolution.png", width = 16*3/5, height = 9*3/5, units = "in", res = 300, pointsize = 16)

par(mar=c(3.1,3.1,2.1,1.1))
# barplot(cases.reported~date, data=covid)
barplot(cases.reported~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, col = "gainsboro", border = "gainsboro")
axis(1, at=at.points, tick=T, labels = paste(format(date.labels, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -1)
axis(2, labels = T, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(xlab = "Date", ylab = "Cases", line = 2)
axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7)
axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)

barplot(cases.symptom.onset~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, add = T, col = viridis::viridis(100, alpha = 0.25)[1], border = viridis::viridis(100, alpha = 0.25)[1])


lines(seq(0.5, nrow(covid)-0.5, by=1), decon.report$Chat, lty = 3, lwd = 2)
lines(seq(0.5, nrow(covid)-0.5, by=1), decon.report$Ihat, lty = 5, lwd = 4)

# lines(seq(0.5, nrow(covid)-0.5, by=1), covid$cases.reported.7dma)
# lines(seq(0.5, nrow(covid)-0.5, by=1), covid$cases.symptom.onset.7dma)


legend("topleft", 
       bty = 'n',
       pt.bg = c("gainsboro", viridis::viridis(100, alpha = 0.25)[1], NA, NA), 
       pt.cex = 2,
       pch = c(22,22,NA,NA),
       lty = c(0, 0, 3, 5), 
       lwd = c(0, 0, 1, 2), 
       col = c("gainsboro", viridis::viridis(100, alpha = 0.25)[1], "black", "black"), 
       legend = c("Reported", "Symptom Onset", "Fitted Convolution", "Estimated Deconvolution"))

# dev.off()

```




```{r epi-curve-decon-ml, eval = F, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap = "Comparison of Epidemic Curves from Dates of Report and Symptom Onset to a Deconvoluted Incidence Curve"}


png(filename = "./consult/03-output/epi-curves-deconvolution.png", width = 16, height = 9, res = 300, unit = "in", pointsize = 16)
par(mar=c(3.1,3.1,2.1,1.1))
# barplot(cases.reported~date, data=covid)
barplot(cases.reported~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, col = "gainsboro", border = "gainsboro")
axis(1, at=at.points, tick=T, labels = paste(format(date.labels, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -1)
axis(2, labels = T, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(xlab = "Date", ylab = "Cases", line = 2)
axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7)
axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)

barplot(cases.symptom.onset~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, add = T, col = viridis::viridis(100, alpha = 0.25)[55], border = viridis::viridis(100, alpha = 0.25)[55])


lines(seq(0.5, nrow(covid)-0.5, by=1), decon.report$Chat, lty = 3, lwd = 2)
lines(seq(0.5, nrow(covid)-0.5, by=1), decon.report$Ihat, lty = 5, lwd = 4)

# lines(seq(0.5, nrow(covid)-0.5, by=1), covid$cases.reported.7dma)
# lines(seq(0.5, nrow(covid)-0.5, by=1), covid$cases.symptom.onset.7dma)


legend("topleft", 
       bty = 'n',
       pt.bg = c("gainsboro", viridis::viridis(100, alpha = 0.25)[55], NA, NA), 
       pt.cex = 2,
       pch = c(22,22,NA,NA),
       lty = c(0, 0, 3, 5), 
       lwd = c(0, 0, 1, 2), 
       col = c("gainsboro", viridis::viridis(100, alpha = 0.25)[55], "black", "black"), 
       legend = c("Reported", "Symptom Onset", "Fitted Convolution", "Estimated Deconvolution"))

dev.off()

```
































\newpage
# It's All Clumping Together... {#correlation}

The ultimate goal of this project was to assess the predictability of COVID-19 cases given the wastewater-based epidemiological surveillance for SARS-CoV-2. So, we will explore the associations between the case counts and the viral loads in wastewater samples. Since we have a hierarchical dataset for the RT-qPCR results, we can take a couple different approaches with the data. We could use the data at any one of the hierarchy levels, but as the unit of analysis gets smaller, the model complexity increases where at the technical replicate level we would likely need to control for the correlation among replicates of the same sample and location. If we were to take a meaningful summary of RT-qPCR results for every given sampling time, then we would effectively remove the additional sources correlation among observations. The correlations among the different replicates will impact the uncertainty but will not impact averages. Furthermore, since the data are time series, we should address potential autocorrelation in the time of the samples. Of note, this approach of summarization to a single viral load estimate per sampling date could contribute to a loss of information from those lower levels of data, but this may be minimal given an appropriate summarization. 


## Determining an Appropriate Summarization Scheme

As can be seen in Figure \@ref(fig:b-and-a), the distributions of the two viral sequence targets are quite different. To further show the discrepancies in N1 and N2 assays, Figure \@ref(fig:copy-profiles) shows the time-series of estimated copies in the wastewater for both N1 and N2 separately. The time-series show two stark contrasts: (1) the timing of the peaks and valleys and (2) the relative magnitudes. The differences in the relative magnitudes is due to a combination of the differences in the fits of the standard curves (Section \@ref(replacement)) and the differences in the estimated limits of detection (Section \@ref(missingness)). The differences in the timing of the peaks and valleys is a bit more difficult to rationalize. For any reason, it still stands that the results are substantially heterogeneous. 
  
  
```{r copy-profiles, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap = "Profiles of Estimated Viral Loads in Wastewater for both Viral Sequence Targets used in RT-qPCRs"}
copy.profiles <- wbe2 %>% 
  group_by(sample_date, facility, biological_replicate, target) %>% 
  summarise(gmean.copiespul = exp(mean(log(copies_per_uL), na.rm = T))) %>% 
  group_by(sample_date, facility, target) %>% 
  summarise(gmean.copiespul = exp(mean(log(gmean.copiespul)))) %>% 
  tidyr::pivot_wider(names_from = c(facility,target), values_from = gmean.copiespul) %>% 
  mutate(N1 = prod(c(CC_N1, MI_N1, NO_N1))^(1/3), N2 = prod(c(CC_N2, MI_N2, NO_N2))^(1/3))


par(mfrow = c(2,1), mar = c(2.1, 3.1, 1.1, 1.1))

plot(copy.profiles$sample_date, copy.profiles$CC_N1, type = "l", yaxt = 'n', ylab = "")
lines(copy.profiles$sample_date, copy.profiles$MI_N1, lty = 3)
lines(copy.profiles$sample_date, copy.profiles$NO_N1, lty = 5)
lines(copy.profiles$sample_date, copy.profiles$N1, lwd = 4)
axis(2, at = seq(500,3500,by=500), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = expression(paste("Copies per ", mu, "L")), line = 1.5)
legend("topleft", lty = c(1,3,5,1), lwd = c(1,1,1,4), legend = c("WRF - CC", "WRF - MI", "WRF - NO", "Daily Average"), cex = 0.7)
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "A - N1", adj = c(0,1), xpd = T, cex = 1, font = 2)

plot(copy.profiles$sample_date, copy.profiles$MI_N2, type = "l", yaxt = 'n', ylab = "", lty = 3)
lines(copy.profiles$sample_date, copy.profiles$CC_N2, lty = 1)
lines(copy.profiles$sample_date, copy.profiles$NO_N2, lty = 5)
lines(copy.profiles$sample_date, copy.profiles$N2, lwd = 4)
axis(2, at = seq(100,500,by=100), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = expression(paste("Copies per ", mu, "L")), line = 1.5)
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "B - N2", adj = c(0,1), xpd = T, cex = 1, font = 2)
```

  
It is also evident in Figure \@ref(fig:copy-profiles) that the concentration time-series differ among the wastewater reclamation facilities. Therefore, it may be necessary to revisit the earlier analyses to address these discrepancies. 
  
However, for the sake of simplicity and consistency, the data will be aggregated to a single measure of viral copies per sampling day in a joint-evaluation of their association with case incidence.    
  
  
## Averaging Technical and Biological Replicates

For any given sampling date and any given wastewater reclamation facility, there are typically three separate wastewater samples (i.e., biological replicates) each of which are used as a source for typically six separate runs (i.e., three technical replicates for both viral sequence targets) of the RT-qPCR. So, to summarize the results for samples from a single day at a single facility, we must average the technical replicates and biological replicates.     
  
The choice of measure of centrality (read average) is not trivial and may have large implications in the analysis. This is explored a bit in Appendix \@ref(means). In the remainder of this report, geometric means will be used to summarize the technical replicates and, subsequently, the biological replicates.    
    
Next, we can aggregate data across facilities so as to account for the total influent water / sewage volume and the total influent suspended solids concentrations. Here, we multiply the averaged viral loads (concentrations with units $viral~copies~per~\mu~L$) by the total volume of wastewater to estimate the total number of viral copies present within the wastewater (assuming a homogenous concentration). These total viral copies data for each wastewater reclamation facility are then summed across the three facilities to yield a single value for each sampling date.       


```{r, echo = F, message=F, warning=F, error=F}


wbe2.wide <- wbe2 %>% 
      group_by(sample_date, facility, target, biological_replicate) %>%
      summarise(copies_per_uL = exp(mean(log(copies_per_uL))), 
                n.miss = sum(is.na(ct)), 
                n = n()
                ) %>%
      ungroup() %>%
      group_by(sample_date, facility, target) %>%
      summarise(copies_per_uL = exp(mean(log(copies_per_uL))), 
                n.miss = sum(n.miss), 
                n = sum(n)
                ) %>%
      ungroup() %>%
      left_join(., plant, by = c("sample_date"="date", "facility"="wrf")) %>%
      mutate(copies = copies_per_uL * 1e6 * influent_flow_L) %>%
      pivot_wider(names_from = c(facility, target), values_from = c(copies_per_uL, copies, influent_flow_L, influent_tss_mg_l, n.miss, n)) %>%
      full_join(covid, by = c("sample_date"="date"))

wbe2.wide <- wbe2.wide[,-which(grepl("^influent.*?N2$", names(wbe2.wide)))]
names(wbe2.wide)[which(grepl("^influent.*?N1$", names(wbe2.wide)))] <- gsub("_N1", "", names(wbe2.wide)[which(grepl("^influent.*?N1$", names(wbe2.wide)))])



wbe2.date <- wbe2 %>% 
      group_by(sample_date, facility, biological_replicate, target) %>%
      summarise(copies_per_uL = exp(mean(log(copies_per_uL))), 
                n.miss = sum(is.na(ct)), 
                n = n()) %>%
      ungroup() %>%
      group_by(sample_date, facility, biological_replicate) %>%
      summarise(copies_per_uL = exp(mean(log(copies_per_uL))), 
                n.miss = sum(n.miss), 
                n = sum(n)
                ) %>%
      ungroup() %>%
      group_by(sample_date, facility) %>%
      summarise(copies_per_uL = exp(mean(log(copies_per_uL))), 
                n.miss = sum(n.miss), 
                n = sum(n)
                ) %>%
      ungroup() %>%
      left_join(., plant, by = c("sample_date"="date", "facility"="wrf")) %>%
      mutate(copies = copies_per_uL * 1e6 * influent_flow_L) %>%
      group_by(sample_date) %>%
      summarise(copies_per_uL = exp(mean(log(copies_per_uL))), copies = sum(copies), n.miss = sum(n.miss), n = sum(n)) %>%
      ungroup() %>% 
      full_join(covid, by = c("sample_date"="date"))

wbe2.facility <- wbe2 %>% 
      group_by(sample_date, facility, biological_replicate, target) %>%
      summarise(copies_per_uL = exp(mean(log(copies_per_uL))), 
                n.miss = sum(is.na(ct)), 
                n = n()
                ) %>%
      ungroup() %>%
      group_by(sample_date, facility, biological_replicate) %>%
      summarise(copies_per_uL = exp(mean(log(copies_per_uL))), 
                n.miss = sum(n.miss), 
                n = sum(n)
                ) %>%
      ungroup() %>%
      group_by(sample_date, facility) %>%
      summarise(copies_per_uL = exp(mean(log(copies_per_uL))), 
                n.miss = sum(n.miss), 
                n = sum(n)
                ) %>%
      ungroup() %>%
      left_join(., plant, by = c("sample_date"="date", "facility"="wrf")) %>%
      mutate(copies = copies_per_uL * 1e6 * influent_flow_L) %>%
      select(-influent_flow_L, -influent_tss_mg_l) %>%
      pivot_wider(names_from = facility, values_from = c(copies_per_uL, copies, n.miss, n)) %>%
      full_join(covid, by = c("sample_date"="date"))



wbe2.target <- wbe2 %>% 
      group_by(sample_date, facility, biological_replicate, target) %>%
      summarise(copies_per_uL = exp(mean(log(copies_per_uL))), 
                n.miss = sum(is.na(ct)), 
                n = n()
                ) %>%
      ungroup() %>%
      group_by(sample_date, facility, target) %>%
      summarise(copies_per_uL = exp(mean(log(copies_per_uL))), 
                n.miss = sum(n.miss), 
                n = sum(n)
                ) %>%
      ungroup() %>%
      left_join(., plant, by = c("sample_date"="date", "facility"="wrf")) %>%
      mutate(copies = copies_per_uL * 1e6 * influent_flow_L) %>%
      group_by(sample_date, target) %>%
      summarise(copies_per_uL = exp(mean(log(copies_per_uL))), copies = exp(mean(log(copies))), n.miss = sum(n.miss), n = sum(n)) %>%
      ungroup() %>%
      pivot_wider(names_from = target, values_from = c(copies_per_uL, copies, n.miss, n)) %>% 
      full_join(covid, by = c("sample_date"="date"))





my.data <- full_join(wbe2.date, full_join(wbe2.facility, full_join(wbe2.target, wbe2.wide))) %>%
  arrange(sample_date)

```



```{r, echo = F, message=F, warning=F, error=F}

# names(my.data)

xvars <- names(my.data)[which(grepl("copies", names(my.data)))]

xcats <- c("total", "total",                                                        # daily aggregate all levels
           "facility", "facility", "facility", "facility", "facility", "facility",  # by facility
           "target", "target", "target", "target",                                  # by target
           "facility-target", "facility-target",                                    # by facility and target 
           "facility-target", "facility-target", 
           "facility-target", "facility-target", 
           "facility-target", "facility-target", 
           "facility-target", "facility-target", 
           "facility-target", "facility-target")

yvars <- c("cases.reported", "cases.reported.7dma", "cases.reported.17dsum", 
           "cases.symptom.onset", "cases.symptom.onset.7dma", "cases.symptom.onset.17dsum", 
           "cases.reported.con", "cases.reported.decon", "cases.reported.decon.17dsum")

yvars <- c("cases.reported", "cases.reported.7dma", "cases.reported.17dsum", 
           "cases.symptom.onset", "cases.symptom.onset.7dma", "cases.symptom.onset.17dsum", 
           "cases.reported.con", "cases.reported.decon", "cases.reported.decon.17dsum", 
           "pcr_pos", "pcr.pos.7dma", "pcr.pos.17dsum")
```





## Cross-correlations

Now, with a summarized dataset (i.e., a single observation for each of the `r length(unique(wbe$sample_date))` sampling dates), it is finally possible to explore the relationships between the viral loads detected in the wastewater samples and the case counts data for Athens-Clarke County. As mentioned previously, the relative timing of viral shedding in wastewater compared to case incidence (or case diagnosis or report) is not well-established. However, some studies have estimated that the RNA concentrations in wastewater ***sludge*** lead positive tests by date of specimen collection by 0-2 days and positive tests by report date by 6-8 days ([Peccia et al, 2020](https://www.nature.com/articles/s41587-020-0684-z)).[^peccia]      
    
To similarly investigate these associations, I computed cross-correlations between total viral loads in wastewater (i.e., the summarized estimate of total viral copies for each sampling date) and case counts at various leads and lags. Instead of using the raw case counts, smoothed 7-day moving averages were used for the cases by report date, PCR-positive tests by date of specimen collection, cases by symptom onset date, and deconvoluted cases by report date were considered. The correlations were calculated using Spearman rank correlations. Figure \@ref(fig:cross-cors) shows these estimated cross-correlations.     

[^peccia]:Peccia et al (2020) used distributed lag Poisson regression models to estimate these lead times (i.e., they fit time-series models to the number of positive cases and included the wastewater sludge RNA concentrations at various lags as predictors and significant coefficient estimates were used to determine time ranges).

```{r, echo = F, message=F, warning=F, error=F}






the.lags <- -21:21
the.lags <- the.lags[-which(the.lags==0)]

the.vars <- yvars

# the.vars <- c("cases.reported", "cases.symptom.onset", "cases.reported.7dma", "cases.symptom.onset.7dma", "cases.reported.decon")

for(ii in 1:length(the.vars)){
  for(i in 1:length(the.lags)){
    if(the.lags[i]<0){
      my.data[,paste0(the.vars[ii], ".lead.", abs(the.lags[i]))] <- lead(my.data[,the.vars[ii]], n = abs(the.lags[i]))
    }else{
      my.data[,paste0(the.vars[ii], ".lag.", abs(the.lags[i]))] <- lag(my.data[,the.vars[ii]], n = the.lags[i])
    }
  }
}





copies.spline.smooth <- smooth.spline(my.data$sample_date[complete.cases(my.data$copies)], my.data$copies[complete.cases(my.data$copies)])

copies.interpolated.smooth <- predict(copies.spline.smooth, data.frame(sample_date = as.numeric(seq(min(wbe$sample_date), max(wbe$sample_date), by = "days")))) %>% 
                              bind_cols() %>% suppressMessages() %>%
                              setNames(., nm = c("sample_date_numeric", "copies.interpolated.smooth")) %>%
                              mutate(sample_date = seq(min(wbe$sample_date), max(wbe$sample_date), by = "days"))


copies.spline.smooth2 <- smooth.spline(my.data$sample_date[complete.cases(my.data$copies)], my.data$copies[complete.cases(my.data$copies)], spar = 0.3)

copies.interpolated.smooth2 <- predict(copies.spline.smooth2, data.frame(sample_date = as.numeric(seq(min(wbe$sample_date), max(wbe$sample_date), by = "days")))) %>% 
                              bind_cols() %>% suppressMessages() %>%
                              setNames(., nm = c("sample_date_numeric", "copies.interpolated.smooth.alternate")) %>%
                              mutate(sample_date = seq(min(wbe$sample_date), max(wbe$sample_date), by = "days"))




copies.spline <- splinefun(my.data$sample_date[complete.cases(my.data$copies)], my.data$copies[complete.cases(my.data$copies)])

copies.spline.smooth <- full_join(copies.interpolated.smooth, copies.interpolated.smooth2, by = c("sample_date", "sample_date_numeric")) %>%
                        mutate(copies.interpolated = copies.spline(sample_date))







copies.spline.smooth.n1 <- smooth.spline(my.data$sample_date[complete.cases(my.data$copies_N1)], my.data$copies_N1[complete.cases(my.data$copies_N1)])

copies.interpolated.smooth.n1 <- predict(copies.spline.smooth.n1, data.frame(sample_date = as.numeric(seq(min(wbe$sample_date), max(wbe$sample_date), by = "days")))) %>% 
                              bind_cols() %>% suppressMessages() %>%
                              setNames(., nm = c("sample_date_numeric", "copies.interpolated.smooth.n1")) %>%
                              mutate(sample_date = seq(min(wbe$sample_date), max(wbe$sample_date), by = "days"))


copies.spline.n1 <- splinefun(my.data$sample_date[complete.cases(my.data$copies_N1)], my.data$copies_N1[complete.cases(my.data$copies_N1)])



copies.spline.smooth.n2 <- smooth.spline(my.data$sample_date[complete.cases(my.data$copies_N2)], my.data$copies_N2[complete.cases(my.data$copies_N2)])

copies.interpolated.smooth.n2 <- predict(copies.spline.smooth.n2, data.frame(sample_date = as.numeric(seq(min(wbe$sample_date), max(wbe$sample_date), by = "days")))) %>% 
                              bind_cols() %>% suppressMessages() %>%
                              setNames(., nm = c("sample_date_numeric", "copies.interpolated.smooth.n2")) %>%
                              mutate(sample_date = seq(min(wbe$sample_date), max(wbe$sample_date), by = "days"))


copies.spline.n2 <- splinefun(my.data$sample_date[complete.cases(my.data$copies_N2)], my.data$copies_N2[complete.cases(my.data$copies_N2)])




copies.spline.smooth.st <- full_join(copies.interpolated.smooth.n1, copies.interpolated.smooth.n2, by = c("sample_date", "sample_date_numeric")) %>% mutate(copies.interpolated.n1 = copies.spline.n1(sample_date), copies.interpolated.n2 = copies.spline.n2(sample_date))




copies.splines <- full_join(copies.spline.smooth, copies.spline.smooth.st, by = c("sample_date", "sample_date_numeric"))














my.data %<>% 
  full_join(copies.splines, by = "sample_date") %>% 
  select(sample_date, copies_per_uL, copies, copies.interpolated, copies.interpolated.smooth, copies.interpolated.smooth.alternate, copies.interpolated.n1, copies.interpolated.smooth.n1, copies.interpolated.n2, copies.interpolated.smooth.n2, everything())








calculate.Cross.correlations <- function(this.var){
  
    the.cors <- matrix(NA, nrow = length(the.lags)+1, ncol = length(the.vars))

    for(ii in 1:length(the.vars)){
      for(i in 1:{length(the.lags)+1}){
        if(i == 1){
          the.cors[i,ii] <- cor(my.data[,this.var], my.data[,the.vars[ii]], use = "pairwise.complete.obs", method = "spearman")
        }else{
          if(the.lags[i-1]<0){
            the.cors[i,ii] <- cor(my.data[,this.var], my.data[,paste0(the.vars[ii], ".lead.", abs(the.lags[i-1]))], use = "pairwise.complete.obs", method = "spearman")
          }else{
            the.cors[i,ii] <- cor(my.data[,this.var], my.data[,paste0(the.vars[ii], ".lag.", abs(the.lags[i-1]))], use = "pairwise.complete.obs", method = "spearman")
          }
        }
        
      }
    }
    
    the.cors <- cbind(c(0,the.lags), the.cors) %>% as.data.frame() %>% setNames(., nm = c("lag", the.vars)) %>% arrange(lag)
    
    return(the.cors)
}



#copies
copies.cors <- calculate.Cross.correlations("copies")
copies.per.uL.cors <- calculate.Cross.correlations("copies_per_uL")

copies.interpolated.cors <- calculate.Cross.correlations("copies.interpolated")
copies.interpolated.smooth.cors <- calculate.Cross.correlations("copies.interpolated.smooth")
copies.interpolated.smooth.alt.cors <- calculate.Cross.correlations("copies.interpolated.smooth.alternate")

copies.interpolated.n1.cors <- calculate.Cross.correlations("copies.interpolated.n1")
copies.interpolated.smooth.n1.cors <- calculate.Cross.correlations("copies.interpolated.smooth.n1")
copies.interpolated.n2.cors <- calculate.Cross.correlations("copies.interpolated.n2")
copies.interpolated.smooth.n2.cors <- calculate.Cross.correlations("copies.interpolated.smooth.n2")

```




















```{r, echo = F, message=F, warning=F, error = F}




plot_Cross_correlation <- function(this.variable, this.variable.pretty, this.label, the.cors){
  
    plot(the.cors[,this.variable] ~ lag, data = the.cors, type = "h", xlab = "Lag in Days", ylab = "Correlation Coefficient", main = this.variable.pretty)
    segments(x0 = the.cors$lag[which.max(abs(the.cors[,this.variable]))], x1 = the.cors$lag[which.max(abs(the.cors[,this.variable]))], y0 = 0, y1 = the.cors[which.max(abs(the.cors[,this.variable])), this.variable], lwd = 4)
    text(par('usr')[2]+(1-par('plt')[2])*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
         par('usr')[3]-(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
         
         labels = bquote(rho == .(round(the.cors[which.max(abs(the.cors[,this.variable])), this.variable], 3)) ~ at ~ lag ~ .(the.cors$lag[which.max(abs(the.cors[,this.variable]))])), xpd = T, adj = c(1,0))
    
    text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
         par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
         labels = this.label, adj = c(0,1), xpd = T, cex = 1, font = 2)
    
    arrows(x0 = -15, x1 = -20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)
    arrows(x0 = 15, x1 = 20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)
    
    text(-15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Future", adj = c(-0.2,0.5))
    text(15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Past", adj = c(1.2,0.5))
  
  
}

plot.Cross.correlation.grid <- function(these.variables, these.variables.pretty, these.labels, the.cors){
  
  for(i in 1:length(these.variables)){
    plot_Cross_correlation(these.variables[i], these.variables.pretty[i], these.labels[i], the.cors)
  }
  
}



par(mfrow = c(1, 1))

these.cors <- calculate.Cross.correlations(xvars[2])
# 
# these.cors <- copies.interpolated.cors
# these.cors <- copies.interpolated.smooth.cors
# these.cors <- copies.interpolated.smooth.alt.cors
# these.cors <- copies.interpolated.n1.cors
# these.cors <- copies.interpolated.smooth.n1.cors
# these.cors <- copies.interpolated.n2.cors
# these.cors <- copies.interpolated.smooth.n2.cors
  
  
  
these.variables.pretty <- c("7-day Moving Average of Cases by Report Date", "7-day Moving Average of Cases by Symptom Onset", "Deconvoluted Cases by Report Date")

```



```{r, cross-cors, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*3, fig.cap = "Cross-correlations Between the Total Viral Copies and COVID-19 Cases"}

# par(mfrow = c(3,1))
# plot_Cross_correlation(yvars[2], these.variables.pretty[1], LETTERS[1], these.cors)
# plot_Cross_correlation(yvars[5], these.variables.pretty[2], LETTERS[2], these.cors)
# plot_Cross_correlation(yvars[8], these.variables.pretty[3], LETTERS[3], these.cors)
these.variables.pretty <- c("7-day Moving Average of Cases by Report Date", "7-day Moving Average of Cases by Symptom Onset", "Deconvoluted Cases by Report Date", "7-day Moving Average of PCR-positive Tests by Date of Specimen Collection")

# png(filename = "./consult/03-output/for-pres/cross-cors.png", width = 16/2, height = 9*2, units = "in", res = 300, pointsize = 16)
par(mfrow = c(4,1))
plot_Cross_correlation(yvars[8], these.variables.pretty[3], LETTERS[1], these.cors)
plot_Cross_correlation(yvars[5], these.variables.pretty[2], LETTERS[2], these.cors)
plot_Cross_correlation(yvars[11], these.variables.pretty[4], LETTERS[3], these.cors)
plot_Cross_correlation(yvars[2], these.variables.pretty[1], LETTERS[4], these.cors)
# dev.off()

```

```{r, echo = F, message=F, warning=F, error=F}

decon.test <- cor.test(my.data$copies, my.data$cases.reported.decon.lag.12, method = "spearman")
so.test <- cor.test(my.data$copies, my.data$cases.symptom.onset.7dma.lag.6, method = "spearman")
sc.test <- cor.test(my.data$copies, my.data$pcr.pos.7dma.lag.5, method = "spearman")
report.test <- cor.test(my.data$copies, my.data$cases.reported.7dma.lead.5, method = "spearman")

```

From Figure \@ref(fig:cross-cors) we can note the point at which the correlations are the strongest. The viral loads within wastewater seem to lead the 7-day moving average of cases by report date by about 5 days ($\hat\rho$ = `r round(report.test$estimate, 3)`, p `r ifelse(round(report.test$p.value, 3)==0, " < 0.001", paste0(" = ", round(report.test$p.value, 3)))`), but seem to lag behind the 7-day moving averages of PCR-positive test by date of specimen collection by approximately 5 days ($\hat\rho$ = `r round(sc.test$estimate, 3)`, p `r ifelse(round(sc.test$p.value, 3)==0, " < 0.001", paste0(" = ", round(sc.test$p.value, 3)))`) and cases by symptom onset by about 6 days ($\hat\rho$ = `r round(so.test$estimate, 3)`, p `r ifelse(round(so.test$p.value, 3)==0, " < 0.001", paste0(" = ", round(so.test$p.value, 3)))`) and the deconvoluted cases by report date by about 12 days ($\hat\rho$ = `r round(decon.test$estimate, 3)`, p `r ifelse(round(decon.test$p.value, 3)==0, " < 0.001", paste0(" = ", round(decon.test$p.value, 3)))`).     
  
To assess any potential effects of the sampling interval, I calculated a spline function for the viral loads in waste and used the estimated spline curve to infer / interpolate the values of the wastewater viral loads missing due to the interval censoring. These new data were used similarly as before to estimate the cross-correlations at various leads and lags of COVID-19 cases (Figure \@ref(fig:cross-cors-interpolated)). Additionally, I used another spline fit to smooth the wastewater viral loads data and, again, calculated cross-correlations (Figure \@ref(fig:cross-cors-smoothed), smoothing parameter = 0.56). 



```{r, cross-cors-interpolated, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*3, fig.cap = "Cross-correlations Between the Interpolated Total Viral Copies and COVID-19 Cases"}

# par(mfrow = c(3,1))
# plot_Cross_correlation(yvars[2], these.variables.pretty[1], LETTERS[1], copies.interpolated.cors)
# plot_Cross_correlation(yvars[5], these.variables.pretty[2], LETTERS[2], copies.interpolated.cors)
# plot_Cross_correlation(yvars[8], these.variables.pretty[3], LETTERS[3], copies.interpolated.cors)


par(mfrow = c(4,1))
plot_Cross_correlation(yvars[8], these.variables.pretty[3], LETTERS[1], copies.interpolated.cors)
plot_Cross_correlation(yvars[5], these.variables.pretty[2], LETTERS[2], copies.interpolated.cors)
plot_Cross_correlation(yvars[11], these.variables.pretty[4], LETTERS[3], copies.interpolated.cors)
plot_Cross_correlation(yvars[2], these.variables.pretty[1], LETTERS[4], copies.interpolated.cors)
```


```{r, cross-cors-smoothed, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*3, fig.cap = "Cross-correlations Between the Smoothed, Interpolated Total Viral Copies and COVID-19 Cases"}

# par(mfrow = c(3,1))
# plot_Cross_correlation(yvars[2], these.variables.pretty[1], LETTERS[1], copies.interpolated.smooth.cors)
# plot_Cross_correlation(yvars[5], these.variables.pretty[2], LETTERS[2], copies.interpolated.smooth.cors)
# plot_Cross_correlation(yvars[8], these.variables.pretty[3], LETTERS[3], copies.interpolated.smooth.cors)

par(mfrow = c(4,1))
plot_Cross_correlation(yvars[8], these.variables.pretty[3], LETTERS[1], copies.interpolated.smooth.cors)
plot_Cross_correlation(yvars[5], these.variables.pretty[2], LETTERS[2], copies.interpolated.smooth.cors)
plot_Cross_correlation(yvars[11], these.variables.pretty[4], LETTERS[3], copies.interpolated.smooth.cors)
plot_Cross_correlation(yvars[2], these.variables.pretty[1], LETTERS[4], copies.interpolated.smooth.cors)
```



For the spline interpolation (Figure \@ref(fig:cross-cors-interpolated)), the pattern holds consistent with wastewater samples leading cases by report date and lagging behind cases by symptom onset date and the deconvoluted case reports. On the other hand, the smoothing spline (Figure \@ref(fig:cross-cors-smoothed)) indicates that the wastewater samples lag behind all case incidence indicators. However, the extent of the smoothing seems to have a large effect on this observed pattern, as can be seen in Figure \@ref(fig:cross-cors-sm-alt) where the data were smoothed to a lesser extent (smoothing parameter = 0.3) and the pattern reverts to as previously observed. 

```{r, cross-cors-sm-alt, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*3, fig.cap = "Cross-correlations Between the Less Smoothed, Interpolated Total Viral Copies and COVID-19 Cases"}

# par(mfrow = c(3,1))
# plot_Cross_correlation(yvars[2], these.variables.pretty[1], LETTERS[1], copies.interpolated.smooth.alt.cors)
# plot_Cross_correlation(yvars[5], these.variables.pretty[2], LETTERS[2], copies.interpolated.smooth.alt.cors)
# plot_Cross_correlation(yvars[8], these.variables.pretty[3], LETTERS[3], copies.interpolated.smooth.alt.cors)


par(mfrow = c(4,1))
plot_Cross_correlation(yvars[8], these.variables.pretty[3], LETTERS[1], copies.interpolated.smooth.alt.cors)
plot_Cross_correlation(yvars[5], these.variables.pretty[2], LETTERS[2], copies.interpolated.smooth.alt.cors)
plot_Cross_correlation(yvars[11], these.variables.pretty[4], LETTERS[3], copies.interpolated.smooth.alt.cors)
plot_Cross_correlation(yvars[2], these.variables.pretty[1], LETTERS[4], copies.interpolated.smooth.alt.cors)
```


So, it seems that our wastewater samples lead the COVID-19 case reports and lag behind the PCR-positive tests by date of specimen collection, cases by symptom onset date, and deconvoluted case reports. However, we may interpret rather a range of days where these correlations are the strongest to be a 3-8 day lead for case reports, a 3-day lead to 5-day lag of PCR-positive tests by date of specimen collection, a 1-day lead to 7-day lag for cases by symptom onset date, and a 8-14-day lag for deconvoluted case reports. 




```{r, eval = F, echo = F}

png(filename = "./consult/03-output/cross-correlations%03d.png",
    width = 16, height = 9, units = "in", pointsize = 16,
    bg = "white", res = 300)



par(mfrow = c(1, 1))

these.cors <- calculate.Cross.correlations(xvars[2])

these.variables.pretty <- c("7-day Moving Average of Cases by Report Date", "7-day Moving Average of Cases by Symptom Onset", "Deconvoluted Cases by Report Date")



plot.Cross.correlation.grid(yvars[c(2, 5, 8)], these.variables.pretty, LETTERS[1:3], these.cors)

dev.off()





png(filename = "./consult/03-output/cross-correlations-v2-%03d.png",
    width = 16, height = 9, units = "in", pointsize = 16,
    bg = "white", res = 300)



par(mfrow = c(1, 1))

these.variables.pretty <- c("7-day Moving Average of Cases by Report Date", "7-day Moving Average of Cases by Symptom Onset", "Deconvoluted Cases by Report Date")



plot.Cross.correlation.grid(yvars[c(2, 5, 8)], these.variables.pretty, LETTERS[1:3], copies.interpolated.cors)

dev.off()




```

 
 
 
 
 
 
 
 
 



 
 
 
 
 
 
 
 
 

\newpage
# Conclusions

This report covers an analysis for the utility of COVID-19 wastewater epidemiological surveillance. We: investigated the occurence of undetermined / missing data from the RT-qPCR analyses; estimated detection and quantification limits; elaborated on the conversion schemes from RT-qPCR results to viral load quantities in wastewater samples; employed our estimates of detection and quantification limits to overcome / mitigate the missingness in the data; explored a method of deconvolution for the COVID-19 epidemic curve; characterized the temporal associations between wastewater samples and population-level COVID-19 case indicators; and, ultimately, conclude that the wastewater samples may serve as useful and informative complements to traditional anthropocentric case surveillance.     

Figure \@ref(fig:epi-curve-decon-with-copies) shows the time-series of the wastewater samples viral loads along with the four case incidence indicators used in comparisons. We can see how closely the wastewater sample profiles approximate the curves for the cases by symptom onset date and cases by report date and how it seemingly lags behind the deconvoluted case reports curve. Biologically, this makes sense as the deconvoluted case reports curve would give us our best estimate of the *onset* of infections / exposures in the population. The actual viral shedding likely doesn't initiate so proximal to exposure, but rather it would take some time for the infection to develop; perhaps, it would better approximate the onset of infectiousness. 










```{r epi-curve-decon-with-copies, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap = "Comparison of Epidemic Curves from Dates of Report and Symptom Onset to a Deconvoluted Incidence Curve"}
par(mfrow = c(1,1))
my.data %<>% arrange(sample_date)

study.period <- covid$date[which(covid$date>=as.Date("2020-06-30"))]

# png(filename = "./consult/03-output/for-pres/overlayed.png", width = 16, height = 9, units = "in", res = 300, pointsize = 16)
# 
# png(filename = "./consult/03-output/cases-and-copies.png", width = 16, height = 9, units = "in", res = 300, pointsize = 16, family = "sans")

plot(covid$date, covid$cases.reported.7dma, type = "l", xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', xaxs = 'i', yaxs = 'i', ylim = c(0, 470), xlim = c(min(study.period)-2, max(study.period)+2))

at.points <- covid$date[which(covid$date>=as.Date("2020-06-30"))][which(format(covid$date[which(covid$date>=as.Date("2020-06-30"))], "%d")=="01")]

axis(1, at=at.points, tick=T, labels = paste(format(at.points, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -1)
axis(2, at=seq(0, 200, by = 50), labels = T, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(xlab = "Date", line = 2)
# title(ylab = "Cases", line = 2, adj = 0.25)

text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     100, 
     labels = "Cases", adj = c(0.5,2), xpd = T, cex = 1, font = 1, srt = 90)

unscaled <- log10(my.data$copies[complete.cases(my.data$copies)])

min.unscaled <- min(unscaled)

range.unscaled <- range(unscaled)

range.diff.unscaled <- diff(range.unscaled)


unscaled.at.points <- seq(range.unscaled[1], range.unscaled[2], length.out = 5)

scaled.at.points <- (unscaled.at.points - min.unscaled)/range.diff.unscaled*200+250


axis(2, at = scaled.at.points, labels = round(10^(unscaled.at.points)/1e15, 1), las = 1, cex = 0.7, cex.axis = 0.7)


text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     350, 
     labels = "Total Viral Copies (trillions)", adj = c(0.5,2), xpd = T, cex = 1, font = 1, srt = 90)


# title(ylab = "Total Viral Copies (trillions)", line = 2.5, adj = 0.75)

# abline(h = 225, lty = '9212', col = "black")
lines(seq(min(study.period)-20, max(study.period)+2, by = "days")[seq(1, length(seq(min(study.period)-20, max(study.period)+2, by = "days")), by = 3)], rep(225, length(seq(min(study.period)-20, max(study.period)+2, by = "days")[seq(1, length(seq(min(study.period)-20, max(study.period)+2, by = "days")), by = 3)])) + c(-5,5), lty = '9212', xpd = T)

lines(seq(min(study.period)-20, max(study.period)+2, by = "days")[seq(1, length(seq(min(study.period)-20, max(study.period)+2, by = "days")), by = 3)], rep(225, length(seq(min(study.period)-20, max(study.period)+2, by = "days")[seq(1, length(seq(min(study.period)-20, max(study.period)+2, by = "days")), by = 3)])) + c(5,-5), lty = '9212', xpd = T)
# 
# lines(seq(min(study.period)-10, max(study.period)+2, by = "days")[seq(1, 228, by = 3)], rep(225, 76) + c(5,-5), lty = '9212', xpd = T)
# 
# lines(seq(min(study.period)-10, max(study.period)+2, by = "days")[seq(1, 228, by = 3)]+1, rep(225, 76) + c(-5,5), lty = '9212', xpd = T)
# lines(seq(min(study.period)-10, max(study.period)+2, by = "days")[seq(1, 228, by = 3)]+1, rep(225, 76) + c(5,-5), lty = '9212', xpd = T)

# axis(3, at = at.points2, labels = format(c(min(wbe$sample_date), max(wbe$sample_date)), "%d %b %y"), tick = T, line = 0, tck = 0.02, cex.axis = 0.7)
# axis(3, at = mean(at.points2), labels = "Study Period", line = 0, padj = -0.25, tck = -0.04)

# barplot(cases.symptom.onset~date, data=covid, las=2, xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', width = 1, space = 0, xaxs = 'i', yaxs = 'i', axes=F, add = T, col = viridis::viridis(100, alpha = 0.25)[55], border = viridis::viridis(100, alpha = 0.25)[55])


# lines(seq(0.5, nrow(covid)-0.5, by=1), decon.report$Chat, lty = 3, lwd = 2)
lines(covid$date, decon.report$Ihat, lty = 5, lwd = 4)

# lines(seq(0.5, nrow(covid)-0.5, by=1), covid$cases.reported.7dma)
lines(covid$date, covid$cases.symptom.onset.7dma, lwd = 2, lty = 3)

lines(covid$date, covid$pcr.pos.7dma, lwd = 2, lty = 4)

abline(v = at.points, lty = 5, col = "gainsboro")



lines(my.data$sample_date[complete.cases(my.data$copies)], (log(my.data$copies[complete.cases(my.data$copies)])-min(log(my.data$copies), na.rm = T))/diff(range(log(my.data$copies), na.rm = T))*200+250, type = "o")
# lines(spline(my.data$sample_date, my.data$copies)$x, (log(spline(my.data$sample_date, my.data$copies)$y)-min(log(my.data$copies), na.rm = T))/diff(range(log(my.data$copies), na.rm = T))*200+250, lwd = 3)

# lines(my.data$sample_date, (log(my.data$copies.interpolated)-min(log(my.data$copies), na.rm = T))/diff(range(log(my.data$copies), na.rm = T))*200+250, type = "l", lwd = 2, lty = 5)
# lines(my.data$sample_date, (log(my.data$copies.interpolated.smooth.alternate)-min(log(my.data$copies), na.rm = T))/diff(range(log(my.data$copies), na.rm = T))*200+250, type = "l", lwd = 2, lty = 3)

# lines(smooth.spline(my.data$sample_date[complete.cases(my.data$copies)], my.data$copies[complete.cases(my.data$copies)])$x, (log(smooth.spline(my.data$sample_date[complete.cases(my.data$copies)], my.data$copies[complete.cases(my.data$copies)])$y)-min(log(my.data$copies), na.rm = T))/diff(range(log(my.data$copies), na.rm = T))*200+250, lwd = 3)
# lines(my.data$sample_date, my.data$cases.reported.7dma.lead.5+150)
# lines(my.data$sample_date, my.data$cases.reported.7dma.lag.5+150)
# lines(my.data$sample_date, my.data$cases.symptom.onset.lag.6+150)
# lines(my.data$sample_date, my.data$cases.reported.decon.lag.12+150)


# lines(plant$date[which(plant$wrf=="CC")][order(plant$date[which(plant$wrf=="CC")])], (log10(n1.lod.copies_per_uL * 1e6 * plant$influent_flow_L[which(plant$wrf=="CC")][order(plant$date[which(plant$wrf=="CC")])])-min.unscaled)/range.diff.unscaled*200+250)
# 
# lines(plant$date[which(plant$wrf=="MI")][order(plant$date[which(plant$wrf=="MI")])], (log10(n1.lod.copies_per_uL * 1e6 * plant$influent_flow_L[which(plant$wrf=="MI")][order(plant$date[which(plant$wrf=="MI")])])-min.unscaled)/range.diff.unscaled*200+250)
# 
# lines(plant$date[which(plant$wrf=="NO")][order(plant$date[which(plant$wrf=="NO")])], (log10(n1.lod.copies_per_uL * 1e6 * plant$influent_flow_L[which(plant$wrf=="NO")][order(plant$date[which(plant$wrf=="NO")])])-min.unscaled)/range.diff.unscaled*200+250)
# 
# 
# View(my.data[,c("sample_date", "copies", "copies.interpolated.smooth", "cases.reported", "cases.reported.lag.1", "cases.reported.lag.2", "cases.reported.lag.3", "cases.reported.lag.4")])





legend("topleft", 
       bty = 'n',
       pt.cex = 1,
       pch = c(1,NA,NA,NA,NA),
       lty = c(1, 1, 4, 3, 5), 
       lwd = c(1, 1, 2, 2, 4), 
       legend = c("Wastewater Viral Loads", "7-day Moving Average of Cases by Report Date", "7-day Moving Average of PCR-positive Tests by Specimen Collection Date", "7-day Moving Average of Cases by Symptom Onset", "Deconvoluted Case Reports"), 
       cex = 0.7)


# dev.off()

# dev.off()
```









```{r epi-curve-decon-with-copies-ml, eval = F, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap = "Comparison of Epidemic Curves from Dates of Report and Symptom Onset to a Deconvoluted Incidence Curve"}
my.data %<>% arrange(sample_date)


study.period <- covid$date[which(covid$date>=as.Date("2020-06-30"))]

png(filename = "./consult/03-output/cases-and-copies.png", width = 16, height = 9, units = "in", res = 300, pointsize = 16, family = "sans")

plot(covid$date, covid$cases.reported.7dma, type = "l", xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', xaxs = 'i', yaxs = 'i', ylim = c(0, 470), xlim = c(min(study.period)-2, max(study.period)+2))

at.points <- covid$date[which(covid$date>=as.Date("2020-06-30"))][which(format(covid$date[which(covid$date>=as.Date("2020-06-30"))], "%d")=="01")]

axis(1, at=at.points, tick=T, labels = paste(format(at.points, "%b %y"),' '), xpd = TRUE, cex.axis=0.7, line = 0, padj = -1)
axis(2, at=seq(0, 200, by = 50), labels = T, tick = T, cex=0.7, cex.axis = 0.7, las = 1)
title(xlab = "Date", line = 2)

text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     100, 
     labels = "Cases", adj = c(0.5,2), xpd = T, cex = 1, font = 1, srt = 90)

unscaled <- log10(my.data$copies[complete.cases(my.data$copies)])

min.unscaled <- min(unscaled)

range.unscaled <- range(unscaled)

range.diff.unscaled <- diff(range.unscaled)


unscaled.at.points <- seq(range.unscaled[1], range.unscaled[2], length.out = 5)

scaled.at.points <- (unscaled.at.points - min.unscaled)/range.diff.unscaled*200+250


axis(2, at = scaled.at.points, labels = round(10^(unscaled.at.points)/1e15, 1), las = 1, cex = 0.7, cex.axis = 0.7)


text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     350, 
     labels = "Total Viral Copies (trillions)", adj = c(0.5,2), xpd = T, cex = 1, font = 1, srt = 90)


abline(v = at.points, lty = 5, col = "gainsboro")


lines(seq(min(study.period)-20, max(study.period)+2, by = "days")[seq(1, length(seq(min(study.period)-20, max(study.period)+2, by = "days")), by = 3)], rep(225, length(seq(min(study.period)-20, max(study.period)+2, by = "days")[seq(1, length(seq(min(study.period)-20, max(study.period)+2, by = "days")), by = 3)])) + c(-5,5), lty = '9212', xpd = T)

lines(seq(min(study.period)-20, max(study.period)+2, by = "days")[seq(1, length(seq(min(study.period)-20, max(study.period)+2, by = "days")), by = 3)], rep(225, length(seq(min(study.period)-20, max(study.period)+2, by = "days")[seq(1, length(seq(min(study.period)-20, max(study.period)+2, by = "days")), by = 3)])) + c(5,-5), lty = '9212', xpd = T)





lines(covid$date, covid$cases.symptom.onset.7dma, lwd = 2, lty = 3)



lines(my.data$sample_date[complete.cases(my.data$copies)], (log(my.data$copies[complete.cases(my.data$copies)])-min(log(my.data$copies), na.rm = T))/diff(range(log(my.data$copies), na.rm = T))*200+250, type = "o")



legend("topleft", 
       bty = 'n',
       pt.cex = 1,
       pch = c(1,NA,NA),
       lty = c(1, 1, 3), 
       lwd = c(1, 1, 2), 
       legend = c("Wastewater Viral Loads", "7-day Moving Average of Cases by Report Date", "7-day Moving Average of Cases by Symptom Onset"), 
       cex = 0.7)


dev.off()

```












This was a multi-step analysis and decisions made at earlier steps may have impacts on the downstream results. Therefore, it is critical to evaluate the sensitivity of our results to these analytical decisions and potential alternatives. Here, I will note on a select few of these decisions that I feel may warrant further attention. 

First, the conversion calculations from RT-qPCR results using the standard curves and positive controls could use further attention. Due to the variability imposed by the standard stock solution (e.g., batch decay?) and lab techniques (e.g., pipetting), the repeated runs of the standard curves were not utilized in this analysis. Instead, single, well-performing runs were considered characteristic of the reaction and used in the calculations. However, the standard curves chosen for the two sequence targets, N1 and N2, differ substantially and their difference can be seen in the downstream analyses. So, it may be worth considering the biological implications of such a difference in standard curves and, perhaps, reassess which values to use in conversions. 

Perhaps the single most impactful step in this analysis was the data management using replacement values for the limits of detection and quantification. Although we took a practical (and common) approach using half of these limits in replacement, it may be possible to devise more complex schemes, and, in Appendix Section \@ref(scenarios), I toyed around with different ways to replace undetermined values within the data. For example, I thought it may be informative to replace undetermined results with values proportional to the limit of detection that scale with the extent of the missingness of for a given sampling frame (e.g., proportion of technical replicates missing on a given day at a given facility from a given biological replicate and for a given sequence target). The impacts of these considerations have yet to be explored.     
    
Similarly with respect to our *confidence* in the observations made in a sampling frame, the summarization schemes (read averaging) of technical and biological replicates could be done in a variety of ways. Appendix Section \@ref(means) illustrates various mean calculations and where a particular measure of centrality falls in relation to other (e.g., geometric mean < arithmetic mean < quadratic mean). With this in mind, I thought that it may be possible to use the extent of the missingness to help choose our averaging calculation (e.g., scale the power mean parameter *p* with the proportion missing). In this analysis, we used exclusively geometric means in our summarizations. Geometric means tend to bias against ***larger*** values in a distribution. This property may be desired given sparse results where very few yielded a detectable result. However, in a situation with a single missing result, we may instead wish to bias against ***smaller*** values, especially when noting the replacement values based on the limit of detection are considerably smaller than other values in the distributions; in this case, we may prefer a quadradic (or higher power) mean. 

Again in consideration of the extent of the "missingness" in the data due to undetermined values, all aforementioned effects may manifest in our cross-correlation analyses. If we were able to capture this degree of "uncertainty" within a particular summarized value, it could be informative in establishing the lead / lag times of associated case incidence indicators. For example, if we could assign some range of values (or simply a measure of dispersion) or some weight to a particular observation, we may prioritize the alignment of more *certain* wastewater samples with case indicators (i.e., disregarding lack of fit for more *uncertain* wastewater samples). This idea is a little less developed with respect to actual implementation and it may be unnecessary given the aforementioned, upstream considerations for accounting for the missingness.     
    
As of now, I consider this analysis incomplete. I can envision more involved development of modeling strategies to better characterize the data. For example, Peccia et al (2020) fit distributed lag models to their data by regressing case indicators on lagged wastewater samples to characterize the temporal associations within the data. These methods are new to me and I may not well understand them, but I think a distributed lag model could be very useful in our analyses. However, I think it would be more well-suited to regress the wastewater samples on led / lagged case indicators to be more respectful to the biology of the system (e.g., cases are shedding into wastewater instead of wastewater exposure generating cases (disproven fecal transmission?)). Furthermore, the characterization of this environmental shedding may be directly incorporated to developing mathematical infectious disease models (e.g., as the process of shedding). I think that an infectious disease model could be very useful in establishing the utility and practicality of wastewater epidemiological surveillance. From a specialized model, we would not only be able to characterize the complex interplay of mechanisms, but we would also be able to fit the model to data for parameter estimation (e.g., average shedding, decay, *true* incidence) and we would establish a forecasting framework. 








```{r, eval = F, echo = F}


temp <- smooth.spline(my.data$sample_date[complete.cases(my.data$copies)], my.data$copies[complete.cases(my.data$copies)])
ypreds <- predict(temp , data.frame(sample_date = as.numeric(seq(min(my.data$sample_date), max(my.data$sample_date), by = "days"))))


ypreds <- predict(temp , data.frame(sample_date = as.numeric(seq(as.Date("2020-06-01"), as.Date("2021-02-28"), by = "days"))))



points(unlist(ypreds$x), (log(unlist(ypreds$y))-min(log(my.data$copies), na.rm = T))/diff(range(log(my.data$copies), na.rm = T))*200+250)





summary(glm(my.data$cases.reported.decon ~ log(lead(my.data$copies, 5)) + log(lead(my.data$copies, 4)) + log(lead(my.data$copies, 3)) + log(lead(my.data$copies, 2)) + log(lead(my.data$copies, 1)) + log(my.data$copies) + log(lag(my.data$copies, 1))))


summary(glm(my.data$cases.reported.decon ~ ns(lead(my.data$copies,1), 3) + ns(my.data$copies, 3) + ns(lag(my.data$copies, 1), 3)))
```


\newpage
# References

<div id="refs"></div>








\newpage
# (APPENDIX) Appendix {-}

# Data Descriptions {#descriptA}

```{r echo=F, message=F, warning=F, error=F}
rm(list=ls())

```



  
  
```{r alldata, echo=F, message=F, warning=F, error=F}
n1 <- read_csv("./data/raw_data/n1_all_cleaned2.csv")
n2 <- read_csv("./data/raw_data/n2_all_cleaned2.csv")

qc <- read_csv("./data/raw_data/QC/all_curves.csv")
qc2 <- read_xlsx("./data/raw_data/QC/sarscov2_rna_control.xlsx")

plant <- read_csv("./data/raw_data/plant_data.csv")

covid <- read_csv("./consult/01-data/ga_covid_data/epicurve_symptom_date.csv") %>% 
            filter(county=="Clarke") %>% 
            select(symptom.date=`symptom date`, cases, moving_avg_cases)

covid.report <- read_csv("./consult/01-data/ga_covid_data/epicurve_rpt_date.csv") %>% 
            filter(county=="Clarke") %>% 
            select(report_date, cases, moving_avg_cases)

covid.testing <- read_csv("./consult/01-data/ga_covid_data/pcr_positives_col.csv") %>% 
            filter(county=="Clarke") %>% 
            select(collection_date = collection_dt, pcr_tests = `ALL PCR tests performed`, pcr_pos = `All PCR positive tests`)

``` 





## Brief Item Analysis

Table \@ref(tab:descriptA) gives a brief description of the data used in this analysis. Aside from some minor manipulations upon import (e.g., COVID-19 case reports were subset to only include records for Athens Clarke County), the descriptions are for the raw data. From this, we can see some initial data formatting may be necessary. For example, many of the date variables are being read as character strings so they need to be parsed as dates to be useful for analysis. Eventually, the data represented in Table \@ref(tab:descriptA) will be aggregated / condensed to a single dataframe object used in final analyses. 
  
```{r, descriptA, echo=F, message=F, warning=F, error=F}
dataset.descriptions <- 
  lapply(
          1:length(ls()), 
          function(.index){
              df <- get(ls(envir=parent.env(environment()))[.index], envir=parent.env(environment())); 
              obs <- nrow(df); 
              vars <- ncol(df); 
              var.names <- paste(names(df), collapse = ",\n"); 
              var.class <- paste(unlist(lapply(df, class)), collapse = ",\n"); 
              
              return(
                data.frame(
                  df=ls(envir=parent.env(environment()))[.index], 
                  obs=obs, 
                  vars=vars, 
                  var.names=var.names, 
                  var.class=var.class)
                )
              }
          ) %>%
  bind_rows() %>%
  setNames(., nm = c("Dataframe Object", "Number of Observations", "Number of Variables", "Variable Names", "Variable Classes"))

dataset.descriptions <- dataset.descriptions[c(4,5,7,8,6,1,2,3),]

dataset.descriptions$`Variable Labels` <- 
  c(
    paste(
      c("PCR Run Date", "PCR Run ID Number", "Sample Collection Date", "Sample Collection Number", "Sample ID", "Sequence Target", "Cycle Threshold", "Slope of Standard Curve", "Y-intercept of Standard Curve", "Copy Number per Microliter of Reaction"), 
      collapse = ",\n"
      ), 
    paste(
      c("PCR Run Date", "PCR Run ID Number", "Sample Collection Date", "Sample Collection Number", "Sample ID", "Sequence Target", "Cycle Threshold", "Slope of Standard Curve", "Y-intercept of Standard Curve", "Copy Number per Microliter of Reaction"), 
      collapse = ",\n"
      ), 
    paste(
      c("Sample Collection Number", "Sequence Target", "Cycle Threshold", "Concentration Quantity Spiked in Sample", "Logarithm Base 10 of Concentration Quantity"), 
      collapse = ",\n"
      ),
    paste(
      c("Sample Collection Number", "Sequence Target", "Cycle Threshold"), 
      collapse = ",\n"
      ), 
    paste(
      c("Date", "Wastewater Reclamation Facility ID", "Volume of Influent Flow in Millions of Gallons", "Total Suspended Solids Concentration in Milligrams per Liter"), 
      collapse=",\n"
      ), 
    paste(
      c("Date of Symptom Onset", "Number of Cases", "Average Number of Cases in Previous 7 Days"), 
      collapse=",\n"
      ), 
    paste(
      c("Date of Report", "Number of Cases", "Average Number of Cases in Previous 7 Days"), 
      collapse=",\n"
      ), 
    paste(
      c("Date of Specimen Collection", "Total PCR Tests Reported or Collected", "Total Positive PCR Tests"), 
      collapse=",\n"
      )
   )


dataset.descriptions$`Dataframe Description` <- 
  c(
    "RT-qPCR Results for N1", 
    "RT-qPCR Results for N2", 
    "Standard Curve Results", 
    "Positive Controls for SARS-CoV-2", 
    "Wastewater Reclamation Facility Processing Data", 
    "COVID-19 Cases by Symptom Onset Date", 
    "COVID-19 Cases by Report Date", 
    "COVID-19 PCR Diagnostics Data"
  )

dataset.descriptions %<>% select(`Dataframe Object`, `Dataframe Description`, everything())

flextable(dataset.descriptions) %>% 
  border_remove() %>% 
  align(part = "header", align = "center") %>%
  align(j=c(1,3,4), align = "center") %>%
  valign(valign = "top") %>% 
  font(part = "all", fontname = "Arial") %>%
  fontsize(part = "all", size = 8) %>% 
  fontsize(part = "header", size = 9) %>% 
  bold(part = "header") %>%
  width(j=7, width = 2.3) %>%
  width(j=6, width = 0.7) %>%
  width(j=5, width = 1.1) %>%
  width(j=c(1,4), width = 0.8) %>%
  width(j=3, width = 1.0) %>%
  width(j=2, width = 0.9) %>%
  border(part = "header", border.bottom = fp_border_default(color = "black", width = 2)) %>%
  border(part = "body", i=1:7, border.bottom = fp_border_default(color = "black", width = 0.5)) %>% 
  set_caption(caption = "Description of Raw Datasets used in Analyses")

```


  
## Initial Data Management and Cleaning (Formatting)

The separate datasets for the N1 and N2 RT-qPCR results are aggregated to a single dataframe to represent all experimental instances. Similarly, the COVID-19 surveillance data are aggregated to a single dataframe as are the positive controls. Each of the variables across all datasets are appropriately formatted and some additional variables are created to aid in structuring (e.g., the wastewater facility IDs are parsed out of the sample IDs). Additionally, some variables are dropped / deleted. These variables will be recreated in later analyses. Finally, the influent flow data from the wastewater reclamation facilities has a unit of a million gallons (assumed US liquid gallon). The influent flow is converted to liters using the unit conversion formula outlined in Equation \@ref(eq:unit-conversionA).

\begin{equation}
  1~US~liquid~gallon \times \frac{231~in^3}{1~US~liquid~gallon} \times \frac{0.0254^3~m^3}{1~in^3} \times \frac{1000~L}{1~m^3}
  (\#eq:unit-conversionA)
\end{equation}
  
  
```{r manageA, echo=F, message=F, warning=F, error=F}
.wbe_old <- bind_rows(n1, n2) %>% 
    mutate(
        run_date=as.Date(run_date, format = "%d-%b-%y"), 
        sample_date=as.Date(sample_date, format = "%d-%b-%y"), 
        facility=substr(sample_id, 1,2), 
        biological_replicate=substr(sample_id, nchar(sample_id), nchar(sample_id)), 
        ct=as.numeric(ifelse(ct=="Undetermined", NA, ct))
    ) %>% 
    arrange(sample_date, facility, target, biological_replicate)


wbe <- bind_rows(n1, n2) %>% 
          mutate(
            run_date=as.Date(run_date, format = "%d-%b-%y"), 
            sample_date=as.Date(sample_date, format = "%d-%b-%y"), 
            facility=substr(sample_id, 1,2), 
            biological_replicate=substr(sample_id, nchar(sample_id), nchar(sample_id)), 
            ct=as.numeric(ifelse(ct=="Undetermined", NA, ct))
            ) %>% 
          arrange(sample_date, facility, target, biological_replicate) %>% 
          select(sample_date, facility, target, biological_replicate, target, collection_num, run_date, run_num, ct)

rm(n1, n2)

qc <- bind_rows(
  qc %>% 
    select(-log_quant) %>% 
    mutate(df="qc1"), 
  qc2 %>% 
    mutate(quantity = 1e8*3/25*2/20, df="qc2") %>% 
    rename(ct=ct_value)
  )
rm(qc2)

plant %<>% mutate(date = as.Date(date, format = "%m/%d/%Y"), influent_flow_L = influent_flow_mg*1e6*231*(0.0254^3)*1000) %>% select(date, wrf, influent_flow_L, influent_tss_mg_l)


.covid_old <- covid

covid <- full_join(
            covid%>%
              select(cases.symptom.onset=cases, date=symptom.date), 
            covid.report%>%
              select(cases.reported=cases, date=report_date), 
            by = "date"
            ) %>% 
         full_join(
           covid.testing%>%
             rename(date=collection_date), 
           by="date"
           ) %>%
         select(date, cases.symptom.onset, cases.reported, pcr_tests, pcr_pos)

rm(covid.report, covid.testing)
rm(dataset.descriptions)
```
  
  
```{r descriptA2, echo=F, message=F, warning=F, error=F, fig.cap="Table 1. R Objects of Raw Data with Descriptions"}
dataset.descriptions <- 
  lapply(
          1:length(ls()), 
          function(.index){
              df <- get(ls(envir=parent.env(environment()))[.index], envir=parent.env(environment())); 
              obs <- nrow(df); 
              vars <- ncol(df); 
              var.names <- paste(names(df), collapse = ",\n"); 
              var.class <- paste(unlist(lapply(df, class)), collapse = ",\n"); 
              
              return(
                data.frame(
                  df=ls(envir=parent.env(environment()))[.index], 
                  obs=obs, 
                  vars=vars, 
                  var.names=var.names, 
                  var.class=var.class)
                )
              }
          ) %>%
  bind_rows() %>%
  setNames(., nm = c("Dataframe Object", "Number of Observations", "Number of Variables", "Variable Names", "Variable Classes"))

dataset.descriptions <- dataset.descriptions[c(4,3,2,1),]

dataset.descriptions$`Variable Labels` <- 
  c(
    paste(
      c("Sample Collection Date", "Wastewater Reclamation Facility", "Sequence Target", "Biological Replicate ID", "Sample Collection Number", "PCR Run Date", "PCR Run ID Number", "Cycle Threshold"), 
      collapse = ",\n"
      ), 
    paste(
      c("Sample Collection Number", "Sequence Target", "Cycle Threshold", "Concentration Quantity Spiked in Sample", "Dataframe Indicator (two separate before merge)"), 
      collapse = ",\n"
      ), 
    paste(
      c("Date", "Wastewater Reclamation Facility ID", "Volume of Influent Flow in Liters", "Total Suspended Solids Concentration in Milligrams per Liter"), 
      collapse=",\n"
      ), 
    paste(
      c("Date", "Number of Cases with Symptom Onset on the Date", "Number of Cases Reported on the Date", "Total PCR Tests Reported or Collected on the Date", "Total Positive PCR Tests from those Reported or Collected on the Date"), 
      collapse=",\n"
      )
   )


dataset.descriptions$`Dataframe Description` <- 
  c(
    "RT-qPCR Results for All Experiments", 
    "Standard Curves and Positive Controls for SARS-CoV-2", 
    "Wastewater Reclamation Facility Processing Data", 
    "COVID-19 Cases by Symptom Onset Date, Cases by Report Date, and PCR Diagnostics Data"
  )

dataset.descriptions %<>% select(`Dataframe Object`, `Dataframe Description`, everything())

flextable(dataset.descriptions) %>% 
  border_remove() %>% 
  align(part = "header", align = "center") %>%
  align(j=c(1,3,4), align = "center") %>%
  valign(valign = "top") %>% 
  font(part = "all", fontname = "Arial") %>%
  fontsize(part = "all", size = 8) %>% 
  fontsize(part = "header", size = 9) %>% 
  bold(part = "header") %>%
  width(j=7, width = 2.8) %>%
  width(j=6, width = 0.7) %>%
  width(j=5, width = 1.2) %>%
  width(j=c(1,4), width = 0.8) %>%
  width(j=3, width = 1.0) %>%
  width(j=2, width = 0.9) %>%
  border(part = "header", border.bottom = fp_border_default(color = "black", width = 2)) %>%
  border(part = "body", i=1:3, border.bottom = fp_border_default(color = "black", width = 0.5))

```
















# Limits of Detection and Quantification {#morelod}



```{r, echo=F, message=F, warning=F, error=F}
wbe.summary.tr <- wbe %>% 
                    group_by(sample_date, facility, target, biological_replicate) %>% 
                    summarise(
                      n=n(), 
                      n.miss=sum(is.na(ct)), 
                      ct.mean=mean(ct,na.rm=T), 
                      ct.sd=sd(ct,na.rm=T)
                      ) %>% 
                    mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                    ungroup()

wbe.summary.br <- wbe.summary.tr %>%
                    group_by(sample_date, facility, target) %>% 
                    summarise(
                      n.bio = n(),
                      n.bio.non.miss = sum(!is.na(ct.mean)),
                      n.bio.miss = sum(is.na(ct.mean)),
                      n.total = sum(n), 
                      n.total.miss = sum(n.miss), 
                      bio.ct.mean = mean(ct.mean, na.rm = T), 
                      bio.ct.sd = sd(ct.mean, na.rm=T), 
                      tech.ct.dists = paste(paste0(biological_replicate, " = ", round(ct.mean,2), " (sd=", round(ct.sd,2), ", n=", n-n.miss, ")"), collapse = "; ")
                      ) %>%
                      mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                      ungroup()

wbe.missing.profile <- wbe.summary.br %>%
                        mutate(p.missing = n.total.miss / n.total) %>%
                        select(sample_date, facility, target, p.missing)

wbe.missing.profile.overall <- wbe.summary.br %>%
                                group_by(sample_date) %>%
                                summarise(p.missing = sum(n.total.miss) / sum(n.total))
wbe.missing.profile.target <- wbe.summary.br %>%
                                group_by(sample_date, target) %>%
                                summarise(p.missing = sum(n.total.miss) / sum(n.total))
wbe.missing.profile.facility <- wbe.summary.br %>%
                                group_by(sample_date, facility) %>%
                                summarise(p.missing = sum(n.total.miss) / sum(n.total))

```




```{r qq-plots-ctA, echo=F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap="Normal Quantile-Quantile Plots of Observed Cycle Thresholds for Viral Sequence Targets N1 and N2"}

qqnorm.ct.n1 <- qqnorm(wbe$ct[which(wbe$target=="N1")], plot.it = F) %>% as.data.frame()
qqnorm.ct.n2 <- qqnorm(wbe$ct[which(wbe$target=="N2")], plot.it = F) %>% as.data.frame()


qqnorm.Explorer.ct <- function(qqnorm.ct){
        qqnorm.ct <- qqnorm.ct[which(complete.cases(qqnorm.ct)),]
        qqnorm.ct <- qqnorm.ct[order(qqnorm.ct$x),]
        qqnorm.ct <- cbind(qqnorm.ct, rbind(NA, qqnorm.ct[-nrow(qqnorm.ct),])) %>% setNames(., nm = c("x", "y", "x-1", "y-1"))
        qqnorm.ct %<>% mutate(rise = y-`y-1`, run = x-`x-1`) %>% mutate(slope = rise / run)
        
        qqnorm.ct$lod <- NA
        qqnorm.ct$loq <- NA
        
        prev.slope <- 1
        lod.found <- 0
        for(i in nrow(qqnorm.ct):2){
          if(lod.found==0){
            if(qqnorm.ct$slope[i]<1 & prev.slope <1){
              qqnorm.ct$lod[i] <- 1
              lod.found <- 1
            }else{
              prev.slope <- qqnorm.ct$slope[i]
            }
          }
          if(lod.found==1){
            if(qqnorm.ct$slope[i]>1){
              qqnorm.ct$loq[i] <- 1
              break
            }else{
              prev.slope <- qqnorm.ct$slope[i]
            }
          }
        }
        
        
        lod.ct <- qqnorm.ct$y[which(qqnorm.ct$lod==1)]
        loq.ct <- qqnorm.ct$y[which(qqnorm.ct$loq==1)]

        return(list(qqnorm.dataset = qqnorm.ct, lod = lod.ct, loq = loq.ct))
}



qqnorm.ct.n1 <- qqnorm.Explorer.ct(qqnorm.ct.n1)
qqnorm.ct.n2 <- qqnorm.Explorer.ct(qqnorm.ct.n2)
        


# png(filename = "./consult/03-output/limits_ct.png", height = 9, width = 8, units = "in", res = 300)
# layout(matrix(c(1,2), nrow = 2), widths = c(lcm(6*2.54)), heights = c(lcm(6*9/16*2.54), lcm(6*9/16*2.54)))
# par(mar = c(2.1, 2.1, 1.1, 0), fig = c(0,1,0.5,1))
# par(mar = c(2.1, 2.1, 1.1, 0), fig = c(0,1,0,0.5), new = T)
# 
# dev.off()

# 
# par(mfcol = c(2,1), mar = c(2.1, 2.1, 1.1, 0))
# 
# # layout.show(2)
# qqnorm(wbe$ct[which(wbe$target=="N1")],  axes = F, ylab = "", xlab = "", main = "")
# qqline(wbe$ct[which(wbe$target=="N1")], col = "gainsboro")
# axis(1, at = -3:3, tick = T, labels = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
# title(xlab = "Theoretical Quantiles", line = 1)
# 
# axis(2, at = 32:39, tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
# title(ylab = "Observed Cyle Thresholds", line = 1.25)
# 
# abline(h = qqnorm.ct.n1$lod)
# text(par('usr')[1], par('usr')[4], labels = paste("LOD =", round(qqnorm.ct.n1$lod,3)), adj = c(-0.05,1.2))
# abline(h = qqnorm.ct.n1$loq, lty = 3)
# text(par('usr')[1], par('usr')[4], labels = paste("LOQ =", round(qqnorm.ct.n1$loq,3)), adj = c(-0.05,2.4))
# legend("bottomright", lty = c(1,3), legend = c("LOD", "LOQ"))
# title(main = "Normal Q-Q Plot for N1 Cycle Threshold", line = 0.25)
# box()
# 
# 
# text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
#      par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
#      labels = "A", adj = c(0,1), xpd = T, cex = 1, font = 2)
# 
# 
# 
# 
# qqnorm(wbe$ct[which(wbe$target=="N2")],  axes = F, ylab = "", xlab = "", main = "")
# qqline(wbe$ct[which(wbe$target=="N2")], col = "gainsboro")
# axis(1, at = -3:3, tick = T, labels = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
# title(xlab = "Theoretical Quantiles", line = 1)
# 
# axis(2, at = 33:39, tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
# title(ylab = "Observed Cyle Thresholds", line = 1.25)
# 
# 
# abline(h = qqnorm.ct.n2$lod)
# text(par('usr')[1], par('usr')[4], labels = paste("LOD =", round(qqnorm.ct.n2$lod,3)), adj = c(-0.05,1.2))
# abline(h = qqnorm.ct.n2$loq, lty = 3)
# text(par('usr')[1], par('usr')[4], labels = paste("LOQ =", round(qqnorm.ct.n2$loq,3)), adj = c(-0.05,2.4))
# legend("bottomright", lty = c(1,3), legend = c("LOD", "LOQ"))
# title(main = "Normal Q-Q Plot for N2 Cycle Threshold", line = 0.25)
# box()
# text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
#      par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
#      labels = "B", adj = c(0,1), xpd = T, cex = 1, font = 2)
# 
# # dev.off()

```

Table \@ref(tab:limits-table) explores where the cycle threshold data fall with respect to these newly defined limits. This table is three separate tables (A, B, C) stacked on top of one another showing summaries at the different hierarchy levels within the data: A = technical replicates; B = biological replicates; and, C = sampling days. 

```{r limits-tableA, echo=F, message=F, warning=F, error=F}
wbe.summary.lod <- wbe %>% 
                        mutate(
                          ct.b.lod = ifelse(target=="N1", ct>qqnorm.ct.n1$lod, ct>qqnorm.ct.n2$lod),
                          ct.loq.lod = ifelse(target=="N1", 
                                              ct>qqnorm.ct.n1$loq & ct<=qqnorm.ct.n1$lod, 
                                              ct>qqnorm.ct.n2$loq & ct<=qqnorm.ct.n2$lod), 
                          ct.good = ifelse(target =="N1", 
                                           ct<=qqnorm.ct.n1$loq, 
                                           ct<=qqnorm.ct.n2$loq)
                        ) %>%
                        group_by(sample_date, facility, target, biological_replicate) %>% 
                        summarise(
                          n=n(), 
                          n.miss = sum(is.na(ct)), 
                          n.b.lod = sum(ct.b.lod, na.rm = T),
                          n.loq.lod = sum(ct.loq.lod, na.rm = T), 
                          n.good = sum(ct.good, na.rm = T)
                        ) %>% 
                        mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                        ungroup() %>%
  
                        group_by(sample_date, facility, target) %>% 
                        summarise(
                          n.bio = n(),
                          n.bio.miss = sum(n==n.miss),
                          n.bio.b.lod = sum(n==n.miss+n.b.lod), 
                          n.bio.loq.lod = sum(n==n.miss+n.b.lod+n.loq.lod),
                          n.bio.good = sum(n!=n.miss+n.b.lod+n.loq.lod),

                          n.total = sum(n), 
                          n.total.miss = sum(n.miss),
                          n.total.b.lod = sum(n.b.lod), 
                          n.total.loq.lod = sum(n.loq.lod), 
                          n.total.good = sum(n.good)
                      ) %>%
                        mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                        ungroup() %>%

                        group_by(facility, target) %>%
                        summarise(
                          n.days = n(), 
                          n.days.miss = sum(n.bio == n.bio.miss), 
                          n.days.b.lod = sum(n.bio == n.bio.b.lod), 
                          n.days.loq.lod = sum(n.bio == n.bio.loq.lod), 
                          n.days.good = sum(n.bio != n.bio.loq.lod),
                          
                          n.bio = sum(n.bio), 
                          n.bio.miss = sum(n.bio.miss), 
                          n.bio.b.lod = sum(n.bio.b.lod), 
                          n.bio.loq.lod = sum(n.bio.loq.lod), 
                          n.bio.good = sum(n.bio.good),
                          
                          n.total = sum(n.total), 
                          n.total.miss = sum(n.total.miss), 
                          n.total.b.lod = sum(n.total.b.lod), 
                          n.total.loq.lod = sum(n.total.loq.lod), 
                          n.total.good = sum(n.total.good)
                        ) %>%
                        mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                        ungroup()

wbe.lod.tr <- wbe.summary.lod %>% 
  mutate(
    total.miss = paste0(n.total.miss, " (", round(n.total.miss/n.total*100,1), ")"), 
    total.b.lod = paste0(n.total.b.lod, " (", round(n.total.b.lod/n.total*100, 2), ")"), 
    total.loq.lod = paste0(n.total.loq.lod, " (", round(n.total.loq.lod / n.total*100, 1), ")"), 
    total.good = paste0(n.total.good, " (", round(n.total.good/n.total*100,1), ")")) %>% 
  select(facility, target, total.miss, total.b.lod, total.loq.lod, total.good)




wbe.lod.bio <- wbe.summary.lod %>% 
  mutate(
    bio.miss = paste0(round(n.bio.miss/n.bio*100,1), " (", n.bio.miss, " / ", n.bio, ")"), 
    bio.b.lod = paste0(round(n.bio.b.lod/n.bio*100, 1), " (", n.bio.b.lod, " / ", n.bio, ")"), 
    bio.loq.lod = paste0(round(n.bio.loq.lod / (n.bio)*100, 1), " (", n.bio.loq.lod, " / ", n.bio, ")"), 
    bio.good = paste0(round(n.bio.good/(n.bio)*100,1), " (", n.bio.good, " / ", n.bio, ")")) %>% 
  select(facility, target, bio.miss, bio.b.lod, bio.loq.lod, bio.good)





wbe.lod.days <- wbe.summary.lod %>% 
  mutate(
    days.miss = paste0(round(n.days.miss/n.days*100,1), " (", n.days.miss, " / ", n.days, ")"), 
    days.b.lod = paste0(round(n.days.b.lod/n.days*100, 1), " (", n.days.b.lod, " / ", n.days, ")"), 
    days.loq.lod = paste0(round(n.days.loq.lod / (n.days)*100, 1), " (", n.days.loq.lod, " / ", n.days, ")"), 
    days.good = paste0(round(n.days.good/(n.days)*100,1), " (", n.days.good, " / ", n.days, ")")) %>% 
  select(facility, target, days.miss, days.b.lod, days.loq.lod, days.good)



wbe.lod.tr %<>% ungroup() %>% add_row(.before = 1) %>% add_row(.before = 1)
wbe.lod.tr[2,] <- as.list(names(wbe.lod.tr))

wbe.lod.bio %<>% ungroup() %>% add_row(.before = 1) %>% add_row(.before = 1)
wbe.lod.bio[2,] <- as.list(names(wbe.lod.bio))
names(wbe.lod.bio) <- names(wbe.lod.tr)

wbe.lod.days %<>% ungroup() %>% add_row(.before = 1) %>% add_row(.before = 1)
wbe.lod.days[2,] <- as.list(names(wbe.lod.days))
names(wbe.lod.days) <- names(wbe.lod.tr)

lod.tab <- rbind(wbe.lod.tr%>%add_row(), wbe.lod.bio%>%add_row(), wbe.lod.days)




flextable(lod.tab) %>%
  delete_part(part = "header") %>%
  border_remove() %>%
  
  border(part = "body", i = which(is.na(lod.tab$facility))[c(2,4)], border.top = fp_border_default(color = "black", width = 0.5)) %>%
  
  border(part = "body", i = c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), border.bottom = fp_border_default(color = "black", width = 2)) %>% 
  
  font(part = "all", fontname = "Arial") %>%
  fontsize(part = "body", size = 8) %>%
  fontsize(part = "body", i = c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), size = 9) %>%
  bold(part = "body", i = c(2, which(is.na(lod.tab$facility))[c(2,4)]+2)) %>% 
  
  merge_v(j = 1) %>% 
  
  width(j=1:2, width = 1) %>%
  width(j=3:6, width = 1.25) %>%
  
  align(part = "body", j = 3:6, align = "right") %>%
  align(part = "body", i = c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), align = "center") %>%
  valign(part = "body", j = 1, valign = "top") %>% 
  
  set_caption(caption = "Distributions of Cycle Thresholds at Each Hierarchy") %>% 
  
  compose(part="body", j=1, i=c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), value = as_paragraph("Wastewater Reclamation Facility")) %>% 
  compose(part="body", j=2, i=c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), value = as_paragraph("Viral Sequence Target")) %>%
  compose(part="body", j=3, i=2, value = as_paragraph("Tech: Undetermined")) %>%
  compose(part="body", j=4, i=2, value = as_paragraph("Tech: Above LOD")) %>%
  compose(part="body", j=5, i=2, value = as_paragraph("Tech: Below LOD & Above LOQ")) %>%
  compose(part="body", j=6, i=2, value = as_paragraph("Tech: Below LOQ")) %>%
  
  compose(part="body", j=3, i=which(is.na(lod.tab$facility))[2]+2, value = as_paragraph("Bio: All TR Undetermined")) %>%
  compose(part="body", j=4, i=which(is.na(lod.tab$facility))[2]+2, value = as_paragraph("Bio: All TR Above LOD")) %>%
  compose(part="body", j=5, i=which(is.na(lod.tab$facility))[2]+2, value = as_paragraph("Bio: All TR Above LOQ")) %>%
  compose(part="body", j=6, i=which(is.na(lod.tab$facility))[2]+2, value = as_paragraph("Bio: Others")) %>%
  
  compose(part="body", j=3, i=which(is.na(lod.tab$facility))[5]+1, value = as_paragraph("Days: All BR Undetermined")) %>%
  compose(part="body", j=4, i=which(is.na(lod.tab$facility))[5]+1, value = as_paragraph("Days: All BR Above LOD")) %>%
  compose(part="body", j=5, i=which(is.na(lod.tab$facility))[5]+1, value = as_paragraph("Days: All BR Above LOQ")) %>%
  compose(part="body", j=6, i=which(is.na(lod.tab$facility))[5]+1, value = as_paragraph("Days: Others")) %>%
  
  compose(part="body", j=1, i=1, value = as_paragraph("A")) %>%
  compose(part="body", j=1, i=which(is.na(lod.tab$facility))[2], value = as_paragraph("B")) %>%
  compose(part="body", j=1, i=which(is.na(lod.tab$facility))[4], value = as_paragraph("C")) %>%
  bold(part="body", j=1, i=c(1, which(is.na(lod.tab$facility))[c(2,4)])) %>%
  fontsize(part="body", j=1, i=c(1, which(is.na(lod.tab$facility))[c(2,4)]), size = 12)



```

Table \@ref(tab:limits-tableA) has some overlap in presented data with Table \@ref(tab:prop-detect); specifically, the frequencies for undetermined results. However, the additional columns show other breakdowns of the cycle threshold values within the data.   
  
There are only `r sum(wbe.summary.lod$n.total.b.lod)` technical replicates with Ct values that are above the newly specified limit of detection (Part A, Table \@ref(tab:limits-tableA)).[^limit-interpret-ct] As such, the columns `Bio: All TR Above LOD` (Part B) and `Days: All BR Above LOD` (Part C) are nearly identical to their respective columns for undetermined results. 

The Part A column for `Tech: Below LOD & Above LOQ` shows the frequency of ct values falling between the limits of detection and quantification, a gray area with respect to distinguishing concentrations of genetic material. The Part A column for `Tech: Below LOQ` shows the frequency of ct values falling below the limit of quantification, a favorable range. The similarly names columns within Parts B and C have a bit different approach in their frequency calculations and, consequently, their interpretations. For Part B `Bio: All TR Above LOQ`, the frequencies refer to biological replicates where ***all*** technical replicates ct values fell above the limit of detection. The `Bio: Non-Miss Below LOQ` column gives the frequencies of biological replicates that have at least one technical replicate with a ct value below the limit of quantification. The Part C columns similarly give aggregations of biological replicates instead of technical replicates as in Part B. Note that in Parts B and C, the columns from left to right, or from all undetermined to all above LOQ correspond to increasingly inclusive thresholds so that the frequencies for all above LOQ are always greater or equal to the other two columns. Also, for Parts B and C the frequencies in the All above LOQ and Others columns sum to account for the total number of replicates or days. 




## An Alternative Summary Table




```{r limits-table2, eval=F, echo=F, message=F, warning=F, error=F}
wbe.summary.lod <- wbe %>% 
                        mutate(
                          ct.b.lod = ifelse(target=="N1", ct>qqnorm.ct.n1$lod, ct>qqnorm.ct.n2$lod),
                          ct.loq.lod = ifelse(target=="N1", 
                                              ct>qqnorm.ct.n1$loq & ct<=qqnorm.ct.n1$lod, 
                                              ct>qqnorm.ct.n2$loq & ct<=qqnorm.ct.n2$lod), 
                          ct.good = ifelse(target =="N1", 
                                           ct<=qqnorm.ct.n1$loq, 
                                           ct<=qqnorm.ct.n2$loq)
                        ) %>%
                        group_by(sample_date, facility, target, biological_replicate) %>% 
                        summarise(
                          n=n(), 
                          n.miss = sum(is.na(ct)), 
                          n.b.lod = sum(ct.b.lod, na.rm = T),
                          n.loq.lod = sum(ct.loq.lod, na.rm = T), 
                          n.good = sum(ct.good, na.rm = T)
                        ) %>% 
                        mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                        ungroup() %>%
  
                        group_by(sample_date, facility, target) %>% 
                        summarise(
                          n.bio = n(),
                          n.bio.miss = sum(n==n.miss),
                          n.bio.b.lod = sum(n==n.miss+n.b.lod), 
                          n.bio.loq.lod = sum(n-n.miss-n.b.lod==n.loq.lod & n.loq.lod!=0 & n.good == 0),
                          n.bio.good = sum(n==n.good),

                          n.total = sum(n), 
                          n.total.miss = sum(n.miss),
                          n.total.b.lod = sum(n.b.lod), 
                          n.total.loq.lod = sum(n.loq.lod), 
                          n.total.good = sum(n.good)
                      ) %>%
                        mutate_all(function(x){ifelse(is.nan(x), NA, x)}) %>% 
                        ungroup() %>%

                        group_by(facility, target) %>%
                        summarise(
                          n.days = n(), 
                          n.days.miss = sum(n.bio == n.bio.miss), 
                          n.days.b.lod = sum(n.bio == n.bio.b.lod), 
                          n.days.loq.lod = sum(n.bio - n.bio.b.lod == n.bio.loq.lod & n.bio.loq.lod!=0), 
                          n.days.good = sum(n.bio - n.bio.b.lod == n.bio.good & n.bio.good != 0),
                          
                          n.bio = sum(n.bio), 
                          n.bio.miss = sum(n.bio.miss), 
                          n.bio.b.lod = sum(n.bio.b.lod), 
                          n.bio.loq.lod = sum(n.bio.loq.lod), 
                          n.bio.good = sum(n.bio.good),
                          
                          n.total = sum(n.total), 
                          n.total.miss = sum(n.total.miss), 
                          n.total.b.lod = sum(n.total.b.lod), 
                          n.total.loq.lod = sum(n.total.loq.lod), 
                          n.total.good = sum(n.total.good)
                        )

wbe.lod.tr <- wbe.summary.lod %>% 
  mutate(
    total.miss = paste0(n.total.miss, " (", round(n.total.miss/n.total*100,1), ")"), 
    total.b.lod = paste0(n.total.b.lod, " (", round(n.total.b.lod/n.total*100, 2), ")"), 
    total.loq.lod = paste0(n.total.loq.lod, " (", round(n.total.loq.lod / n.total*100, 1), ")"), 
    total.good = paste0(n.total.good, " (", round(n.total.good/n.total*100,1), ")")) %>% 
  select(facility, target, total.miss, total.b.lod, total.loq.lod, total.good)




wbe.lod.bio <- wbe.summary.lod %>% 
  mutate(
    bio.miss = paste0(round(n.bio.miss/n.bio*100,1), " (", n.bio.miss, " / ", n.bio, ")"), 
    bio.b.lod = paste0(round(n.bio.b.lod/n.bio*100, 1), " (", n.bio.b.lod, " / ", n.bio, ")"), 
    bio.loq.lod = paste0(round(n.bio.loq.lod / (n.bio-n.bio.b.lod)*100, 1), " (", n.bio.loq.lod, " / ", n.bio-n.bio.b.lod, ")"), 
    bio.good = paste0(round(n.bio.good/(n.bio-n.bio.b.lod)*100,1), " (", n.bio.good, " / ", n.bio-n.bio.b.lod, ")")) %>% 
  select(facility, target, bio.miss, bio.b.lod, bio.loq.lod, bio.good)





wbe.lod.days <- wbe.summary.lod %>% 
  mutate(
    days.miss = paste0(round(n.days.miss/n.days*100,1), " (", n.days.miss, " / ", n.days, ")"), 
    days.b.lod = paste0(round(n.days.b.lod/n.days*100, 1), " (", n.days.b.lod, " / ", n.days, ")"), 
    days.loq.lod = paste0(round(n.days.loq.lod / (n.days-n.days.b.lod)*100, 1), " (", n.days.loq.lod, " / ", n.days-n.days.b.lod, ")"), 
    days.good = paste0(round(n.days.good/(n.days-n.days.b.lod)*100,1), " (", n.days.good, " / ", n.days-n.days.b.lod, ")")) %>% 
  select(facility, target, days.miss, days.b.lod, days.loq.lod, days.good)



wbe.lod.tr %<>% ungroup() %>% add_row(.before = 1) %>% add_row(.before = 1)
wbe.lod.tr[2,] <- as.list(names(wbe.lod.tr))

wbe.lod.bio %<>% ungroup() %>% add_row(.before = 1) %>% add_row(.before = 1)
wbe.lod.bio[2,] <- as.list(names(wbe.lod.bio))
names(wbe.lod.bio) <- names(wbe.lod.tr)

wbe.lod.days %<>% ungroup() %>% add_row(.before = 1) %>% add_row(.before = 1)
wbe.lod.days[2,] <- as.list(names(wbe.lod.days))
names(wbe.lod.days) <- names(wbe.lod.tr)

lod.tab <- rbind(wbe.lod.tr%>%add_row(), wbe.lod.bio%>%add_row(), wbe.lod.days)




flextable(lod.tab) %>%
  delete_part(part = "header") %>%
  border_remove() %>%
  
  border(part = "body", i = which(is.na(lod.tab$facility))[c(2,4)], border.top = fp_border_default(color = "black", width = 0.5)) %>%
  
  border(part = "body", i = c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), border.bottom = fp_border_default(color = "black", width = 2)) %>% 
  
  font(part = "all", fontname = "Arial") %>%
  fontsize(part = "body", size = 8) %>%
  fontsize(part = "body", i = c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), size = 9) %>%
  bold(part = "body", i = c(2, which(is.na(lod.tab$facility))[c(2,4)]+2)) %>% 
  
  merge_v(j = 1) %>% 
  
  width(j=1:2, width = 1) %>%
  width(j=3:6, width = 1.25) %>%
  
  align(part = "body", j = 3:6, align = "right") %>%
  align(part = "body", i = c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), align = "center") %>%
  valign(part = "body", j = 1, valign = "top") %>% 
  
  set_caption(caption = "Distributions of Cycle Thresholds at Each Hierarchy") %>% 
  
  compose(part="body", j=1, i=c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), value = as_paragraph("Wastewater Reclamation Facility")) %>% 
  compose(part="body", j=2, i=c(2, which(is.na(lod.tab$facility))[c(2,4)]+2), value = as_paragraph("Viral Sequence Target")) %>%
  compose(part="body", j=3, i=2, value = as_paragraph("Tech: Undetermined")) %>%
  compose(part="body", j=4, i=2, value = as_paragraph("Tech: Above LOD")) %>%
  compose(part="body", j=5, i=2, value = as_paragraph("Tech: Below LOD & Above LOQ")) %>%
  compose(part="body", j=6, i=2, value = as_paragraph("Tech: Below LOQ")) %>%
  
  compose(part="body", j=3, i=which(is.na(lod.tab$facility))[2]+2, value = as_paragraph("Bio: All TR Undetermined")) %>%
  compose(part="body", j=4, i=which(is.na(lod.tab$facility))[2]+2, value = as_paragraph("Bio: All TR Above LOD")) %>%
  compose(part="body", j=5, i=which(is.na(lod.tab$facility))[2]+2, value = as_paragraph("Bio: Non-Miss Below LOD & Above LOQ")) %>%
  compose(part="body", j=6, i=which(is.na(lod.tab$facility))[2]+2, value = as_paragraph("Bio: Non-Miss Below LOQ")) %>%
  
  compose(part="body", j=3, i=which(is.na(lod.tab$facility))[5]+1, value = as_paragraph("Days: All BR Undetermined")) %>%
  compose(part="body", j=4, i=which(is.na(lod.tab$facility))[5]+1, value = as_paragraph("Days: All BR Above LOD")) %>%
  compose(part="body", j=5, i=which(is.na(lod.tab$facility))[5]+1, value = as_paragraph("Days: Non-Miss Below LOD & Above LOQ")) %>%
  compose(part="body", j=6, i=which(is.na(lod.tab$facility))[5]+1, value = as_paragraph("Days: Non-Miss Below LOQ")) %>%
  
  vline(i = {which(is.na(lod.tab$facility))[2]+3}:{which(is.na(lod.tab$facility))[4]-1}, j = 4, border = fp_border_default(color = "black", width = 0.5)) %>% 
  vline(i = {which(is.na(lod.tab$facility))[4]+3}:nrow(lod.tab), j = 4, border = fp_border_default(color = "black", width = 0.5)) %>% 
  
  compose(part="body", j=1, i=1, value = as_paragraph("A")) %>%
  compose(part="body", j=1, i=which(is.na(lod.tab$facility))[2], value = as_paragraph("B")) %>%
  compose(part="body", j=1, i=which(is.na(lod.tab$facility))[4], value = as_paragraph("C")) %>%
  bold(part="body", j=1, i=c(1, which(is.na(lod.tab$facility))[c(2,4)])) %>%
  fontsize(part="body", j=1, i=c(1, which(is.na(lod.tab$facility))[c(2,4)]), size = 12)



```


The Part A column for `Tech: Below LOD & Above LOQ` shows the frequency of ct values falling between the limits of detection and quantification, a gray area with respect to distinguishing concentrations of genetic material. The Part A column for `Tech: Below LOQ` shows the frequency of ct values falling below the limit of quantification, a favorable range. The similarly names columns within Parts B and C have a bit different approach in their frequency calculations and, consequently, their interpretations. For Part B `Bio: Non-Miss Below LOD & Above LOQ`, the frequencies refer to biological replicates where ***all*** technical replicates ct values fell between the limits. The denominators given for the relative frequencies correspond to the number of biological replicates excluding the ones where all technical replicates yielded undetermined results. The `Bio: Non-Miss Below LOQ` column gives the frequencies of biological replicates for which all technical replicates yielded ct values below the limit of quantification. Note that these two instances of values either purely between the limits or below the LOQ do not account for all the biological replicates excluding the ones where all technical replicates yielded undetermined results. The Part C columns similarly give aggregations of biological replicates instead of technical replicates as in Part B. 



# Fitting of Standard Curves {#morestandardcurves}


```{r standard-curvesA, echo = F, message=F, warning=F, error=F}
# summary(qc)
# plot(ct~log10(quantity), data=qc[which(qc$target=="N1" & qc$df=="qc1"),])
# lm(ct~log10(quantity), data=qc[which(qc$target=="N1" & qc$df=="qc1"),])
# 
# plot(ct~log10(quantity), data=qc[which(qc$target=="N2" & qc$df=="qc1"),])
# lm(ct~log10(quantity), data=qc[which(qc$target=="N2" & qc$df=="qc1"),])

qc.means <- qc %>% filter(df=="qc1") %>% group_by(target, quantity) %>% summarise(n= n(), mean.ct = mean(ct), sd.ct = sd(ct))

sc.mean.n1 <- lm(mean.ct~log10(quantity), data=qc.means[which(qc.means$target=="N1"),])
sc.mean.n2 <- lm(mean.ct~log10(quantity), data=qc.means[which(qc.means$target=="N2"),])

# i=1
# repeat{
#   cn <- unique(qc$collection_num)[i]
#   
#   cat("\n \n \n")
#   print(cn)
#   
#   cat("\n \n \n N1")
#   try(print(summary(lm(ct~log10(quantity), data=qc[which(qc$target=="N1" & qc$collection_num==cn), ]))))
#   
#   cat("\n \n \n N2")
#   try(print(summary(lm(ct~log10(quantity), data=qc[which(qc$target=="N2" & qc$collection_num==cn), ]))))
#   
#   continue <- readline("Continue?")
#   
#   if(continue==0){break}
#   i = i+1
# }

n1.cn <- 13
n2.cn <- 24



qc.by.target <- split(qc %>% filter(df=="qc1"), f = qc$target[which(qc$df=="qc1")])

qc.by.target.by.collection.number <- lapply(qc.by.target, function(x){split(x, f = x$collection_num)})

qc.standard.curves <- lapply(qc.by.target.by.collection.number, function(x){lapply(x, function(y){lm(ct~log10(quantity), data=y)})})

qc.sc.ests <- lapply(qc.standard.curves, function(x){lapply(x, function(y){c(coef(y), r2=summary(y)$r.squared)})})

qc.sc.ests <- lapply(qc.sc.ests, function(x){bind_rows(x, .id = "CN")}) %>% bind_rows(.id="target")

qc.sc.ests$Efficiency <- 10^(-1/qc.sc.ests$`log10(quantity)`)-1

qc.sc.ests.summary <- qc.sc.ests %>% 
  group_by(target) %>% 
  summarise(
    `(Intercept)`=paste0(round(mean(`(Intercept)`), 3), " (", round(sd(`(Intercept)`),3), ")"), 
    `log10(quantity)`=paste0(round(mean(`log10(quantity)`),3), " (", round(sd(`log10(quantity)`),3), ")"), 
    r2 = paste0(round(mean(r2),3), " (", round(sd(r2),3), ")"), 
    Efficiency = paste0(round(mean(Efficiency),3), " (", round(sd(Efficiency),3), ")")
    ) %>% 
  ungroup() %>% 
  mutate(CN = "MEAN (SD)") %>% 
  bind_rows(qc.sc.ests%>%mutate_if(is.numeric, ~round(.,3))%>%mutate_all(~as.character(.)), .) %>% 
  arrange(target, CN)





as_grouped_data(qc.sc.ests.summary, "target") %>% 
  flextable() %>% 
  border_remove() %>% 
  align(part = "all", align = "right") %>%
  align(part = "header", align = "center") %>%
  align(part = "body", j = 1, align = "left") %>%
  hline(part = "header", i = 1, border = fp_border_default(color = "black", width = 2)) %>%
  hline(part = "body", i = which(!is.na(as_grouped_data(qc.sc.ests.summary, "target")$target)), border = fp_border_default(color = "black", width = 0.5)) %>% 
  bold(part = "header") %>% 
  bold(part = "body", i = which({qc.sc.ests.summary$CN==13 & qc.sc.ests.summary$target=="N1"} | {qc.sc.ests.summary$CN==24 & qc.sc.ests.summary$target=="N2"}) + c(1,2)) %>%
  font(part = "all", fontname = "Arial") %>%
  fontsize(part = "header", size = 9) %>% 
  fontsize(part = "body", size = 8)%>%
  autofit() %>% 
  set_caption("Standard Curve Linear Regression Fits")
 

```




The rows of Table \@ref(tab:standard-curvesA) for target N1, collection number 13 and target N2, collection number 24 are bold as they were selected as the ideal candidates for the calibration. Briefly, these single runs were chosen due to the variability in the standard curves as a whole. The variability in the data is not representative of the process itself, rather due mainly to the quality of the samples used in the experiment; the samples with known concentrations of substrate are known to degrade over time, yielding inconsistent results.





























# Data Adjustment using Limits of Detection and Quantification {#scenarios}

It is relatively common to use the limit of detection to replace undetermined results within the data. So, I have concocted a few scenarios and created a few datasets varying on the replacement of undetermined values within the dataset. Briefly, scenario 1 uses the data as is without replacing undetermined values. Scenario 2 uses half of the limit of detection to replace undetermined values and half the limit of quantification to replace the data with concentrations below that limit. Scenario 3 again uses half of the limit of detection for undetermined, but also scales values between the LOD and LOQ to stretch from the LOQ to the LOD/2. Scenario 4 replaces undetermined values with the LOD scaled to incorporate the relative frequency of undetermined results and, then, scales the values between the LOD and LOQ to the missing frequency-scaled LOD. Scenario 5 does not replace undetermined values, but does scale the values below the LOQ to half of the LOD. Figure \@ref(fig:scenario-hists) shows histograms of each of the datasets. 


```{r, echo = F, message=F, warning=F, error=F}
n1.int <- qc.sc.ests$`(Intercept)`[which(qc.sc.ests$target=="N1" & qc.sc.ests$CN==n1.cn)]
n1.slope <- qc.sc.ests$`log10(quantity)`[which(qc.sc.ests$target=="N1" & qc.sc.ests$CN==n1.cn)]
n2.int <- qc.sc.ests$`(Intercept)`[which(qc.sc.ests$target=="N2" & qc.sc.ests$CN==n2.cn)]
n2.slope <- qc.sc.ests$`log10(quantity)`[which(qc.sc.ests$target=="N2" & qc.sc.ests$CN==n2.cn)]

wbe %<>% 
  mutate(copies_per_uL_rxn = ifelse(target=="N1", 
                                           10^(-(n1.int-ct)/n1.slope), 
                                           10^(-(n2.int-ct)/n2.slope)
                                           )
                ) %>%
  mutate(copies_per_uL = copies_per_uL_rxn*20/2*25/3*60)      

n1.lod.copies_per_uL <- 10^(-(n1.int-qqnorm.ct.n1$lod)/n1.slope) * 20/2*25/3*60
n1.loq.copies_per_uL <- 10^(-(n1.int-qqnorm.ct.n1$loq)/n1.slope) * 20/2*25/3*60
n2.lod.copies_per_uL <- 10^(-(n2.int-qqnorm.ct.n2$lod)/n2.slope) * 20/2*25/3*60
n2.loq.copies_per_uL <- 10^(-(n2.int-qqnorm.ct.n2$loq)/n2.slope) * 20/2*25/3*60



# Scenario 1 -- raw data
## leave missings out, calculate correlations with positive results only
## leave all values between LOD and LOQ as is







# Scenario 2 -- simple replacement
## use LOD/2 to replace missings
## use LOQ/2 to replace values between LOD and LOQ


wbe2 <- wbe %>% 
          mutate(copies_per_uL = ifelse(target == "N1", 
                                        ifelse(
                                          copies_per_uL < n1.lod.copies_per_uL | is.na(copies_per_uL), 
                                          n1.lod.copies_per_uL/2, 
                                          ifelse(
                                            copies_per_uL < n1.loq.copies_per_uL, 
                                            n1.loq.copies_per_uL/2,
                                            copies_per_uL
                                          )),
                                        ifelse(
                                          copies_per_uL < n2.lod.copies_per_uL | is.na(copies_per_uL), 
                                          n2.lod.copies_per_uL/2,
                                          ifelse(
                                            copies_per_uL < n2.loq.copies_per_uL, 
                                            n2.loq.copies_per_uL/2,
                                            copies_per_uL
                                          )
                                        )
                                  )
                 )









# Scenario 3 -- simple replacement with scaled LOQ
## use LOD/2 to replace missings
## scale values between LOD and LOQ from LOQ to LOD/2

n1.lod.loq.min <- min(wbe$copies_per_uL[which(wbe$copies_per_uL>=n1.lod.copies_per_uL & wbe$copies_per_uL<n1.loq.copies_per_uL & wbe$target=="N1")], na.rm = T)

n1.lod.loq.max <- max(wbe$copies_per_uL[which(wbe$copies_per_uL>=n1.lod.copies_per_uL & wbe$copies_per_uL<n1.loq.copies_per_uL & wbe$target=="N1")], na.rm = T)

n1.lod.loq.rdiff <- diff(range(wbe$copies_per_uL[which(wbe$copies_per_uL>=n1.lod.copies_per_uL & wbe$copies_per_uL<n1.loq.copies_per_uL & wbe$target=="N1")], na.rm = T))


n2.lod.loq.min <- min(wbe$copies_per_uL[which(wbe$copies_per_uL>=n2.lod.copies_per_uL & wbe$copies_per_uL<n2.loq.copies_per_uL & wbe$target=="N2")], na.rm = T)

n2.lod.loq.max <- max(wbe$copies_per_uL[which(wbe$copies_per_uL>=n2.lod.copies_per_uL & wbe$copies_per_uL<n2.loq.copies_per_uL & wbe$target=="N2")], na.rm = T)

n2.lod.loq.rdiff <- diff(range(wbe$copies_per_uL[which(wbe$copies_per_uL>=n2.lod.copies_per_uL & wbe$copies_per_uL<n2.loq.copies_per_uL & wbe$target=="N2")], na.rm = T))




wbe3 <- wbe %>% 
          mutate(copies_per_uL = ifelse(target == "N1", 
                                        ifelse(
                                          copies_per_uL < n1.lod.copies_per_uL | is.na(copies_per_uL), 
                                          n1.lod.copies_per_uL/2, 
                                          ifelse(
                                            copies_per_uL < n1.loq.copies_per_uL, 
                                            copies_per_uL+(copies_per_uL-n1.lod.loq.max)/n1.lod.loq.rdiff*(n1.lod.loq.min-(n1.lod.copies_per_uL/2)),
                                            copies_per_uL
                                          )),
                                        ifelse(
                                          copies_per_uL < n2.lod.copies_per_uL | is.na(copies_per_uL), 
                                          n2.lod.copies_per_uL/2,
                                          ifelse(
                                            copies_per_uL < n2.loq.copies_per_uL, 
                                            copies_per_uL+(copies_per_uL-n2.lod.loq.max)/n2.lod.loq.rdiff*(n2.lod.loq.min-(n2.lod.copies_per_uL/2)),
                                            copies_per_uL
                                          )
                                        )
                                  )
                 )

# View(cbind(wbe$copies_per_uL[which(wbe$copies_per_uL>=n1.lod.copies_per_uL & wbe$copies_per_uL<n1.loq.copies_per_uL & wbe$target=="N1")], wbe3$copies_per_uL[which(wbe$copies_per_uL>=n1.lod.copies_per_uL & wbe$copies_per_uL<n1.loq.copies_per_uL & wbe$target=="N1")]))
# 
# 
# View(cbind(wbe$copies_per_uL[which(wbe$copies_per_uL>=n2.lod.copies_per_uL & wbe$copies_per_uL<n2.loq.copies_per_uL & wbe$target=="N2")], wbe3$copies_per_uL[which(wbe$copies_per_uL>=n2.lod.copies_per_uL & wbe$copies_per_uL<n2.loq.copies_per_uL & wbe$target=="N2")]))






# Scenario 4 -- scaling replacement
## use LOD scaled by proportion of missings to replace missings
### if 1/2 tech reps are missing, then LOD*0.5
## scale values between LOD and LOQ from LOQ to scaled LOD



wbe4 <- wbe %>% left_join(., wbe.summary.br %>% mutate(p.miss.tr = n.total.miss / n.total) %>% select(sample_date, facility, target, p.miss.tr), by = c("sample_date", "facility", "target")) %>% 
          mutate(copies_per_uL = ifelse(target == "N1", 
                                        ifelse(
                                          copies_per_uL < n1.lod.copies_per_uL | is.na(copies_per_uL), 
                                          n1.lod.copies_per_uL/(2+2*p.miss.tr), 
                                          ifelse(
                                            copies_per_uL < n1.loq.copies_per_uL, 
                                            copies_per_uL+(copies_per_uL-n1.lod.loq.max)/n1.lod.loq.rdiff*(n1.lod.loq.min-(n1.lod.copies_per_uL/(2+2*p.miss.tr))),
                                            copies_per_uL
                                          )),
                                        ifelse(
                                          copies_per_uL < n2.lod.copies_per_uL | is.na(copies_per_uL), 
                                          n2.lod.copies_per_uL/(2+2*p.miss.tr),
                                          ifelse(
                                            copies_per_uL < n2.loq.copies_per_uL, 
                                            copies_per_uL+(copies_per_uL-n2.lod.loq.max)/n2.lod.loq.rdiff*(n2.lod.loq.min-(n2.lod.copies_per_uL/(2+2*p.miss.tr))),
                                            copies_per_uL
                                          )
                                        )
                                  )
                 )


# View(cbind(wbe$copies_per_uL[which(wbe$copies_per_uL>=n1.lod.copies_per_uL & wbe$copies_per_uL<n1.loq.copies_per_uL & wbe$target=="N1")], wbe4$copies_per_uL[which(wbe$copies_per_uL>=n1.lod.copies_per_uL & wbe$copies_per_uL<n1.loq.copies_per_uL & wbe$target=="N1")]))
# 
# 
# View(cbind(wbe$copies_per_uL[which(wbe$copies_per_uL>=n2.lod.copies_per_uL & wbe$copies_per_uL<n2.loq.copies_per_uL & wbe$target=="N2")], wbe4$copies_per_uL[which(wbe$copies_per_uL>=n2.lod.copies_per_uL & wbe$copies_per_uL<n2.loq.copies_per_uL & wbe$target=="N2")]))



# Scenario 5
## leave missings, scale loq


wbe5 <- wbe %>% 
          mutate(copies_per_uL = ifelse(target == "N1", 
                                        ifelse(
                                          copies_per_uL < n1.lod.copies_per_uL | is.na(copies_per_uL), 
                                          NA, 
                                          ifelse(
                                            copies_per_uL < n1.loq.copies_per_uL, 
                                            copies_per_uL+(copies_per_uL-n1.lod.loq.max)/n1.lod.loq.rdiff*(n1.lod.loq.min-(n1.lod.copies_per_uL/2)),
                                            copies_per_uL
                                          )),
                                        ifelse(
                                          copies_per_uL < n2.lod.copies_per_uL | is.na(copies_per_uL), 
                                          NA,
                                          ifelse(
                                            copies_per_uL < n2.loq.copies_per_uL, 
                                            copies_per_uL+(copies_per_uL-n2.lod.loq.max)/n2.lod.loq.rdiff*(n2.lod.loq.min-(n2.lod.copies_per_uL/2)),
                                            copies_per_uL
                                          )
                                        )
                                  )
                 )
```


```{r scenario-hists, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap = "Histograms of the Natural Logarithm of Sample Viral Sequence Copy Concentrations"}

par(mfrow = c(3,2), mar = c(2.1,2.1,0,0))


# > hist(log(wbe$copies_per_uL[which(wbe$target=="N1")]), breaks = 50)
# > abline(v=log(n1.lod.copies_per_uL), lwd = 2)
# > abline(v=log(n1.loq.copies_per_uL), lwd = 0.5)
# > abline(v=log(n1.lod.copies_per_uL/2), lwd = 2, lty = 3)
# > abline(v=log(n1.loq.copies_per_uL/2), lwd = 0.5, lty = 3)
# > abline(v=log(n1.lod.copies_per_uL/sqrt(2)), lwd = 2, lty = 5)
# > abline(v=log(n1.loq.copies_per_uL/sqrt(2)), lwd = 0.5, lty = 5)

# 
# hist(log(wbe$copies_per_uL[which(wbe$target=="N2")]), breaks = 50)
# abline(v=log(n2.lod.copies_per_uL), lwd = 2)
# abline(v=log(n2.loq.copies_per_uL), lwd = 0.5)
# abline(v=log(n2.lod.copies_per_uL/2), lwd = 2, lty = 3)
# abline(v=log(n2.loq.copies_per_uL/2), lwd = 0.5, lty = 3)
# abline(v=log(n2.lod.copies_per_uL/sqrt(2)), lwd = 2, lty = 5)
# abline(v=log(n2.loq.copies_per_uL/sqrt(2)), lwd = 0.5, lty = 5)
# 

hist(log(wbe$copies_per_uL), breaks = 100, axes = F, xlab = "", ylab = "", main = "", xlim = c(4,10))
abline(v=log(n1.lod.copies_per_uL), lwd = 2)
abline(v=log(n1.loq.copies_per_uL), lwd = 0.5)
abline(v=log(n2.lod.copies_per_uL), lwd = 1, lty = 3)
abline(v=log(n2.loq.copies_per_uL), lwd = 0.5, lty = 3)
axis(1, at = 4:10, labels = T, tick = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = expression(paste("log(Copies per ", mu, "L)")), line = 1)
axis(2, at = seq(0,40,by=10), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Frequency", line = 1.25)
# legend("topright", lwd = c(2,1,2,1), lty = c(1,1,3,3), legend = c(expression(LOD[~N1]), expression(LOQ[~N1]), expression(LOD[~N2]),expression(LOQ[~N2])))
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "A", adj = c(0,1), xpd = T, cex = 1, font = 2)


hist(log(wbe2$copies_per_uL), breaks = 100, axes = F, xlab = "", ylab = "", main = "", xlim = c(4,10))
abline(v=log(n1.lod.copies_per_uL), lwd = 1)
abline(v=log(n1.loq.copies_per_uL), lwd = 0.5)
abline(v=log(n2.lod.copies_per_uL), lwd = 1, lty = 3)
abline(v=log(n2.loq.copies_per_uL), lwd = 0.5, lty = 3)
axis(1, at = 4:10, labels = T, tick = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = expression(paste("log(Copies per ", mu, "L)")), line = 1)
axis(2, at = seq(0,800,by=100), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Frequency", line = 1.25)
# legend("topright", lwd = c(2,1,2,1), lty = c(1,1,3,3), legend = c(expression(LOD[~N1]), expression(LOQ[~N1]), expression(LOD[~N2]),expression(LOQ[~N2])))
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "B", adj = c(0,1), xpd = T, cex = 1, font = 2)


hist(log(wbe3$copies_per_uL), breaks = 100, axes = F, xlab = "", ylab = "", main = "", xlim = c(4,10))
abline(v=log(n1.lod.copies_per_uL), lwd = 1)
abline(v=log(n1.loq.copies_per_uL), lwd = 0.5)
abline(v=log(n2.lod.copies_per_uL), lwd = 1, lty = 3)
abline(v=log(n2.loq.copies_per_uL), lwd = 0.5, lty = 3)
axis(1, at = 4:10, labels = T, tick = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = expression(paste("log(Copies per ", mu, "L)")), line = 1)
axis(2, at = seq(0,800,by=100), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Frequency", line = 1.25)
# legend("topright", lwd = c(2,1,2,1), lty = c(1,1,3,3), legend = c(expression(LOD[~N1]), expression(LOQ[~N1]), expression(LOD[~N2]),expression(LOQ[~N2])))
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "C", adj = c(0,1), xpd = T, cex = 1, font = 2)


hist(log(wbe4$copies_per_uL), breaks = 100, axes = F, xlab = "", ylab = "", main = "", xlim = c(4,10))
abline(v=log(n1.lod.copies_per_uL), lwd = 1)
abline(v=log(n1.loq.copies_per_uL), lwd = 0.5)
abline(v=log(n2.lod.copies_per_uL), lwd = 1, lty = 3)
abline(v=log(n2.loq.copies_per_uL), lwd = 0.5, lty = 3)
axis(1, at = 4:10, labels = T, tick = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = expression(paste("log(Copies per ", mu, "L)")), line = 1)
axis(2, at = seq(0,200,by=50), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Frequency", line = 1.25)
# legend("topright", lwd = c(2,1,2,1), lty = c(1,1,3,3), legend = c(expression(LOD[~N1]), expression(LOQ[~N1]), expression(LOD[~N2]),expression(LOQ[~N2])))
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "D", adj = c(0,1), xpd = T, cex = 1, font = 2)



hist(log(wbe5$copies_per_uL), breaks = 100, axes = F, xlab = "", ylab = "", main = "", xlim = c(4,10))
abline(v=log(n1.lod.copies_per_uL), lwd = 1)
abline(v=log(n1.loq.copies_per_uL), lwd = 0.5)
abline(v=log(n2.lod.copies_per_uL), lwd = 1, lty = 3)
abline(v=log(n2.loq.copies_per_uL), lwd = 0.5, lty = 3)
axis(1, at = 4:10, labels = T, tick = T, cex.axis = 0.7, line = 0, padj = -1.75, tck = -0.0125)
title(xlab = expression(paste("log(Copies per ", mu, "L)")), line = 1)
axis(2, at = seq(0,20,by=5), tick = T, labels = T, cex.axis = 0.7, line = 0, hadj = 0.25, tck = -0.0125, las = 1)
title(ylab = "Frequency", line = 1.25)
# legend("topright", lwd = c(2,1,2,1), lty = c(1,1,3,3), legend = c(expression(LOD[~N1]), expression(LOQ[~N1]), expression(LOD[~N2]),expression(LOQ[~N2])))
text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "E", adj = c(0,1), xpd = T, cex = 1, font = 2)

plot.new()
legend(x=0, y=1, lwd = c(1,0.5,1,0.5), lty = c(1,1,3,3), legend = c(expression(LOD[~N1]), expression(LOQ[~N1]), expression(LOD[~N2]),expression(LOQ[~N2])), cex = 2, xjust = 0, yjust = 1)

text(x = 0.75, y=1, labels = "A: Scenario 1\nB: Scenario 2\nC: Scenario 3\nD: Scenario 4\nE: Scenario 5", adj = c(0, 1), cex = 1)

```








# Averages Calculations {#means}




```{r averaging, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*2, fig.cap = "Exploring Different Mean Formulae"}
# 
# #all pos
# ex1 <- wbe2[which(wbe2$sample_date==as.Date("2021-01-19") & wbe2$facility=="NO" & wbe2$target=="N1" & wbe$biological_replicate=="B"),]
# #most pos
# ex2 <- wbe2[which(wbe2$sample_date==as.Date("2021-01-11") & wbe2$facility=="MI" & wbe2$target=="N1" & wbe$biological_replicate=="C"),]
# #most neg
# ex3 <- wbe2[which(wbe2$sample_date==as.Date("2021-01-11") & wbe2$facility=="CC" & wbe2$target=="N1" & wbe$biological_replicate=="B"),]
# #all neg
# ex4 <- wbe2[which(wbe2$sample_date==as.Date("2020-06-30") & wbe2$facility=="CC" & wbe2$target=="N1" & wbe$biological_replicate=="C"),]
# 
# ex.mat <- matrix(c(ex1$copies_per_uL, ex2$copies_per_uL, ex3$copies_per_uL, ex4$copies_per_uL), nrow = 3)
# 
# dotchart(ex.mat)
# 
# 
# 
# 
# 
# plot(NA, type = 'n', xlim = c(0, 18000), ylim = c(0, 10))
# 
# abline(h = seq(0,10, by = 3), lty = 5, col = "gainsboro")
# abline(h = seq(0,10, by = 3)+1, lty = 5, col = "gainsboro")
# 
# y.vals <- c(0,3,6,9)
# 
# 
# points(ex.mat[,1], rep(y.vals[1], nrow(ex.mat)), pch = 16)
# points(ex.mat[,2], rep(y.vals[2], nrow(ex.mat)), pch = as.numeric(as.character(factor(is.na(ex2$copies_per_uL_rxn), levels = c(F,T), labels = c(16,1)))))
# 
# points(ex.mat[,3], c(y.vals[3], 5.8,6.2), pch = as.numeric(as.character(factor(is.na(ex3$copies_per_uL_rxn), levels = c(F,T), labels = c(16,1)))))
# 
# points(ex.mat[,4], c(y.vals[4], 8.7,9.3), pch = as.numeric(as.character(factor(is.na(ex4$copies_per_uL_rxn), levels = c(F,T), labels = c(16,1)))))
# 
# 
# points(mean(ex1$copies_per_uL), y.vals[1]+1, pch = "A")
# points(exp(mean(log(ex1$copies_per_uL))), y.vals[1]+1, pch = "G")
# points(length(ex1$copies_per_uL)/sum(1/ex1$copies_per_uL), y.vals[1]+1, pch = "H")
# points(mean(ex1$copies_per_uL^2)^(1/2), y.vals[1]+1, pch = "Q")
# # points(median(ex1$copies_per_uL), y.vals[1]+1, pch = "M")
# points(mean(ex1$copies_per_uL^3)^(1/3), y.vals[1]+1, pch = "C")
# 
# points(mean(ex2$copies_per_uL), y.vals[2]+1, pch = "A")
# points(exp(mean(log(ex2$copies_per_uL))), y.vals[2]+1, pch = "G")
# points(length(ex2$copies_per_uL)/sum(1/ex2$copies_per_uL), y.vals[2]+1, pch = "H")
# points(mean(ex2$copies_per_uL^2)^(1/2), y.vals[2]+1, pch = "Q")
# # points(median(ex2$copies_per_uL), y.vals[1]+1, pch = "M")
# points(mean(ex2$copies_per_uL^3)^(1/3), y.vals[2]+1, pch = "C")
# 
# points(mean(ex3$copies_per_uL), y.vals[3]+1, pch = "A")
# points(exp(mean(log(ex3$copies_per_uL))), y.vals[3]+1, pch = "G")
# points(length(ex3$copies_per_uL)/sum(1/ex3$copies_per_uL), y.vals[3]+1, pch = "H")
# points(mean(ex3$copies_per_uL^2)^(1/2), y.vals[3]+1, pch = "Q")
# # points(median(ex3$copies_per_uL), y.vals[1]+1, pch = "M")
# points(mean(ex3$copies_per_uL^3)^(1/3), y.vals[3]+1, pch = "C")
# 
# points(mean(ex4$copies_per_uL), y.vals[4]+1, pch = "A")
# points(exp(mean(log(ex4$copies_per_uL))), y.vals[4]+1, pch = "G")
# points(length(ex4$copies_per_uL)/sum(1/ex4$copies_per_uL), y.vals[4]+1, pch = "H")
# points(mean(ex4$copies_per_uL^2)^(1/2), y.vals[4]+1, pch = "Q")
# # points(median(ex4$copies_per_uL), y.vals[1]+1, pch = "M")
# points(mean(ex4$copies_per_uL^3)^(1/3), y.vals[4]+1, pch = "C")
# 
# legend("topright", pch = c("H", "G", "A", "Q", "C"), legend = c("Harmonic Mean", "Geometric Mean", "Arithmetic Mean", "Quadratic Mean", "Cubic Mean"))
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 

ex1b <- wbe2[which(wbe2$sample_date==as.Date("2021-01-19") & wbe2$facility=="NO" & wbe2$target=="N1" & wbe$biological_replicate=="B"),]
ex1a <- wbe2[which(wbe2$sample_date==as.Date("2021-01-19") & wbe2$facility=="NO" & wbe2$target=="N1" & wbe$biological_replicate=="A"),]
ex1c <- wbe2[which(wbe2$sample_date==as.Date("2021-01-19") & wbe2$facility=="NO" & wbe2$target=="N1" & wbe$biological_replicate=="C"),]

ex.mat <- matrix(c(ex1a$copies_per_uL, ex1b$copies_per_uL, ex1c$copies_per_uL), nrow = 3)

dotchart(ex.mat)




plot(NA, type = 'n', xlim = c(0, 18000), ylim = c(0, 16), ylab = "", xlab = expression(paste("log(Copies per ", mu, "L)")), yaxt = 'n', cex.axis = 0.7, cex.lab = 0.7)

abline(h = seq(0,16, by = 3), lty = 5, col = "gainsboro")
abline(h = seq(0,16, by = 3)+1, lty = 5, col = "gainsboro")

y.vals <- seq(0,16, by = 3)


points(ex.mat[,1], c(-0.2,0.2, y.vals[1]), pch = as.numeric(as.character(factor(is.na(ex1a$copies_per_uL_rxn), levels = c(F,T), labels = c(16,1)))))
points(ex.mat[,2], rep(y.vals[2], nrow(ex.mat)), pch = as.numeric(as.character(factor(is.na(ex1b$copies_per_uL_rxn), levels = c(F,T), labels = c(16,1)))))

points(ex.mat[,3], c(y.vals[3], 5.8,6.2), pch = as.numeric(as.character(factor(is.na(ex1c$copies_per_uL_rxn), levels = c(F,T), labels = c(16,1)))))



points(mean(ex1a$copies_per_uL), y.vals[1]+1, pch = "a")
points(exp(mean(log(ex1a$copies_per_uL))), y.vals[1]+1, pch = "g")
points(length(ex1a$copies_per_uL)/sum(1/ex1a$copies_per_uL), y.vals[1]+1, pch = "h")
points(mean(ex1a$copies_per_uL^2)^(1/2), y.vals[1]+1, pch = "q")
# points(median(ex1a$copies_per_uL), y.vals[1]+1, pch = "M")
points(mean(ex1a$copies_per_uL^3)^(1/3), y.vals[1]+1, pch = "c")

points(mean(ex1b$copies_per_uL), y.vals[2]+1, pch = "a")
points(exp(mean(log(ex1b$copies_per_uL))), y.vals[2]+1, pch = "g")
points(length(ex1b$copies_per_uL)/sum(1/ex1b$copies_per_uL), y.vals[2]+1, pch = "h")
points(mean(ex1b$copies_per_uL^2)^(1/2), y.vals[2]+1, pch = "q")
# points(median(ex1b$copies_per_uL), y.vals[1]+1, pch = "M")
points(mean(ex1b$copies_per_uL^3)^(1/3), y.vals[2]+1, pch = "c")

points(mean(ex1c$copies_per_uL), y.vals[3]+1, pch = "a")
points(exp(mean(log(ex1c$copies_per_uL))), y.vals[3]+1, pch = "g")
points(length(ex1c$copies_per_uL)/sum(1/ex1c$copies_per_uL), y.vals[3]+1, pch = "h")
points(mean(ex1c$copies_per_uL^2)^(1/2), y.vals[3]+1, pch = "q")
# points(median(ex1c$copies_per_uL), y.vals[1]+1, pch = "M")
points(mean(ex1c$copies_per_uL^3)^(1/3), y.vals[3]+1, pch = "c")




points(c(mean(ex1b$copies_per_uL), mean(ex1a$copies_per_uL), mean(ex1c$copies_per_uL)), y = rep(y.vals[4],3), pch = "a")
points(mean(c(mean(ex1b$copies_per_uL), mean(ex1a$copies_per_uL), mean(ex1c$copies_per_uL))), y = y.vals[4]+1, pch = "A")
points(exp(mean(log(c(mean(ex1b$copies_per_uL), mean(ex1a$copies_per_uL), mean(ex1c$copies_per_uL))))), y = y.vals[4]+1, pch = "G")
points(mean(c(mean(ex1b$copies_per_uL), mean(ex1a$copies_per_uL), mean(ex1c$copies_per_uL))^2)^(1/2), y = y.vals[4]+1, pch = "Q")


points(c(exp(mean(log(ex1b$copies_per_uL))), exp(mean(log(ex1a$copies_per_uL))), exp(mean(log(ex1c$copies_per_uL)))), y = rep(y.vals[5],3), pch = "g")
points(mean(c(exp(mean(log(ex1b$copies_per_uL))), exp(mean(log(ex1a$copies_per_uL))), exp(mean(log(ex1c$copies_per_uL))))), y = y.vals[5]+1, pch = "A")
points(exp(mean(log(c(exp(mean(log(ex1b$copies_per_uL))), exp(mean(log(ex1a$copies_per_uL))), exp(mean(log(ex1c$copies_per_uL))))))), y = y.vals[5]+1, pch = "G")
points(mean(c(exp(mean(log(ex1b$copies_per_uL))), exp(mean(log(ex1a$copies_per_uL))), exp(mean(log(ex1c$copies_per_uL))))^2)^(1/2), y = y.vals[5]+1, pch = "Q")


points(c(mean(ex1b$copies_per_uL^2)^(1/2), mean(ex1a$copies_per_uL^2)^(1/2), mean(ex1c$copies_per_uL^2)^(1/2)), y = rep(y.vals[6],3), pch = "q")
points(mean(c(mean(ex1b$copies_per_uL^2)^(1/2), mean(ex1a$copies_per_uL^2)^(1/2), mean(ex1c$copies_per_uL^2)^(1/2))), y = y.vals[6]+1, pch = "A")
points(exp(mean(log(c(mean(ex1b$copies_per_uL^2)^(1/2), mean(ex1a$copies_per_uL^2)^(1/2), mean(ex1c$copies_per_uL^2)^(1/2))))), y = y.vals[6]+1, pch = "G")
points(mean(c(mean(ex1b$copies_per_uL^2)^(1/2), mean(ex1a$copies_per_uL^2)^(1/2), mean(ex1c$copies_per_uL^2)^(1/2))^2)^(1/2), y = y.vals[6]+1, pch = "Q")




abline(h = 8, lty = 5, xpd = T)

axis(2, at = y.vals[1:3], labels = c("A", "B", "C"), tick = T, las = 1, cex.axis = 0.7)
mtext("Biological Replicate", 2, line = 2, adj = 0.25, cex = 0.7)
mtext("Averaging Scheme", 2, line = 2, adj = 0.75, cex = 0.7)


legend("topright", pch = c("h", "g", "a", "q", "c"), legend = c(" = Harmonic Mean", " = Geometric Mean", " = Arithmetic Mean", " = Quadratic Mean", " = Cubic Mean"), cex = 0.7)








```













# Extra Code





## Copies and Normality

```{r, echo=F, eval=F}
dev.off()
plot(wbe$sample_date[which(wbe$target=="N1" & wbe$facility=="CC")], log10(wbe$copies[which(wbe$target=="N1" & wbe$facility=="CC")]*1e6), pch = as.numeric(as.factor(wbe$biological_replicate[which(wbe$target=="N1" & wbe$facility=="CC")])))

plot(wbe$sample_date[which(wbe$target=="N2" & wbe$facility=="CC")], log10(wbe$copies[which(wbe$target=="N2" & wbe$facility=="CC")]*1e6), pch = as.numeric(as.factor(wbe$biological_replicate[which(wbe$target=="N2" & wbe$facility=="CC")])))




plot(wbe$sample_date[which(wbe$target=="N1" & wbe$facility=="MI")], log10(wbe$copies[which(wbe$target=="N1" & wbe$facility=="MI")]*1e6), pch = as.numeric(as.factor(wbe$biological_replicate[which(wbe$target=="N1" & wbe$facility=="MI")])))

plot(wbe$sample_date[which(wbe$target=="N2" & wbe$facility=="MI")], log10(wbe$copies[which(wbe$target=="N2" & wbe$facility=="MI")]*1e6), pch = as.numeric(as.factor(wbe$biological_replicate[which(wbe$target=="N2" & wbe$facility=="MI")])))




plot(wbe$sample_date[which(wbe$target=="N1" & wbe$facility=="NO")], log10(wbe$copies[which(wbe$target=="N1" & wbe$facility=="NO")]*1e6), pch = as.numeric(as.factor(wbe$biological_replicate)))

plot(wbe$sample_date[which(wbe$target=="N2" & wbe$facility=="NO")], log10(wbe$copies[which(wbe$target=="N2" & wbe$facility=="NO")]*1e6), pch = as.numeric(as.factor(wbe$biological_replicate)))
```


```{r, eval = F, echo = F}
# hist(wbe$copy_num_uL_rxn)
# hist(wbe$copy_num_uL_rxn, breaks = 100)
# hist(wbe$copy_num_uL_rxn[which(wbe$copy_num_uL_rxn<1)])
# hist(wbe$copy_num_uL_rxn[which(wbe$copy_num_uL_rxn<1)], breaks = 100)
# hist(wbe$copy_num_uL_rxn[which(wbe$copy_num_uL_rxn<0.2)])
# hist(wbe$copy_num_uL_rxn[which(wbe$copy_num_uL_rxn<0.2)], breaks = 100)
# hist(wbe$copy_num_uL_rxn[which(wbe$copy_num_uL_rxn<0.1)])
# hist(wbe$copy_num_uL_rxn[which(wbe$copy_num_uL_rxn<0.1)], breaks = 100)
# 
# hist(wbe$copies)
# hist(log(wbe$copies))
# 
# hist(log(wbe$copy_num_uL_rxn))
# 
# summary(wbe$copy_num_uL_rxn)
# summary(wbe$copies)
# summary(wbe$copies[which(wbe$copies>min(wbe$copies, na.rm=T))])
# 
# View(wbe[order(wbe$copies),])
```


```{r, echo=F, eval=F}
# layout(matrix(c(1,2), nrow = 2), widths = c(lcm(8*2.54)), heights = c(lcm(4.5*2.54), lcm(4.5*2.54)))
# # layout.show(2)
# qqnorm.copies.n1 <- qqnorm(log(wbe$copies[which(wbe$target=="N1")]), main = "Normal Q-Q Plot for N1 log(Copies)") %>% as.data.frame()
# qqnorm.copies.n2 <- qqnorm(log(wbe$copies[which(wbe$target=="N2")]), main = "Normal Q-Q Plot for N2 log(Copies)") %>% as.data.frame()
# 
# 
# qqnorm.Explorer <- function(qqnorm.copies){
#         qqnorm.copies <- qqnorm.copies[which(complete.cases(qqnorm.copies)),]
#         qqnorm.copies <- qqnorm.copies[order(qqnorm.copies$x),]
#         qqnorm.copies <- cbind(qqnorm.copies, rbind(NA, qqnorm.copies[-nrow(qqnorm.copies),])) %>% setNames(., nm = c("x", "y", "x-1", "y-1"))
#         qqnorm.copies %<>% mutate(rise = y-`y-1`, run = x-`x-1`) %>% mutate(slope = rise / run)
#         
#         qqnorm.copies$lod <- NA
#         qqnorm.copies$loq <- NA
#         
#         prev.slope <- 1
#         lod.found <- 0
#         for(i in 2:nrow(qqnorm.copies)){
#           if(lod.found==0){
#             if(qqnorm.copies$slope[i]<1 & prev.slope <1){
#               qqnorm.copies$lod[i] <- 1
#               lod.found <- 1
#             }else{
#               prev.slope <- qqnorm.copies$slope[i]
#             }
#           }
#           if(lod.found==1){
#             if(qqnorm.copies$slope[i]>1){
#               qqnorm.copies$loq[i] <- 1
#               break
#             }else{
#               prev.slope <- qqnorm.copies$slope[i]
#             }
#           }
#         }
# 
#         lod.copies <- qqnorm.copies$y[which(qqnorm.copies$lod==1)]
#         loq.copies <- qqnorm.copies$y[which(qqnorm.copies$loq==1)]
#         
#         return(list(qqnorm.dataset = qqnorm.copies, lod = lod.copies, loq = loq.copies))
# }
# 
# qqnorm.copies.n1 <- qqnorm.Explorer(qqnorm.copies.n1)
# qqnorm.copies.n2 <- qqnorm.Explorer(qqnorm.copies.n2)
# 
# 
# 
# layout(matrix(c(1,2), nrow = 2), widths = c(lcm(8*2.54)), heights = c(lcm(4.5*2.54), lcm(4.5*2.54)))
# # layout.show(2)
# qqnorm(log(wbe$copies[which(wbe$target=="N1")]), main = "Normal Q-Q Plot for N1 log(Copies)")
# abline(h = qqnorm.copies.n1$lod)
# text(par('usr')[1], par('usr')[4], labels = paste("LOD =", round(qqnorm.copies.n1$lod,3)), adj = c(-0.05,1.2))
# abline(h = qqnorm.copies.n1$loq, lty = 3)
# text(par('usr')[1], par('usr')[4], labels = paste("LOQ =", round(qqnorm.copies.n1$loq,3)), adj = c(-0.05,2.4))
# legend("bottomright", lty = c(1,3), legend = c("LOD", "LOQ"))
# 
# 
# qqnorm(log(wbe$copies[which(wbe$target=="N2")]), main = "Normal Q-Q Plot for N2 log(Copies)")
# abline(h = qqnorm.copies.n2$lod)
# text(par('usr')[1], par('usr')[4], labels = paste("LOD =", round(qqnorm.copies.n2$lod,3)), adj = c(-0.05,1.2))
# abline(h = qqnorm.copies.n2$loq, lty = 3)
# text(par('usr')[1], par('usr')[4], labels = paste("LOQ =", round(qqnorm.copies.n2$loq,3)), adj = c(-0.05,2.4))
# legend("bottomright", lty = c(1,3), legend = c("LOD", "LOQ"))




# n1 ct --> copies
# > 10^((37.14072-34.008)/-3.3890)
# [1] 0.1190203
# > 10^((36.96446-34.008)/-3.3890)
# [1] 0.1341623

# n2 ct --> copies
# > 10^((37.09759-32.416)/-3.3084)
# [1] 0.03845372
# > 10^((36.99855-32.416)/-3.3084)
# [1] 0.04119782


# n1 copies
# > exp(lod.copies.n1)
# [1] 0.1194078
# > exp(loq.copies.n1)
# [1] 0.1379563

# n2 copies
# > exp(loq.copies.n2)
# [1] 0.04412582
# > exp(lod.copies.n2)
# [1] 0.03863563


```




```{r, echo=F, eval=F}

cns.all.qc <- qc %>% group_by(collection_num, target) %>% summarise(n=n()) %>% filter(n>=15) %>% select(collection_num, target)

qc.index <- c()

for(i in 1:nrow(cns.all.qc)){
  qc.index <- c(qc.index, which(qc$collection_num==cns.all.qc$collection_num[i] & qc$target==cns.all.qc$target[i]))
}


qc.test <- qc[qc.index,]

qc.by.target <- split(qc.test , f = qc.test$target)

qc.by.target.by.collection.number <- lapply(qc.by.target, function(x){split(x, f = x$collection_num)})

qc.standard.curves <- lapply(qc.by.target.by.collection.number, function(x){lapply(x, function(y){lm(ct~log10(quantity), data=y)})})

qc.sc.ests <- lapply(qc.standard.curves, function(x){lapply(x, function(y){c(coef(y), r2=summary(y)$r.squared)})})

qc.sc.ests <- lapply(qc.sc.ests, function(x){bind_rows(x, .id = "CN")}) %>% bind_rows(.id="target")

qc.sc.ests$Efficiency <- 10^(-1/qc.sc.ests$`log10(quantity)`)-1

qc.sc.ests <- qc.sc.ests %>% 
  group_by(target) %>% 
  summarise(
    `(Intercept)`=paste0(round(mean(`(Intercept)`), 3), " (", round(sd(`(Intercept)`),3), ")"), 
    `log10(quantity)`=paste0(round(mean(`log10(quantity)`),3), " (", round(sd(`log10(quantity)`),3), ")"), 
    r2 = paste0(round(mean(r2),3), " (", round(sd(r2),3), ")"), 
    Efficiency = paste0(round(mean(Efficiency),3), " (", round(sd(Efficiency),3), ")")
    ) %>% 
  ungroup() %>% 
  mutate(CN = "MEAN (SD)") %>% 
  bind_rows(qc.sc.ests%>%mutate_if(is.numeric, ~round(.,3))%>%mutate_all(~as.character(.)), .) %>% 
  arrange(target, CN)





as_grouped_data(qc.sc.ests, "target") %>% 
  flextable() %>% 
  border_remove() %>% 
  align(part = "all", align = "right") %>%
  align(part = "header", align = "center") %>%
  align(part = "body", j = 1, align = "left") %>%
  hline(part = "header", i = 1, border = fp_border_default(color = "black", width = 2)) %>%
  hline(part = "body", i = which(!is.na(as_grouped_data(qc.sc.ests, "target")$target)), border = fp_border_default(color = "black", width = 0.5)) %>% 
  bold(part = "header") %>%
  font(part = "all", fontname = "Arial") %>%
  fontsize(part = "header", size = 9) %>% 
  fontsize(part = "body", size = 8)%>%
  autofit()
 






qc.means <- qc %>% group_by(target, quantity) %>% summarise(n= n(), mean.ct = mean(ct), sd.ct = sd(ct))

sc.mean.n1 <- lm(mean.ct~log10(quantity), data=qc.means[which(qc.means$target=="N1"),])
sc.mean.n2 <- lm(mean.ct~log10(quantity), data=qc.means[which(qc.means$target=="N2"),])






layout(matrix(c(1,2), nrow = 2), widths = c(lcm(8*2.54)), heights = c(lcm(4.5*2.54), lcm(4.5*2.54)))
# layout.show(2)

plot(0,type='n',axes=TRUE, ylim = c(10,40), xlim = c(0,7), xlab = "log10(quantity)", ylab = "Cycle Threshold")
abline(sc.mean.n1, lwd = 10, col = "darkgrey")


for(i in 1:length(qc.standard.curves$N1)){
  abline(qc.standard.curves$N1[[i]], lty = 3)
}
abline(qc.standard.curves$N1$`13`, lwd = 4)
points(qc.by.target.by.collection.number$N1$`13`$ct~log10(qc.by.target.by.collection.number$N1$`13`$quantity), cex = 2)
text(par('usr')[2],par('usr')[4],labels = paste("13: Ct =",round(coef(qc.standard.curves$N1$`13`)[1],3), round(coef(qc.standard.curves$N1$`13`)[2],3), "*log10(quantity)"), adj = c(1, 1.2))
text(par('usr')[2],par('usr')[4],labels = paste("Mean: Ct =",round(coef(sc.mean.n1)[1],3), round(coef(sc.mean.n1)[2],3), "*log10(quantity)"), adj = c(1, 2.4))

title(main = "Standard Curves for N1")


plot(0,type='n',axes=TRUE, ylim = c(10,40), xlim = c(0,7), xlab = "log10(quantity)", ylab = "Cycle Threshold")
abline(sc.mean.n2, lwd = 10, col = "darkgrey")

for(i in 1:length(qc.standard.curves$N2)){
  abline(qc.standard.curves$N2[[i]], lty = 3)
}
abline(qc.standard.curves$N2$`24`, lwd = 4)
points(qc.by.target.by.collection.number$N2$`24`$ct~log10(qc.by.target.by.collection.number$N2$`24`$quantity), cex = 2)
text(par('usr')[2],par('usr')[4],labels = paste("24: Ct =",round(coef(qc.standard.curves$N2$`24`)[1],3), round(coef(qc.standard.curves$N2$`24`)[2],3), "*log10(quantity)"), adj = c(1, 1.2))
text(par('usr')[2],par('usr')[4],labels = paste("Mean: Ct =",round(coef(sc.mean.n2)[1],3), round(coef(sc.mean.n2)[2],3), "*log10(quantity)"), adj = c(1, 2.4))
title(main = "Standard Curves for N2")

```



## Autocorrelation


```{r, eval = F, echo = F}

my.data <- wbe.scenarios.date[[2]]

my.data %<>% mutate(log.copies = log(copies+1), 
                    log.copies.per.uL = log(copies_per_uL+1)) %>%
          arrange(sample_date)


# my.data <- my.data[-which(my.data$sample_date%in%as.Date(c("2020-11-19", "2020-11-18"))),]



acf(my.data$cases.symptom.onset)
acf(my.data$cases.symptom.onset.7dma[which(complete.cases(my.data$cases.symptom.onset.7dma))])
acf(my.data$cases.reported.decon)


```


## Scatterplots and Correlations


```{r, eval = F, echo = F}

all.cors <- c()


i=1
ii=1
repeat{

  these.cors <- c()
  repeat{
    my.cor <- cor(as.matrix(my.data[,xvars[i]]), as.matrix(my.data[,yvars[ii]]), use = "pairwise.complete.obs", method = "spearman")
    plot(as.matrix(my.data[,xvars[i]]), as.matrix(my.data[,yvars[ii]]), xlab = xvars[i], ylab = yvars[ii])
    mtext(bquote(rho~" = "~.(round(my.cor, 4))), 3)
    
    these.cors <- c(these.cors, my.cor)
    
    cil <- readline(prompt = "Continue inner loop?")
    if(cil == 0 | ii == length(yvars)){break}else{ii <- ii + 1}
  }
  
  out.loop <- readline(prompt = "Continue outer loop?")
  if(out.loop == 0 | i == length(xvars)){break}else{i <- i + 1; ii <- 1}

  all.cors <- c(all.cors, these.cors)
}


rep(xvars, each=length(yvars))[which.max(all.cors)]

yvars[which.max(all.cors[which(rep(xvars, each=length(yvars))==rep(xvars, each=length(yvars))[which.max(all.cors)])])]



plot(my.data$copies_N1, my.data$cases.reported.con)
cor(my.data$copies_N1, my.data$cases.reported.con, use = "pairwise.complete.obs", method = "spearman")
```







## Extensive Plotting


```{r, eval = F, echo = F, message=F, warning=F, error=F}

wbe.scenarios <- list(wbe, wbe2, wbe3, wbe4, wbe5)

summarize_Replicates <- function(x){
  x %<>%
      group_by(sample_date, facility, biological_replicate, target) %>% 
      summarise(copies_per_uL = exp(mean(log(copies_per_uL), na.rm=T))) %>% 
      ungroup() %>% 
      group_by(sample_date, facility, biological_replicate) %>% 
      summarise(copies_per_uL = exp(mean(log(copies_per_uL), na.rm = T))) %>% 
      ungroup() %>% 
      group_by(sample_date, facility) %>% 
      summarise(copies_per_uL = exp(mean(log(copies_per_uL), na.rm = T))) %>% 
      ungroup()
  return(x)
}

wbe.scenarios.facility <- lapply(wbe.scenarios, summarize_Replicates)
 

combine_Plant_data <- function(x){
  x %<>% 
      full_join(plant, by = c("sample_date"="date", "facility"="wrf")) %>%
      mutate(copies = copies_per_uL * 1e6 * influent_flow_L)
  return(x)
}

wbe.scenarios %<>% lapply(combine_Plant_data)
wbe.scenarios.facility %<>% lapply(combine_Plant_data)


summarize_Facilities <- function(x){
  x %<>%
      group_by(sample_date) %>%
      summarise(
        copies = sum(copies, na.rm = T), 
        copies_per_uL = exp(mean(log(copies_per_uL), na.rm=T)),
        mean.tss = exp(mean(log(influent_tss_mg_l), na.rm=T))
      ) %>%
      ungroup()
  return(x)
}


wbe.scenarios.date <- wbe.scenarios.facility %>% lapply(summarize_Facilities)



combine_COVID <- function(x){
  x %<>% 
      full_join(covid, by = c("sample_date"="date")) #%>%
      # filter(sample_date %in% wbe$sample_date)
  return(x)
}


wbe.scenarios %<>% lapply(combine_COVID)
wbe.scenarios.facility %<>% lapply(combine_COVID)
wbe.scenarios.date %<>% lapply(combine_COVID)


```








## All the Plots

These many plots vary with respect to their x-variables (either copy concentration, or copy number), y-variables (raw case counts, 7-day moving averages, deconvoluted case counts), and the level of the dataset (facility level or sampling day level). 


```{r eval=F, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*3}
# plot(log(wbe.scenarios[[1]]$copies_per_uL), wbe.scenarios[[1]]$cases.reported)
# plot(log(wbe.scenarios[[1]]$copies_per_uL), wbe.scenarios[[1]]$cases.symptom.onset)
# plot(log(wbe.scenarios[[1]]$copies_per_uL), wbe.scenarios[[1]]$cases.reported.7dma)
# plot(log(wbe.scenarios[[1]]$copies_per_uL), wbe.scenarios[[1]]$cases.symptom.onset.7dma)
# plot(log(wbe.scenarios[[1]]$copies_per_uL), wbe.scenarios[[1]]$cases.reported.decon)
# 
# plot(wbe.scenarios[[1]]$cases.symptom.onset.17dsum, log(wbe.scenarios[[1]]$copies_per_uL))
# 
# 
# 
# 
# plot(log(wbe.scenarios.facility[[1]]$copies_per_uL), wbe.scenarios.facility[[1]]$cases.reported)
# plot(log(wbe.scenarios.facility[[1]]$copies_per_uL), wbe.scenarios.facility[[1]]$cases.symptom.onset)
# plot(log(wbe.scenarios.facility[[1]]$copies_per_uL), wbe.scenarios.facility[[1]]$cases.reported.7dma)
# plot(log(wbe.scenarios.facility[[1]]$copies_per_uL), wbe.scenarios.facility[[1]]$cases.symptom.onset.7dma)
# plot(log(wbe.scenarios.facility[[1]]$copies_per_uL), wbe.scenarios.facility[[1]]$cases.reported.decon)
# 
# plot(wbe.scenarios.facility[[1]]$cases.symptom.onset.17dsum, log(wbe.scenarios.facility[[1]]$copies_per_uL))
# 
# 
# plot(log(wbe.scenarios.facility[[1]]$copies), wbe.scenarios.facility[[1]]$cases.reported)
# plot(log(wbe.scenarios.facility[[1]]$copies), wbe.scenarios.facility[[1]]$cases.symptom.onset)
# plot(log(wbe.scenarios.facility[[1]]$copies), wbe.scenarios.facility[[1]]$cases.reported.7dma)
# plot(log(wbe.scenarios.facility[[1]]$copies), wbe.scenarios.facility[[1]]$cases.symptom.onset.7dma)
# plot(log(wbe.scenarios.facility[[1]]$copies), wbe.scenarios.facility[[1]]$cases.reported.decon)
# 
# plot(wbe.scenarios.facility[[1]]$cases.symptom.onset.17dsum, log(wbe.scenarios.facility[[1]]$copies))
# plot(log(wbe.scenarios.facility[[1]]$cases.symptom.onset.17dsum), log(wbe.scenarios.facility[[1]]$copies))
# 
# 
# facility <- "CC"
# facility <- "MI"
# facility <- "NO"
# 
# 
# plot(log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]), wbe.scenarios.facility[[1]]$cases.reported[which(wbe.scenarios.facility[[1]]$facility==facility)])
# plot(log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]), wbe.scenarios.facility[[1]]$cases.symptom.onset[which(wbe.scenarios.facility[[1]]$facility==facility)])
# plot(log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]), wbe.scenarios.facility[[1]]$cases.reported.7dma[which(wbe.scenarios.facility[[1]]$facility==facility)])
# plot(log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]), wbe.scenarios.facility[[1]]$cases.symptom.onset.7dma[which(wbe.scenarios.facility[[1]]$facility==facility)])
# plot(log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]), wbe.scenarios.facility[[1]]$cases.reported.decon[which(wbe.scenarios.facility[[1]]$facility==facility)])
# 
# plot(wbe.scenarios.facility[[1]]$cases.symptom.onset.17dsum[which(wbe.scenarios.facility[[1]]$facility==facility)], log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]))
# plot(log(wbe.scenarios.facility[[1]]$cases.symptom.onset.17dsum[which(wbe.scenarios.facility[[1]]$facility==facility)]), log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]))
# 
# 
# 
# 
# 
# plot(log(wbe.scenarios.date[[1]]$copies), wbe.scenarios.date[[1]]$cases.reported)
# plot(log(wbe.scenarios.date[[1]]$copies), wbe.scenarios.date[[1]]$cases.symptom.onset)
# plot(log(wbe.scenarios.date[[1]]$copies), wbe.scenarios.date[[1]]$cases.reported.7dma)
# plot(log(wbe.scenarios.date[[1]]$copies), wbe.scenarios.date[[1]]$cases.symptom.onset.7dma)
# plot(log(wbe.scenarios.date[[1]]$copies), wbe.scenarios.date[[1]]$cases.reported.decon)
# 
# plot(wbe.scenarios.date[[1]]$cases.symptom.onset.17dsum, log(wbe.scenarios.date[[1]]$copies))
# plot(log(wbe.scenarios.date[[1]]$cases.symptom.onset.17dsum), log(wbe.scenarios.date[[1]]$copies))




explore.Plots <- function(level=3, aggregate=TRUE, scenario=1, xvar="copies_per_uL", yvar="cases.symptom.onset.7dma", x.transform=TRUE, y.transform=FALSE){
  if(level==1){
    the.data.list <- wbe.scenarios
    the.vars <- c(xvar, yvar, "facility", "target")
  }else{
    if(level==2){
      the.data.list <- wbe.scenarios.facility
      the.vars <- c(xvar, yvar, "facility")
    }else{
      if(level==3){
        the.data.list <- wbe.scenarios.date
        the.vars <- c(xvar, yvar)
      }
    }
  }
  
  the.data <- the.data.list[[scenario]][,the.vars]
  
  if(x.transform){
    the.data[,xvar] <- log(the.data[,xvar])
  }  
  
  if(y.transform){
    the.data[,yvar] <- log(the.data[,yvar])
  }
  
  if(aggregate){
    plot(as.matrix(the.data[,xvar]), as.matrix(the.data[,yvar]))
  }else{
    if(level==1){
      targets <- c("N1", "N2")
      facilities <- c("CC", "MI", "NO")
      combos <- expand.grid(t=targets, f=facilities)
      
      plot(as.matrix(the.data[which(the.data$target==combos$t[1] & the.data$facility==combos$f[1]),xvar]), 
           as.matrix(the.data[which(the.data$target==combos$t[1] & the.data$facility==combos$f[1]),yvar]), 
           pch = 1)
      
      for(ii in 2:nrow(combos)){
        points(as.matrix(the.data[which(the.data$target==combos$t[ii] & the.data$facility==combos$f[ii]),xvar]), 
               as.matrix(the.data[which(the.data$target==combos$t[ii] & the.data$facility==combos$f[ii]),yvar]), 
               pch = ii)
      }
      
      legend("topleft", pch = 1:ii, legend = paste(combos$t, combos$f))
      
    }else{
      if(level==2){
        
      facilities <- c("CC", "MI", "NO")
      
      plot(as.matrix(the.data[which(the.data$facility==facilities[1]),xvar]), 
           as.matrix(the.data[which(the.data$facility==facilities[1]),yvar]), 
           pch = 1)
      
      for(ii in 2:length(facilities)){
        points(as.matrix(the.data[which(the.data$facility==facilities[ii]),xvar]), 
               as.matrix(the.data[which(the.data$facility==facilities[ii]),yvar]), 
               pch = ii)
      }
      
      legend("topleft", pch = 1:ii, legend = facilities)
      }else{
        plot(as.matrix(the.data[,xvar]), as.matrix(the.data[,yvar]))
      }
    }
    
  }
  

}


xvars <- c("copies_per_uL", 
           "copies", 
           "cases.symptom.onset.17dsum")

yvars <- c("cases.reported", "cases.symptom.onset", 
           "cases.reported.7dma", "cases.symptom.onset.7dma", 
           "cases.reported.decon", 
           "copies_per_uL", "copies")

x.index <- 2
y.index <- 5
level.index <- 3

for(level.index in 2:3){
for(x.index in 1:2){
for(y.index in 1:5){
par(mfrow=c(3,2), mar = c(2.1, 2.1, 0.1, 0.1))
for(i in 1:5){
explore.Plots(level=level.index, 
              aggregate = F,
              scenario=i, 
              xvar=xvars[x.index], 
              yvar=yvars[y.index], 
              x.transform=TRUE, 
              y.transform=FALSE)
}
plot.new()
text(par('usr')[1], par('usr')[4], labels = paste(c(paste0("x = ", xvars[x.index]), paste0("y = ", yvars[y.index]), paste0("level = ", level.index)), collapse = "\n"), adj = c(0, 1))
}
}
}
```





















```{r all-the-plots, eval = F, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*3}
# plot(log(wbe.scenarios[[1]]$copies_per_uL), wbe.scenarios[[1]]$cases.reported)
# plot(log(wbe.scenarios[[1]]$copies_per_uL), wbe.scenarios[[1]]$cases.symptom.onset)
# plot(log(wbe.scenarios[[1]]$copies_per_uL), wbe.scenarios[[1]]$cases.reported.7dma)
# plot(log(wbe.scenarios[[1]]$copies_per_uL), wbe.scenarios[[1]]$cases.symptom.onset.7dma)
# plot(log(wbe.scenarios[[1]]$copies_per_uL), wbe.scenarios[[1]]$cases.reported.decon)
# 
# plot(wbe.scenarios[[1]]$cases.symptom.onset.17dsum, log(wbe.scenarios[[1]]$copies_per_uL))
# 
# 
# 
# 
# plot(log(wbe.scenarios.facility[[1]]$copies_per_uL), wbe.scenarios.facility[[1]]$cases.reported)
# plot(log(wbe.scenarios.facility[[1]]$copies_per_uL), wbe.scenarios.facility[[1]]$cases.symptom.onset)
# plot(log(wbe.scenarios.facility[[1]]$copies_per_uL), wbe.scenarios.facility[[1]]$cases.reported.7dma)
# plot(log(wbe.scenarios.facility[[1]]$copies_per_uL), wbe.scenarios.facility[[1]]$cases.symptom.onset.7dma)
# plot(log(wbe.scenarios.facility[[1]]$copies_per_uL), wbe.scenarios.facility[[1]]$cases.reported.decon)
# 
# plot(wbe.scenarios.facility[[1]]$cases.symptom.onset.17dsum, log(wbe.scenarios.facility[[1]]$copies_per_uL))
# 
# 
# plot(log(wbe.scenarios.facility[[1]]$copies), wbe.scenarios.facility[[1]]$cases.reported)
# plot(log(wbe.scenarios.facility[[1]]$copies), wbe.scenarios.facility[[1]]$cases.symptom.onset)
# plot(log(wbe.scenarios.facility[[1]]$copies), wbe.scenarios.facility[[1]]$cases.reported.7dma)
# plot(log(wbe.scenarios.facility[[1]]$copies), wbe.scenarios.facility[[1]]$cases.symptom.onset.7dma)
# plot(log(wbe.scenarios.facility[[1]]$copies), wbe.scenarios.facility[[1]]$cases.reported.decon)
# 
# plot(wbe.scenarios.facility[[1]]$cases.symptom.onset.17dsum, log(wbe.scenarios.facility[[1]]$copies))
# plot(log(wbe.scenarios.facility[[1]]$cases.symptom.onset.17dsum), log(wbe.scenarios.facility[[1]]$copies))
# 
# 
# facility <- "CC"
# facility <- "MI"
# facility <- "NO"
# 
# 
# plot(log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]), wbe.scenarios.facility[[1]]$cases.reported[which(wbe.scenarios.facility[[1]]$facility==facility)])
# plot(log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]), wbe.scenarios.facility[[1]]$cases.symptom.onset[which(wbe.scenarios.facility[[1]]$facility==facility)])
# plot(log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]), wbe.scenarios.facility[[1]]$cases.reported.7dma[which(wbe.scenarios.facility[[1]]$facility==facility)])
# plot(log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]), wbe.scenarios.facility[[1]]$cases.symptom.onset.7dma[which(wbe.scenarios.facility[[1]]$facility==facility)])
# plot(log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]), wbe.scenarios.facility[[1]]$cases.reported.decon[which(wbe.scenarios.facility[[1]]$facility==facility)])
# 
# plot(wbe.scenarios.facility[[1]]$cases.symptom.onset.17dsum[which(wbe.scenarios.facility[[1]]$facility==facility)], log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]))
# plot(log(wbe.scenarios.facility[[1]]$cases.symptom.onset.17dsum[which(wbe.scenarios.facility[[1]]$facility==facility)]), log(wbe.scenarios.facility[[1]]$copies[which(wbe.scenarios.facility[[1]]$facility==facility)]))
# 
# 
# 
# 
# 
# plot(log(wbe.scenarios.date[[1]]$copies), wbe.scenarios.date[[1]]$cases.reported)
# plot(log(wbe.scenarios.date[[1]]$copies), wbe.scenarios.date[[1]]$cases.symptom.onset)
# plot(log(wbe.scenarios.date[[1]]$copies), wbe.scenarios.date[[1]]$cases.reported.7dma)
# plot(log(wbe.scenarios.date[[1]]$copies), wbe.scenarios.date[[1]]$cases.symptom.onset.7dma)
# plot(log(wbe.scenarios.date[[1]]$copies), wbe.scenarios.date[[1]]$cases.reported.decon)
# 
# plot(wbe.scenarios.date[[1]]$cases.symptom.onset.17dsum, log(wbe.scenarios.date[[1]]$copies))
# plot(log(wbe.scenarios.date[[1]]$cases.symptom.onset.17dsum), log(wbe.scenarios.date[[1]]$copies))




explore.Plots <- function(level=3, aggregate=TRUE, scenario=1, xvar="copies_per_uL", yvar="cases.symptom.onset.7dma", x.transform=TRUE, y.transform=FALSE){
  if(level==1){
    the.data.list <- wbe.scenarios
    the.vars <- c(xvar, yvar, "facility", "target")
  }else{
    if(level==2){
      the.data.list <- wbe.scenarios.facility
      the.vars <- c(xvar, yvar, "facility")
    }else{
      if(level==3){
        the.data.list <- wbe.scenarios.date
        the.vars <- c(xvar, yvar)
      }
    }
  }
  
  the.data <- the.data.list[[scenario]][,the.vars]
  
  if(x.transform){
    the.data[,xvar] <- log(the.data[,xvar])
  }  
  
  if(y.transform){
    the.data[,yvar] <- log(the.data[,yvar])
  }
  
  if(aggregate){
    plot(as.matrix(the.data[,xvar]), as.matrix(the.data[,yvar]))
  }else{
    if(level==1){
      targets <- c("N1", "N2")
      facilities <- c("CC", "MI", "NO")
      combos <- expand.grid(t=targets, f=facilities)
      
      plot(as.matrix(the.data[which(the.data$target==combos$t[1] & the.data$facility==combos$f[1]),xvar]), 
           as.matrix(the.data[which(the.data$target==combos$t[1] & the.data$facility==combos$f[1]),yvar]), 
           pch = 1)
      
      for(ii in 2:nrow(combos)){
        points(as.matrix(the.data[which(the.data$target==combos$t[ii] & the.data$facility==combos$f[ii]),xvar]), 
               as.matrix(the.data[which(the.data$target==combos$t[ii] & the.data$facility==combos$f[ii]),yvar]), 
               pch = ii)
      }
      
      legend("topleft", pch = 1:ii, legend = paste(combos$t, combos$f))
      
    }else{
      if(level==2){
        
      facilities <- c("CC", "MI", "NO")
      
      plot(as.matrix(the.data[which(the.data$facility==facilities[1]),xvar]), 
           as.matrix(the.data[which(the.data$facility==facilities[1]),yvar]), 
           pch = 1)
      
      for(ii in 2:length(facilities)){
        points(as.matrix(the.data[which(the.data$facility==facilities[ii]),xvar]), 
               as.matrix(the.data[which(the.data$facility==facilities[ii]),yvar]), 
               pch = ii)
      }
      
      legend("topleft", pch = 1:ii, legend = facilities)
      }else{
        plot(as.matrix(the.data[,xvar]), as.matrix(the.data[,yvar]))
      }
    }
    
  }
  

}


xvars <- c("copies_per_uL", 
           "copies", 
           "cases.symptom.onset.17dsum")

yvars <- c("cases.reported", "cases.symptom.onset", 
           "cases.reported.7dma", "cases.symptom.onset.7dma", 
           "cases.reported.decon", 
           "copies_per_uL", "copies")

x.index <- 2
y.index <- 5
level.index <- 3

for(level.index in 2:3){
for(x.index in 1:2){
for(y.index in 1:5){
par(mfrow=c(3,2), mar = c(2.1, 2.1, 0.1, 0.1))
for(i in 1:5){
explore.Plots(level=level.index, 
              aggregate = F,
              scenario=i, 
              xvar=xvars[x.index], 
              yvar=yvars[y.index], 
              x.transform=TRUE, 
              y.transform=FALSE)
}
plot.new()
text(par('usr')[1], par('usr')[4], labels = paste(c(paste0("x = ", xvars[x.index]), paste0("y = ", yvars[y.index]), paste0("level = ", level.index)), collapse = "\n"), adj = c(0, 1))
}
}
}
```













```{r, eval = F, echo=F}
fit <- MASS::glm.nb(cases.reported.decon.lag.13 ~ log(copies_N1) + log(copies_N2), data = my.data)


# cn1.scaled <- scale(my.data$copies_N1)
# cn2.scaled <- scale(my.data$copies_N2)
# 
# fit <- MASS::glm.nb(cases.reported.decon.lag.13 ~ cn1.scaled + cn2.scaled, data = my.data)


summary(fit)

car::Anova(fit)

acf(my.data$cases.reported.decon.lag.13)


fit <- MASS::glm.nb(cases.reported.decon.lag.13 ~ log(copies_N1) + log(copies_N2) + log(lag(copies_N1, 1)) + log(lag(copies_N2, 1)) + log(lead(copies_N1, 1)) + log(lead(copies_N2, 1)), data = my.data)


```













```{r, eval = F, echo = F}







# 
# 
# 
# 
# the.cors <- matrix(NA, nrow = length(the.lags)+1, ncol = length(the.vars))
# 
# 
# 
# 
# 
# for(ii in 1:length(the.vars)){
#   for(i in 1:{length(the.lags)+1}){
#     if(i == 1){
#       the.cors[i,ii] <- cor(my.data[,"copies"], my.data[,the.vars[ii]], use = "pairwise.complete.obs", method = "spearman")
#     }else{
#       if(the.lags[i-1]<0){
#         the.cors[i,ii] <- cor(my.data[,"copies"], my.data[,paste0(the.vars[ii], ".lead.", abs(the.lags[i-1]))], use = "pairwise.complete.obs", method = "spearman")
#       }else{
#         the.cors[i,ii] <- cor(my.data[,"copies"], my.data[,paste0(the.vars[ii], ".lag.", abs(the.lags[i-1]))], use = "pairwise.complete.obs", method = "spearman")
#       }
#     }
#     
#   }
# }
# 
# 
# 
# 
# 
# 
# # for(ii in 1:length(the.vars)){
# #   for(i in 1:{length(the.lags)+1}){
# #     if(i == 1){
# #       the.cors[i,ii] <- cor(my.data[,"log.copies"], my.data[,the.vars[ii]], use = "pairwise.complete.obs", method = "spearman")
# #     }else{
# #       if(the.lags[i-1]<0){
# #         the.cors[i,ii] <- cor(my.data[,"log.copies"], my.data[,paste0(the.vars[ii], ".lead.", abs(the.lags[i-1]))], use = "pairwise.complete.obs", method = "spearman")
# #       }else{
# #         the.cors[i,ii] <- cor(my.data[,"log.copies"], my.data[,paste0(the.vars[ii], ".lag.", abs(the.lags[i-1]))], use = "pairwise.complete.obs", method = "spearman")
# #       }
# #     }
# #     
# #   }
# # }
# 
# the.cors <- cbind(c(0,the.lags), the.cors) %>% as.data.frame() %>% setNames(., nm = c("lag", the.vars)) %>% arrange(lag)




```







```{r, eval = F, echo = F, message=F, warning=F, error=F}




plot_Cross_correlation <- function(this.variable, this.label, the.cors){
  
    plot(the.cors[,this.variable] ~ lag, data = the.cors, type = "h", xlab = "Lag in Days", ylab = "Correlation Coefficient", main = this.variable)
    segments(x0 = the.cors$lag[which.max(abs(the.cors[,this.variable]))], x1 = the.cors$lag[which.max(abs(the.cors[,this.variable]))], y0 = 0, y1 = the.cors[which.max(abs(the.cors[,this.variable])), this.variable], lwd = 4)
    text(par('usr')[2]+(1-par('plt')[2])*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
         par('usr')[3]-(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
         
         labels = bquote(rho == .(round(the.cors[which.max(abs(the.cors[,this.variable])), this.variable], 3)) ~ at ~ lag ~ .(the.cors$lag[which.max(abs(the.cors[,this.variable]))])), xpd = T, adj = c(1,0))
    
    text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
         par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
         labels = this.label, adj = c(0,1), xpd = T, cex = 1, font = 2)
    
    arrows(x0 = -15, x1 = -20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)
    arrows(x0 = 15, x1 = 20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)
    
    text(-15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Past", adj = c(-0.2,0.5))
    text(15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Future", adj = c(1.2,0.5))
  
  
}




plot.Cross.correlation.grid <- function(these.variables, these.labels, the.cors){
  
  for(i in 1:length(these.variables)){
    plot_Cross_correlation(these.variables[i], these.labels[i], the.cors)
  }
  
}

par(mfrow = c(3,3))

plot.Cross.correlation.grid(these.variables = yvars, these.labels = LETTERS, the.cors = copies.cors)

# 
# plot.Cross.correlation.grid(yvars, LETTERS, copies.per.uL.cors)
# 
# 
# 
# 
# 
# 
# 
# 
# dir("./consult/03-output/cross-correlations")
# 
# 
# png(filename = "./consult/03-output/cross-correlations/cross-correlations%03d.png",
#     width = 16, height = 9, units = "in", pointsize = 12,
#     bg = "white", res = 300)
# 
# lapply(xvars, function(x){
#   these.cors <- calculate.Cross.correlations(x);
#   par(mfrow = c(3,3));
#   plot.Cross.correlation.grid(yvars, c(paste0(LETTERS[1], ": ", x), LETTERS[2:length(yvars)]), these.cors)
# })
# 
# dev.off()


```













```{r eval = F, echo = F, message=F, warning=F, error=F, fig.height=6*9/16*3, fig.cap = "Cross-correlations Between the Natural Logarithm of Total Viral Copies and COVID-19 Cases"}









par(mfrow = c(3,2))
plot(cases.reported ~ lag, data = the.cors, type = "h", xlab = "Lag in Days", ylab = "Correlation Coefficient", main = "Cases by Report Date")
segments(x0 = the.cors$lag[which.max(abs(the.cors$cases.reported))], x1 = the.cors$lag[which.max(abs(the.cors$cases.reported))], y0 = 0, y1 = the.cors$cases.reported[which.max(abs(the.cors$cases.reported))], lwd = 2)
text(par('usr')[2]+(1-par('plt')[2])*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[3]-(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     
     labels = bquote(rho == .(round(the.cors$cases.reported[which.max(abs(the.cors$cases.reported))], 3)) ~ at ~ lag ~ .(the.cors$lag[which.max(abs(the.cors$cases.reported))])), xpd = T, adj = c(1,0))

text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "A", adj = c(0,1), xpd = T, cex = 1, font = 2)

arrows(x0 = -15, x1 = -20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)
arrows(x0 = 15, x1 = 20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)

text(-15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Past", adj = c(-0.2,0.5))
text(15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Future", adj = c(1.2,0.5))




plot(cases.symptom.onset ~ lag, data = the.cors, type = "h", xlab = "Lag in Days", ylab = "Correlation Coefficient", main = "Cases by Symptom Onset Date")
segments(x0 = the.cors$lag[which.max(abs(the.cors$cases.symptom.onset))], x1 = the.cors$lag[which.max(abs(the.cors$cases.symptom.onset))], y0 = 0, y1 = the.cors$cases.symptom.onset[which.max(abs(the.cors$cases.symptom.onset))], lwd = 2)
text(par('usr')[2]+(1-par('plt')[2])*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[3]-(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]),
     
     labels = bquote(rho == .(round(the.cors$cases.symptom.onset[which.max(abs(the.cors$cases.symptom.onset))], 3)) ~ at ~ lag ~ .(the.cors$lag[which.max(abs(the.cors$cases.symptom.onset))])), xpd = T, adj = c(1,0))

text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "B", adj = c(0,1), xpd = T, cex = 1, font = 2)

arrows(x0 = -15, x1 = -20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)
arrows(x0 = 15, x1 = 20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)

text(-15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Past", adj = c(-0.2,0.5))
text(15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Future", adj = c(1.2,0.5))







plot(cases.reported.7dma ~ lag, data = the.cors, type = "h", xlab = "Lag in Days", ylab = "Correlation Coefficient", main = "7-day Moving Average of Cases by Report Date")
segments(x0 = the.cors$lag[which.max(abs(the.cors$cases.reported.7dma))], x1 = the.cors$lag[which.max(abs(the.cors$cases.reported.7dma))], y0 = 0, y1 = the.cors$cases.reported.7dma[which.max(abs(the.cors$cases.reported.7dma))], lwd = 2)
text(par('usr')[2]+(1-par('plt')[2])*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[3]-(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]),
     
     labels = bquote(rho == .(round(the.cors$cases.reported.7dma[which.max(abs(the.cors$cases.reported.7dma))], 3)) ~ at ~ lag ~ .(the.cors$lag[which.max(abs(the.cors$cases.reported.7dma))])), xpd = T, adj = c(1,0))

text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "C", adj = c(0,1), xpd = T, cex = 1, font = 2)

arrows(x0 = -15, x1 = -20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)
arrows(x0 = 15, x1 = 20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)

text(-15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Past", adj = c(-0.2,0.5))
text(15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Future", adj = c(1.2,0.5))






plot(cases.symptom.onset.7dma ~ lag, data = the.cors, type = "h", xlab = "Lag in Days", ylab = "Correlation Coefficient", main = "7-day Moving Average of Cases by Symptom Onset Date")
segments(x0 = the.cors$lag[which.max(abs(the.cors$cases.symptom.onset.7dma))], x1 = the.cors$lag[which.max(abs(the.cors$cases.symptom.onset.7dma))], y0 = 0, y1 = the.cors$cases.symptom.onset.7dma[which.max(abs(the.cors$cases.symptom.onset.7dma))], lwd = 2)
text(par('usr')[2]+(1-par('plt')[2])*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[3]-(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     
     labels = bquote(rho == .(round(the.cors$cases.symptom.onset.7dma[which.max(abs(the.cors$cases.symptom.onset.7dma))], 3)) ~ at ~ lag ~ .(the.cors$lag[which.max(abs(the.cors$cases.symptom.onset.7dma))])), xpd = T, adj = c(1,0))

text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "D", adj = c(0,1), xpd = T, cex = 1, font = 2)

arrows(x0 = -15, x1 = -20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)
arrows(x0 = 15, x1 = 20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)

text(-15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Past", adj = c(-0.2,0.5))
text(15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Future", adj = c(1.2,0.5))









plot(cases.reported.decon ~ lag, data = the.cors, type = "h", xlab = "Lag in Days", ylab = "Correlation Coefficient", main = "Deconvoluted Cases by Report Date")
segments(x0 = the.cors$lag[which.max(abs(the.cors$cases.reported.decon))], x1 = the.cors$lag[which.max(abs(the.cors$cases.reported.decon))], y0 = 0, y1 = the.cors$cases.reported.decon[which.max(abs(the.cors$cases.reported.decon))], lwd = 2)


text(par('usr')[2]+(1-par('plt')[2])*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[3]-(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     
     labels = bquote(rho == .(round(the.cors$cases.reported.decon[which.max(abs(the.cors$cases.reported.decon))], 3)) ~ at ~ lag ~ .(the.cors$lag[which.max(abs(the.cors$cases.reported.decon))])), xpd = T, adj = c(1,0))

text(par('usr')[1]-par('plt')[1]*diff(par('usr')[1:2])/diff(par('plt')[1:2]), 
     par('usr')[4]+(1-par('plt')[4])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), 
     labels = "E", adj = c(0,1), xpd = T, cex = 1, font = 2)

arrows(x0 = -15, x1 = -20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)
arrows(x0 = 15, x1 = 20, y0 = par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), y1 =  par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), code = 2, xpd = T, length = par('fin')[1]*0.02)

text(-15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Past", adj = c(-0.2,0.5))
text(15, par('usr')[3]-0.5*(par('plt')[3])*diff(par('usr')[3:4])/diff(par('plt')[3:4]), xpd = T, labels = "Future", adj = c(1.2,0.5))
```



























## Exploring Model Fits



```{r, eval = F, echo = F}

my.data <- wbe.scenarios.date[[2]]

my.data %<>% mutate(log.copies = log(copies+1), 
                    log.copies.per.uL = log(copies_per_uL+1))

my.data <- my.data[-which(my.data$sample_date%in%as.Date(c("2020-11-19", "2020-11-18"))),]



fit1 <- lm(cases.symptom.onset ~ log.copies, data = my.data)
# plot(fit1)
summary(fit1)
plot(cases.symptom.onset ~ log.copies, data = my.data)
abline(fit1)

fit2 <- glm(cases.symptom.onset ~ log.copies, data = my.data, family = "gaussian")
# plot(fit2)
summary(fit2)
plot(cases.symptom.onset ~ log.copies, data = my.data)
abline(fit2)

fit3 <- glm(cases.symptom.onset ~ log.copies, data = my.data, family = "poisson")
# plot(fit3)
summary(fit3)
1-pchisq(fit3$deviance, fit3$df.residual)

plot(cases.symptom.onset ~ log.copies, data = my.data)
preds.fit3 <- cbind(my.data$log.copies, predict(fit3, type = "response"))
preds.fit3 <- preds.fit3[order(preds.fit3[,1]),]
lines(preds.fit3[,1], preds.fit3[,2])



# fit3 <- glm(cases.symptom.onset ~ copies, data = my.data, family = "poisson")
# # plot(fit3)
# summary(fit3)
# plot(cases.symptom.onset ~ copies, data = my.data)
# preds.fit3 <- cbind(my.data$copies, predict(fit3, type = "response"))
# preds.fit3 <- preds.fit3[order(preds.fit3[,1]),]
# lines(preds.fit3[,1], preds.fit3[,2])


fit4 <- MASS::glm.nb(cases.symptom.onset ~ log.copies, data = my.data)
# plot(fit4)
summary(fit4)

1-pchisq(fit4$deviance, fit4$df.residual)

plot(cases.symptom.onset ~ log.copies, data = my.data)
preds.fit4 <- cbind(my.data$log.copies, predict(fit4, type = "response"))
preds.fit4 <- preds.fit4[order(preds.fit4[,1]),]
lines(preds.fit4[,1], preds.fit4[,2])

```






```{r, eval = F, echo = F}

my.data <- wbe.scenarios.date[[2]]

my.data %<>% mutate(log.copies = log(copies+1), 
                    log.copies.per.uL = log(copies_per_uL+1))

my.data <- my.data[-which(my.data$sample_date%in%as.Date(c("2020-11-19", "2020-11-18"))),]




# fit1 <- lm(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# # plot(fit1)
# summary(fit1)
# plot(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# abline(fit1)

fit2 <- glm(cases.symptom.onset.7dma ~ log.copies, data = my.data, family = "gaussian")
# plot(fit2)
summary(fit2)
plot(cases.symptom.onset.7dma ~ log.copies, data = my.data)
abline(fit2)



# fit3 <- glm(cases.symptom.onset.7dma ~ log.copies, data = my.data, family = "poisson")
# # plot(fit3)
# summary(fit3)
# 1-pchisq(fit3$deviance, fit3$df.residual)
# 
# plot(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# preds.fit3 <- cbind(my.data$log.copies, predict(fit3, type = "response"))
# preds.fit3 <- preds.fit3[order(preds.fit3[,1]),]
# lines(preds.fit3[,1], preds.fit3[,2])



# fit3 <- glm(cases.symptom.onset ~ copies, data = my.data, family = "poisson")
# # plot(fit3)
# summary(fit3)
# plot(cases.symptom.onset ~ copies, data = my.data)
# preds.fit3 <- cbind(my.data$copies, predict(fit3, type = "response"))
# preds.fit3 <- preds.fit3[order(preds.fit3[,1]),]
# lines(preds.fit3[,1], preds.fit3[,2])


fit4 <- MASS::glm.nb(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# plot(fit4)
summary(fit4)

1-pchisq(fit4$deviance, fit4$df.residual)

plot(cases.symptom.onset.7dma ~ log.copies, data = my.data)
preds.fit4 <- cbind(my.data$log.copies, predict(fit4, type = "response"))
preds.fit4 <- preds.fit4[order(preds.fit4[,1]),]
lines(preds.fit4[,1], preds.fit4[,2])


```







```{r, eval = F, echo = F}

my.data <- wbe.scenarios.date[[2]]

my.data %<>% mutate(log.copies = log(copies+1), 
                    log.copies.per.uL = log(copies_per_uL+1))

my.data <- my.data[-which(my.data$sample_date%in%as.Date(c("2020-11-19", "2020-11-18", "2020-11-04")) | my.data$sample_date < min(wbe$sample_date) | is.na(my.data$log.copies.per.uL)),]




# fit1 <- lm(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# # plot(fit1)
# summary(fit1)
# plot(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# abline(fit1)

fit2 <- glm(cases.reported.decon ~ log.copies, data = my.data, family = "gaussian")
# plot(fit2)
summary(fit2)
plot(cases.reported.decon ~ log.copies, data = my.data)
abline(fit2)



# fit3 <- glm(cases.symptom.onset.7dma ~ log.copies, data = my.data, family = "poisson")
# # plot(fit3)
# summary(fit3)
# 1-pchisq(fit3$deviance, fit3$df.residual)
# 
# plot(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# preds.fit3 <- cbind(my.data$log.copies, predict(fit3, type = "response"))
# preds.fit3 <- preds.fit3[order(preds.fit3[,1]),]
# lines(preds.fit3[,1], preds.fit3[,2])



# fit3 <- glm(cases.symptom.onset ~ copies, data = my.data, family = "poisson")
# # plot(fit3)
# summary(fit3)
# plot(cases.symptom.onset ~ copies, data = my.data)
# preds.fit3 <- cbind(my.data$copies, predict(fit3, type = "response"))
# preds.fit3 <- preds.fit3[order(preds.fit3[,1]),]
# lines(preds.fit3[,1], preds.fit3[,2])


fit4 <- MASS::glm.nb(cases.reported.decon ~ log.copies, data = my.data)
# plot(fit4)
summary(fit4)

1-pchisq(fit4$deviance, fit4$df.residual)

plot(cases.reported.decon ~ log.copies, data = my.data)
preds.fit4 <- cbind(my.data$log.copies, predict(fit4, type = "response"))
preds.fit4 <- preds.fit4[order(preds.fit4[,1]),]
lines(preds.fit4[,1], preds.fit4[,2])


```





```{r, eval = F, echo = F}

my.data <- wbe.scenarios.facility[[2]] 

my.data %<>% mutate(log.copies = log(copies+1), 
                    log.copies.per.uL = log(copies_per_uL+1))

my.data <- my.data[-which(my.data$sample_date%in%as.Date(c("2020-11-19", "2020-11-18", "2020-11-04")) | my.data$sample_date < min(wbe$sample_date) | is.na(my.data$log.copies.per.uL)),]

my.data.wide <- my.data %>% select(sample_date, facility, log.copies) %>% tidyr::pivot_wider(names_from = facility, values_from = log.copies) %>% left_join(., my.data%>%select(sample_date, cases.reported.decon), by = "sample_date")


# fit1 <- lm(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# # plot(fit1)
# summary(fit1)
# plot(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# abline(fit1)

fit2 <- glm(cases.reported.decon ~ log.copies, data = my.data, family = "gaussian")
# plot(fit2)
summary(fit2)
plot(cases.reported.decon ~ log.copies, data = my.data)
abline(fit2)



# fit3 <- glm(cases.symptom.onset.7dma ~ log.copies, data = my.data, family = "poisson")
# # plot(fit3)
# summary(fit3)
# 1-pchisq(fit3$deviance, fit3$df.residual)
# 
# plot(cases.symptom.onset.7dma ~ log.copies, data = my.data)
# preds.fit3 <- cbind(my.data$log.copies, predict(fit3, type = "response"))
# preds.fit3 <- preds.fit3[order(preds.fit3[,1]),]
# lines(preds.fit3[,1], preds.fit3[,2])



# fit3 <- glm(cases.symptom.onset ~ copies, data = my.data, family = "poisson")
# # plot(fit3)
# summary(fit3)
# plot(cases.symptom.onset ~ copies, data = my.data)
# preds.fit3 <- cbind(my.data$copies, predict(fit3, type = "response"))
# preds.fit3 <- preds.fit3[order(preds.fit3[,1]),]
# lines(preds.fit3[,1], preds.fit3[,2])


fit4 <- MASS::glm.nb(cases.reported.decon ~ log.copies, data = my.data)
# plot(fit4)
summary(fit4)

1-pchisq(fit4$deviance, fit4$df.residual)

plot(cases.reported.decon ~ log.copies, data = my.data)
preds.fit4 <- cbind(my.data$log.copies, predict(fit4, type = "response"))
preds.fit4 <- preds.fit4[order(preds.fit4[,1]),]
lines(preds.fit4[,1], preds.fit4[,2])


```





















```{r, eval = F, echo = F}
wbe_covid %<>% 
  group_by(sample_date, target, facility, biological_replicate, cases, moving_avg_cases) %>% 
  summarise(copies.lod = mean(copy_num_uL_rxn, na.rm=T), copies = mean(copies, na.rm=T))



plot(wbe_covid$sample_date[which(complete.cases(wbe_covid[,c("sample_date", "copies.lod")]) & wbe_covid$target=="N1")], log10(wbe_covid$copies.lod[which(complete.cases(wbe_covid[,c("sample_date", "copies.lod")]) & wbe_covid$target=="N1")]*20/2*25/3*60*1e6))

points(wbe_covid$sample_date[which(complete.cases(wbe_covid[,c("sample_date", "copies")]) & wbe_covid$target=="N1")], log10(wbe_covid$copies[which(complete.cases(wbe_covid[,c("sample_date", "copies")]) & wbe_covid$target=="N1")]*20/2*25/3*60*1e6), pch = 16)

abline(h = log10(2e-4*20/2*25/3*60*1e6))

abline(h = log10(exp(qqnorm.copies.n1$lod)*20/2*25/3*60*1e6))
abline(h = log10(exp(qqnorm.copies.n1$lod)*20/2*25/3*60*1e6/sqrt(2)))
abline(h = log10(exp(qqnorm.copies.n1$lod)*20/2*25/3*60*1e6)/sqrt(2))


abline(h = log10(exp(qqnorm.copies.n1$lod)*20/2*25/3*60*1e6)/2, lty=2)
abline(h = log10(exp(qqnorm.copies.n1$lod)*20/2*25/3*60*1e6/2), lty=2)

10^8

```







## Distributed Lag (Lead) Models

To further investigate the temporal association of wastewater viral loads and case incidence, a distributed lag (lead) model will be fit to the data in a regression of cases on wastewater viral loads at various leads (Equation \@ref(eq:dlm) ; adapted from [Peccia et al, 2020](https://www.nature.com/articles/s41587-020-0684-z)). 


\begin{equation}
  Cases = \beta_0 + \sum_{j=-1}^d \beta_j VL_{t-j}
  (\#eq:dlm)
\end{equation}

where *Cases* refers to the case counts, *d* refers to the lead times, and *VL* refers to he wastewater viral loads at time *t*. 


Since the smoothed case counts are not discrete, a Poisson regression may not be appropriate. Instead, I fit a generalized linear model for a Gamma distribution with a natural logarithm link function. 


```{r, eval = F, echo = F}


the.lags <- -10:10
the.lags <- the.lags[-which(the.lags==0)]

the.vars <- "copies"

# the.vars <- c("cases.reported", "cases.symptom.onset", "cases.reported.7dma", "cases.symptom.onset.7dma", "cases.reported.decon")

for(ii in 1:length(the.vars)){
  for(i in 1:length(the.lags)){
    if(the.lags[i]<0){
      my.data[,paste0(the.vars[ii], ".lead.", abs(the.lags[i]))] <- lead(my.data[,the.vars[ii]], n = abs(the.lags[i]))
    }else{
      my.data[,paste0(the.vars[ii], ".lag.", abs(the.lags[i]))] <- lag(my.data[,the.vars[ii]], n = the.lags[i])
    }
  }
}






temp <- my.data %>% select(copies.interpolated, names(my.data)[grepl("cases.reported.7dma", names(my.data))]) %>% 
          mutate(lag1 = log(cases.reported.7dma.lag.1) * 17 / 18, 
                 lag2 = log(cases.reported.7dma.lag.2) * 16 / 18,
                 lag3 = log(cases.reported.7dma.lag.3) * 15 / 18,
                 lag4 = log(cases.reported.7dma.lag.4) * 14 / 18,
                 lag5 = log(cases.reported.7dma.lag.5) * 13 / 18, 
                 lag6 = log(cases.reported.7dma.lag.6) * 12 / 18,
                 lag7 = log(cases.reported.7dma.lag.7) * 11 / 18,
                 lag8 = log(cases.reported.7dma.lag.8) * 10 / 18,
                 lag9 = log(cases.reported.7dma.lag.9) * 9 / 18, 
                 lag10 = log(cases.reported.7dma.lag.10) * 8 / 18,
                 lag11 = log(cases.reported.7dma.lag.11) * 7 / 18,
                 lag12 = log(cases.reported.7dma.lag.12) * 6 / 18,
                 lag13 = log(cases.reported.7dma.lag.13) * 5 / 18, 
                 lag14 = log(cases.reported.7dma.lag.14) * 4 / 18,
                 lag15 = log(cases.reported.7dma.lag.15) * 3 / 18,
                 lag16 = log(cases.reported.7dma.lag.16) * 2 / 18,
                 lag17 = log(cases.reported.7dma.lag.17) * 1 / 18
                 )

temp <- my.data %>% select(copies.interpolated, names(my.data)[grepl("cases.symptom.onset.7dma", names(my.data))]) %>% 
          mutate(lag1 = log(cases.symptom.onset.7dma.lag.1) * 17 / 18, 
                 lag2 = log(cases.symptom.onset.7dma.lag.2) * 16 / 18,
                 lag3 = log(cases.symptom.onset.7dma.lag.3) * 15 / 18,
                 lag4 = log(cases.symptom.onset.7dma.lag.4) * 14 / 18,
                 lag5 = log(cases.symptom.onset.7dma.lag.5) * 13 / 18, 
                 lag6 = log(cases.symptom.onset.7dma.lag.6) * 12 / 18,
                 lag7 = log(cases.symptom.onset.7dma.lag.7) * 11 / 18,
                 lag8 = log(cases.symptom.onset.7dma.lag.8) * 10 / 18,
                 lag9 = log(cases.symptom.onset.7dma.lag.9) * 9 / 18, 
                 lag10 = log(cases.symptom.onset.7dma.lag.10) * 8 / 18,
                 lag11 = log(cases.symptom.onset.7dma.lag.11) * 7 / 18,
                 lag12 = log(cases.symptom.onset.7dma.lag.12) * 6 / 18,
                 lag13 = log(cases.symptom.onset.7dma.lag.13) * 5 / 18, 
                 lag14 = log(cases.symptom.onset.7dma.lag.14) * 4 / 18,
                 lag15 = log(cases.symptom.onset.7dma.lag.15) * 3 / 18,
                 lag16 = log(cases.symptom.onset.7dma.lag.16) * 2 / 18,
                 lag17 = log(cases.symptom.onset.7dma.lag.17) * 1 / 18
                 )



temp <- my.data %>% select(copies.interpolated, names(my.data)[grepl("cases.reported.decon", names(my.data))]) %>% 
          mutate(lag1 = log(cases.reported.decon.lag.1) * 17 / 18, 
                 lag2 = log(cases.reported.decon.lag.2) * 16 / 18,
                 lag3 = log(cases.reported.decon.lag.3) * 15 / 18,
                 lag4 = log(cases.reported.decon.lag.4) * 14 / 18,
                 lag5 = log(cases.reported.decon.lag.5) * 13 / 18, 
                 lag6 = log(cases.reported.decon.lag.6) * 12 / 18,
                 lag7 = log(cases.reported.decon.lag.7) * 11 / 18,
                 lag8 = log(cases.reported.decon.lag.8) * 10 / 18,
                 lag9 = log(cases.reported.decon.lag.9) * 9 / 18, 
                 lag10 = log(cases.reported.decon.lag.10) * 8 / 18,
                 lag11 = log(cases.reported.decon.lag.11) * 7 / 18,
                 lag12 = log(cases.reported.decon.lag.12) * 6 / 18,
                 lag13 = log(cases.reported.decon.lag.13) * 5 / 18, 
                 lag14 = log(cases.reported.decon.lag.14) * 4 / 18,
                 lag15 = log(cases.reported.decon.lag.15) * 3 / 18,
                 lag16 = log(cases.reported.decon.lag.16) * 2 / 18,
                 lag17 = log(cases.reported.decon.lag.17) * 1 / 18
                 )




summary(glm(copies.interpolated ~ 
            log(cases.symptom.onset.7dma)
            + lag1
            + lag2
            + lag3
            + lag4
            + lag5
            + lag6
            + lag7
            + lag8
            + lag9
            + lag10
            + lag11
            + lag12
            + lag13
            + lag14
            + lag15
            + lag16
            + lag17
            
            , data = temp, family = Gamma(link = "log"), na.action = na.exclude))




summary(glm(copies.interpolated ~ 
            log(cases.reported.7dma)
            + log(lag(cases.reported.7dma, 1))*5/6
            + log(lag(cases.reported.7dma, 2))*4/6
            + log(lag(cases.reported.7dma, 3))*3/6
            + log(lag(cases.reported.7dma, 4))*2/6
            + log(lag(cases.reported.7dma, 5))*1/6
            , data = my.data, family = Gamma(link = "log"), na.action = na.exclude))








summary(glm(cases.reported.7dma ~ 
              # log(lead(copies.interpolated, 15)) +
              # log(lead(copies.interpolated, 14)) +
              # log(lead(copies.interpolated, 13)) +
              # log(lead(copies.interpolated, 12)) +
              # log(lead(copies.interpolated, 11)) +
              # log(lead(copies.interpolated, 10)) +
              # log(lead(copies.interpolated, 9)) +
              # log(lead(copies.interpolated, 8)) +
              # log(lead(copies.interpolated, 7)) +
              # log(lead(copies.interpolated, 6)) +
              # log(lead(copies.interpolated, 5)) +
              # log(lead(copies.interpolated, 4)) +
              # log(lead(copies.interpolated, 3)) +
              # log(lead(copies.interpolated, 2)) +
              log(lead(copies.interpolated, 1)) +
              log(copies.interpolated)
              + log(lag(copies.interpolated, 1))
              # + log(lag(copies.interpolated, 2))
              # + log(lag(copies.interpolated, 3))
              # + log(lag(copies.interpolated, 4))
              # + log(lag(copies.interpolated, 5))
              # + log(lag(copies.interpolated, 6))
              # + log(lag(copies.interpolated, 7))
              # + log(lag(copies.interpolated, 8))
              #  + log(lag(copies.interpolated, 9))
              #  + log(lag(copies.interpolated, 10))
            , data = my.data, family = Gamma(link = "log"), na.action = na.exclude))



summary(glm(cases.symptom.onset.7dma ~ 
              # log(lead(copies.interpolated, 15)) + 
              # log(lead(copies.interpolated, 14)) + 
              # log(lead(copies.interpolated, 13)) + 
              # log(lead(copies.interpolated, 12)) + 
              # log(lead(copies.interpolated, 11)) + 
              # log(lead(copies.interpolated, 10)) + 
              # log(lead(copies.interpolated, 9)) + 
              # log(lead(copies.interpolated, 8)) + 
              # log(lead(copies.interpolated, 7)) + 
              # log(lead(copies.interpolated, 6)) + 
              # log(lead(copies.interpolated, 5)) + 
              log(lead(copies.interpolated, 4)) + 
              log(lead(copies.interpolated, 3)) + 
              log(lead(copies.interpolated, 2)) + 
              log(lead(copies.interpolated, 1)) + 
              log(copies.interpolated) 
               + log(lag(copies.interpolated, 1))
               + log(lag(copies.interpolated, 2))
               + log(lag(copies.interpolated, 3))
               + log(lag(copies.interpolated, 4))
               # + log(lag(copies.interpolated, 5))
              #  + log(lag(copies.interpolated, 6)) 
              #  + log(lag(copies.interpolated, 7)) 
              #  + log(lag(copies.interpolated, 8)) 
              #  + log(lag(copies.interpolated, 9)) 
              #  + log(lag(copies.interpolated, 10))
            , data = my.data, family = Gamma(link = "log"), na.action = na.exclude))






summary(glm(cases.reported.decon ~ 
              # log(lead(copies.interpolated, 15)) + 
              # log(lead(copies.interpolated, 14)) + 
              # log(lead(copies.interpolated, 13)) + 
              # log(lead(copies.interpolated, 12)) + 
              # log(lead(copies.interpolated, 11)) + 
              # log(lead(copies.interpolated, 10)) + 
              # log(lead(copies.interpolated, 9)) + 
              # log(lead(copies.interpolated, 8)) + 
              # log(lead(copies.interpolated, 7)) + 
              # log(lead(copies.interpolated, 6)) + 
              log(lead(copies.interpolated, 5)) + 
              log(lead(copies.interpolated, 4)) + 
              log(lead(copies.interpolated, 3)) + 
              log(lead(copies.interpolated, 2)) + 
              log(lead(copies.interpolated, 1)) + 
              log(copies.interpolated) 
               + log(lag(copies.interpolated, 1))
               + log(lag(copies.interpolated, 2))
               + log(lag(copies.interpolated, 3))
               + log(lag(copies.interpolated, 4))
               + log(lag(copies.interpolated, 5))
              #  + log(lag(copies.interpolated, 6)) 
              #  + log(lag(copies.interpolated, 7)) 
              #  + log(lag(copies.interpolated, 8)) 
              #  + log(lag(copies.interpolated, 9)) 
              #  + log(lag(copies.interpolated, 10))
            , data = my.data, family = Gamma(link = "log"), na.action = na.exclude))








fit <- dLagM::dlm(x = my.data$cases.reported.7dma, y = my.data$copies.interpolated, q = 21)

summary(fit)


dLagM::rolCorPlot(x = ts(my.data$cases.reported.7dma, start = min(my.data$sample_date), end = max(my.data$sample_date)), y = ts(my.data$copies.interpolated, start = min(my.data$sample_date), end = max(my.data$sample_date)), width = 7)


fit.almon <- dLagM::polyDlm(x = my.data$cases.reported.7dma, y = my.data$copies.interpolated, q = 21, k = 3)

fit.almon <- dLagM::polyDlm(x = log(my.data$copies.interpolated), y = my.data$cases.reported.7dma, q = 7, k = 2)

summary(fit.almon)



my.data %<>% mutate(cases.so.21d.rs = zoo::rollsum(cases.symptom.onset.7dma, 21, align = "right")) %>% mutate(cases.so.21d.rs.lag.1 = lag(cases.so.21d.rs, 1))




my.q <- 1:49
my.k <- 1:5

my.bic <- matrix(NA, nrow = length(my.q), ncol = length(my.k))


for(this.q in 1:length(my.q)){
  for(this.k in 1:length(my.k)){
    fit.almon <- dLagM::polyDlm(x = my.data$cases.symptom.onset.7dma, y = log(my.data$copies.interpolated), q = this.q, k = this.k)
    my.bic[this.q, this.k] <- BIC(fit.almon)
  }
}


which(my.bic == min(my.bic), arr.ind = TRUE)

the.q <- my.q[22]
the.k <- my.k[2]

fit.almon <- dLagM::polyDlm(x = my.data$cases.symptom.onset.7dma, y = log(my.data$copies.interpolated), q = the.q, k = the.k)

summary(fit.almon)

BIC(fit.almon)


preds <- predict(fit.almon$model)

my.data$dlm.preds <- NA
my.data$dlm.preds[as.numeric(names(preds))+the.q] <- preds


acf(resid(fit.almon$model))
plot(fit.almon$model)


plot(my.data$sample_date, log(my.data$copies.interpolated), type = "l")

lines(my.data$sample_date, my.data$dlm.preds, lty = 3, lwd = 3)













test <- my.data %>% filter(sample_date>=as.Date("2021-01-01"))
train <- my.data %>% filter(sample_date<as.Date("2021-01-01"))



my.q <- 1:59
my.k <- 1:7

my.bic <- matrix(NA, nrow = length(my.q), ncol = length(my.k))
my.sse <- matrix(NA, nrow = length(my.q), ncol = length(my.k))



for(this.q in 1:length(my.q)){
  for(this.k in 1:length(my.k)){
    if(this.k > this.q){next}
    fit.almon <- dLagM::polyDlm(x = log(train$copies.interpolated.smooth), y = log(train$cases.symptom.onset.7dma), q = this.q, k = this.k)
    my.bic[this.q, this.k] <- BIC(fit.almon)
    
    my.forecast <- dLagM::forecast(model = fit.almon, x = log(test$copies.interpolated.smooth), h = length(log(test$copies.interpolated)), interval = F)
    
    my.sse[this.q, this.k] <- sum((log(test$cases.symptom.onset.7dma)-my.forecast$forecasts)^2, na.rm = T)
    
  }
}


which(my.bic == min(my.bic), arr.ind = TRUE)

which(my.sse == min(my.sse[which(my.sse!=0)]), arr.ind = TRUE)

the.q <- my.q[53]
the.k <- my.k[2]

fit.almon <- dLagM::polyDlm(x = log(train$copies.interpolated.smooth), y = log(train$cases.symptom.onset.7dma), q = the.q, k = the.k)

summary(fit.almon)

BIC(fit.almon)


preds <- predict(fit.almon$model)

train$dlm.preds <- NA
train$dlm.preds[as.numeric(names(preds))+the.q] <- preds


acf(resid(fit.almon$model))
plot(fit.almon$model)




png(filename = "./consult/03-output/for-pres/forecast.png", width = 16, height = 9, units = "in", res = 300, pointsize = 16)
plot(my.data$sample_date, my.data$cases.symptom.onset.7dma, type = "l")

lines(train$sample_date, exp(train$dlm.preds), lty = 5, lwd = 3)




my.forecast <- dLagM::forecast(model = fit.almon, x = log(test$copies.interpolated.smooth)[1:34], h = length(log(test$copies.interpolated)[1:34]), interval = T)

test$dlm.preds <- NA
test$dlm.preds[1:34] <- my.forecast$forecasts$Estimate

test$dlm.preds.lb <- NA
test$dlm.preds.lb[1:34] <- my.forecast$forecasts$Lower

test$dlm.preds.ub <- NA
test$dlm.preds.ub[1:34] <- my.forecast$forecasts$Upper


lines(test$sample_date, exp(test$dlm.preds), lty = 3, lwd = 3, col = "blue")

lines(test$sample_date, exp(test$dlm.preds.lb), lty = 1, lwd = 1, col = "blue")
lines(test$sample_date, exp(test$dlm.preds.ub), lty = 1, lwd = 1, col = "blue")



dev.off()















test <- my.data %>% filter(sample_date>=as.Date("2021-01-01"))
train <- my.data %>% filter(sample_date<as.Date("2021-01-01"))



my.q <- 1:59
my.k <- 1:7

my.bic <- matrix(NA, nrow = length(my.q), ncol = length(my.k))
my.sse <- matrix(NA, nrow = length(my.q), ncol = length(my.k))



for(this.q in 1:length(my.q)){
  for(this.k in 1:length(my.k)){
    if(this.k > this.q){next}
    fit.almon <- dLagM::polyDlm(x = log(train$copies.interpolated.smooth), y = log(train$pcr.pos.7dma), q = this.q, k = this.k)
    my.bic[this.q, this.k] <- BIC(fit.almon)
    
    my.forecast <- dLagM::forecast(model = fit.almon, x = log(test$copies.interpolated.smooth), h = length(log(test$copies.interpolated)), interval = F)
    
    my.sse[this.q, this.k] <- sum((log(test$pcr.pos.7dma)-my.forecast$forecasts)^2, na.rm = T)
    
  }
}


which(my.bic == min(my.bic), arr.ind = TRUE)

which(my.sse == min(my.sse[which(my.sse!=0)]), arr.ind = TRUE)

the.q <- my.q[51]
the.k <- my.k[2]

fit.almon <- dLagM::polyDlm(x = log(train$copies.interpolated.smooth), y = log(train$pcr.pos.7dma), q = the.q, k = the.k)

summary(fit.almon)

BIC(fit.almon)


preds <- predict(fit.almon$model)

train$dlm.preds <- NA
train$dlm.preds[as.numeric(names(preds))+the.q] <- preds


acf(resid(fit.almon$model))
plot(fit.almon$model)


plot(my.data$sample_date, my.data$pcr.pos.7dma, type = "l")

lines(train$sample_date, exp(train$dlm.preds), lty = 5, lwd = 3)




my.forecast <- dLagM::forecast(model = fit.almon, x = log(test$copies.interpolated.smooth)[1:34], h = length(log(test$copies.interpolated)[1:34]), interval = T)

test$dlm.preds <- NA
test$dlm.preds[1:34] <- my.forecast$forecasts$Estimate

test$dlm.preds.lb <- NA
test$dlm.preds.lb[1:34] <- my.forecast$forecasts$Lower

test$dlm.preds.ub <- NA
test$dlm.preds.ub[1:34] <- my.forecast$forecasts$Upper


lines(test$sample_date, exp(test$dlm.preds), lty = 3, lwd = 3, col = "blue")

lines(test$sample_date, exp(test$dlm.preds.lb), lty = 1, lwd = 1, col = "blue")
lines(test$sample_date, exp(test$dlm.preds.ub), lty = 1, lwd = 1, col = "blue")




























my.data$copies.interpolated.21dma <- zoo::rollmean(my.data$copies.interpolated, k = 21, fill = NA, align = 'right')

plot(my.data$sample_date[which(complete.cases(my.data$copies))], my.data$copies[which(complete.cases(my.data$copies))], type = "o")

# lines(my.data$sample_date, my.data$copies.interpolated.smooth, lty = 5, lwd = 2)

lines(my.data$sample_date, my.data$copies.interpolated.21dma, lty = 3, lwd = 3)













test <- my.data %>% filter(sample_date>=as.Date("2021-01-01"))
train <- my.data %>% filter(sample_date<as.Date("2021-01-01"))



my.q <- 1:59
my.k <- 1:7

my.bic <- matrix(NA, nrow = length(my.q), ncol = length(my.k))
my.sse <- matrix(NA, nrow = length(my.q), ncol = length(my.k))



for(this.q in 1:length(my.q)){
  for(this.k in 1:length(my.k)){
    if(this.k > this.q){next}
    fit.almon <- dLagM::polyDlm(x = log(train$copies.interpolated.21dma), y = log(train$pcr.pos.7dma), q = this.q, k = this.k)
    my.bic[this.q, this.k] <- BIC(fit.almon)
    
    my.forecast <- dLagM::forecast(model = fit.almon, x = log(test$copies.interpolated.21dma), h = length(log(test$copies.interpolated)), interval = F)
    
    my.sse[this.q, this.k] <- sum((log(test$pcr.pos.7dma)-my.forecast$forecasts)^2, na.rm = T)
    
  }
}


which(my.bic == min(my.bic), arr.ind = TRUE)

which(my.sse == min(my.sse[which(my.sse!=0)]), arr.ind = TRUE)

the.q <- my.q[3]
the.k <- my.k[3]

fit.almon <- dLagM::polyDlm(x = log(train$copies.interpolated.21dma), y = log(train$pcr.pos.7dma), q = the.q, k = the.k)

summary(fit.almon)

BIC(fit.almon)


preds <- predict(fit.almon$model)

train$dlm.preds <- NA
train$dlm.preds[as.numeric(names(preds))+the.q] <- preds


acf(resid(fit.almon$model))
plot(fit.almon$model)


plot(my.data$sample_date, my.data$pcr.pos.7dma, type = "l")

lines(train$sample_date, exp(train$dlm.preds), lty = 5, lwd = 3)




my.forecast <- dLagM::forecast(model = fit.almon, x = log(test$copies.interpolated.21dma)[1:24], h = length(log(test$copies.interpolated)[1:24]), interval = T)

test$dlm.preds <- NA
test$dlm.preds[1:24] <- my.forecast$forecasts$Estimate

test$dlm.preds.lb <- NA
test$dlm.preds.lb[1:24] <- my.forecast$forecasts$Lower

test$dlm.preds.ub <- NA
test$dlm.preds.ub[1:24] <- my.forecast$forecasts$Upper


lines(test$sample_date, exp(test$dlm.preds), lty = 3, lwd = 3, col = "blue")

lines(test$sample_date, exp(test$dlm.preds.lb), lty = 1, lwd = 1, col = "blue")
lines(test$sample_date, exp(test$dlm.preds.ub), lty = 1, lwd = 1, col = "blue")

















temp <- my.data$copies.interpolated


temp.mat <- cbind(temp, lag(temp, 1))

for(i in 2:21){
  temp.mat <- cbind(temp.mat, lag(temp, i))
}



# View(cbind(fit.almon$designMatrix.x, log(temp.mat[22:335,])))

temp1 <- rowSums(log(temp.mat))


temp.mat <- lag(log(temp), 1)

for(i in 2:21){
  temp.mat <- cbind(temp.mat, lag(log(temp), i) * i)
}

temp2 <- rowSums(temp.mat)


# View(cbind(fit.almon$designMatrix[,2], temp2))





fit.almon <- dLagM::polyDlm(x = log(train$copies.interpolated), y = log(train$pcr.pos.7dma), q = 21, k = 1)

summary(fit.almon)

summary(lm(log(my.data$pcr.pos.7dma[22:335])~fit.almon$designMatrix))


summary(lm(log(my.data$pcr.pos.7dma[22:335]) ~ temp1[22:335] + temp2[22:335]))




fit <- glm(my.data$pcr.pos.7dma ~ 
             lag(my.data$pcr.pos.7dma, 1) + 
             # lag(my.data$pcr.pos.7dma, 2) + 
             # lag(my.data$pcr.pos.7dma, 3) + 
             # lag(my.data$pcr.pos.7dma, 4) + 
             # lag(my.data$pcr.pos.7dma, 5) + 
             # lag(my.data$pcr.pos.7dma, 6) + 
             # lag(my.data$pcr.pos.7dma, 7) + 
             # lag(my.data$pcr.pos.7dma, 8) + 
             # lag(my.data$pcr.pos.7dma, 9) + 
             # lag(my.data$pcr.pos.7dma, 10) + 
             # lag(my.data$pcr.pos.7dma, 11) + 
             # lag(my.data$pcr.pos.7dma, 12) + 
             # 
             # lag(my.data$pcr.pos.7dma, 13) + 
             # lag(my.data$pcr.pos.7dma, 14) + 
             # lag(my.data$pcr.pos.7dma, 15) + 
             # lag(my.data$pcr.pos.7dma, 16) + 
             # lag(my.data$pcr.pos.7dma, 17) + 
             # lag(my.data$pcr.pos.7dma, 18) + 
             # lag(my.data$pcr.pos.7dma, 19) + 
             # lag(my.data$pcr.pos.7dma, 20) + 
             # lag(my.data$pcr.pos.7dma, 21) + 
             # lag(my.data$pcr.pos.7dma, 22) + 
             # lag(my.data$pcr.pos.7dma, 23) + 
             # lag(my.data$pcr.pos.7dma, 24) +
             # 
             # lag(my.data$pcr.pos.7dma, 25) + 
             # lag(my.data$pcr.pos.7dma, 26) + 
             # lag(my.data$pcr.pos.7dma, 27) + 
             # lag(my.data$pcr.pos.7dma, 28) + 
             # lag(my.data$pcr.pos.7dma, 29) + 
             # lag(my.data$pcr.pos.7dma, 30) + 
             # lag(my.data$pcr.pos.7dma, 31) + 
             # lag(my.data$pcr.pos.7dma, 32) + 
             # lag(my.data$pcr.pos.7dma, 33) + 
             # lag(my.data$pcr.pos.7dma, 34) + 
             # lag(my.data$pcr.pos.7dma, 35) + 
             # lag(my.data$pcr.pos.7dma, 36) +
             # 
             # lag(my.data$pcr.pos.7dma, 37) + 
             # lag(my.data$pcr.pos.7dma, 38) + 
             # lag(my.data$pcr.pos.7dma, 39) + 
             # lag(my.data$pcr.pos.7dma, 40) + 
             # lag(my.data$pcr.pos.7dma, 41) + 
             # lag(my.data$pcr.pos.7dma, 42) + 
             # lag(my.data$pcr.pos.7dma, 43) + 
             # lag(my.data$pcr.pos.7dma, 44) + 
             # lag(my.data$pcr.pos.7dma, 45) + 
             # lag(my.data$pcr.pos.7dma, 46) + 
             # lag(my.data$pcr.pos.7dma, 47) + 
             # lag(my.data$pcr.pos.7dma, 48) + 
             # 
             # lag(my.data$pcr.pos.7dma, 49) + 
             # lag(my.data$pcr.pos.7dma, 50) + 
             # lag(my.data$pcr.pos.7dma, 51) + 
             # lag(my.data$pcr.pos.7dma, 52) + 
             # lag(my.data$pcr.pos.7dma, 53) + 
             # lag(my.data$pcr.pos.7dma, 54) + 
             # lag(my.data$pcr.pos.7dma, 55) + 
             # lag(my.data$pcr.pos.7dma, 56) + 
             # lag(my.data$pcr.pos.7dma, 57) + 
             # lag(my.data$pcr.pos.7dma, 58) + 
             # lag(my.data$pcr.pos.7dma, 59) + 
             # lag(my.data$pcr.pos.7dma, 60) +
             # 
             # lag(my.data$pcr.pos.7dma, 61) + 
             # lag(my.data$pcr.pos.7dma, 62) + 
             # lag(my.data$pcr.pos.7dma, 63) + 
             # lag(my.data$pcr.pos.7dma, 64) + 
             # lag(my.data$pcr.pos.7dma, 65) + 
             # lag(my.data$pcr.pos.7dma, 66) + 
             # lag(my.data$pcr.pos.7dma, 67) + 
             # lag(my.data$pcr.pos.7dma, 68) + 
             # lag(my.data$pcr.pos.7dma, 69) + 
             # lag(my.data$pcr.pos.7dma, 70) + 
             # lag(my.data$pcr.pos.7dma, 71) + 
             # lag(my.data$pcr.pos.7dma, 72) 
           
           + temp1 + temp2,
           
           family = Gamma(link = "log"))

summary(fit)



glm.preds <- predict(fit)


my.data$glm.preds <- NA

my.data$glm.preds[as.numeric(names(glm.preds))] <- glm.preds






plot(my.data$sample_date, my.data$pcr.pos.7dma, type = "l", ylim = c(0, 200))

lines(my.data$sample_date, exp(my.data$glm.preds), lty = 5, lwd = 1)








```



```{r eval = F, echo = F}

wbe.model <- wbe %>% 
              full_join(covid, by = c("sample_date"="date")) %>% 
              full_join(plant, by = c("sample_date"="date", "facility"="wrf")) %>% 
              mutate(bio.id = paste0(biological_replicate, "_", facility, "_", format(sample_date, "%y%m%d")), 
                     cpm = ifelse(is.na(copies_per_uL), 0, 1)) %>% 
              filter(sample_date %in% wbe$sample_date) %>% 
              mutate(cpm = relevel(factor(cpm), ref = "1")) %>%
              mutate(influent_flow_L = scale(influent_flow_L), 
                     influent_tss_mg_l = scale(influent_tss_mg_l))



plot(wbe.model$copies_per_uL~wbe.model$cases.symptom.onset.7dma)
boxplot(wbe.model$copies_per_uL~factor(wbe.model$facility)*factor(wbe.model$target))
plot(wbe.model$copies_per_uL~wbe.model$influent_tss_mg_l+wbe.model$influent_flow_L)



library(lme4)



fit1 <- glmer(cpm ~ (1|bio.id) + facility + influent_flow_L + influent_tss_mg_l + target + cases.symptom.onset.7dma, data = wbe.model, family = "binomial")

summary(fit1)
car::Anova(fit1, type = 3)


lag.mat <- cbind(lag(wbe.model$cases.symptom.onset.7dma, 1), lag(wbe.model$cases.symptom.onset.7dma, 2), lag(wbe.model$cases.symptom.onset.7dma, 3), lag(wbe.model$cases.symptom.onset.7dma, 4), lag(wbe.model$cases.symptom.onset.7dma, 5))

lag.mat2 <- cbind(lag(wbe.model$cases.symptom.onset.7dma, 1), lag(wbe.model$cases.symptom.onset.7dma, 2)*2, lag(wbe.model$cases.symptom.onset.7dma, 3)*3, lag(wbe.model$cases.symptom.onset.7dma, 4)*4, lag(wbe.model$cases.symptom.onset.7dma, 5)*5)


cases.lagged.sum <- rowSums(cbind(wbe.model$cases.symptom.onset.7dma, lag.mat))

cases.lagged.sum2 <- rowSums(lag.mat2)


fit1a <- glmer(cpm ~ (1|bio.id) + facility + cases.lagged.sum, data = wbe.model, family = "binomial")

summary(fit1a)
car::Anova(fit1a, type = 3)



fit2 <- glmer(copies_per_uL ~ (1|bio.id) + facility + influent_flow_L + influent_tss_mg_l + target + cases.lagged.sum[which(wbe.model$cpm=="1")] + cases.lagged.sum2[which(wbe.model$cpm=="1")], data = wbe.model[which(wbe.model$cpm=="1"),], family = Gamma(link = "log"))


summary(fit2)





wbe.missing.profile.model <- wbe.missing.profile %>% full_join(covid, by = c("sample_date"="date")) %>% full_join(plant, by = c("sample_date"="date"))


plot((1-wbe.missing.profile.model$p.missing[which(complete.cases(wbe.missing.profile.model$p.missing) & wbe.missing.profile.model$facility=="NO")])~wbe.missing.profile.model$cases.symptom.onset.7dma[which(complete.cases(wbe.missing.profile.model$p.missing) & wbe.missing.profile.model$facility=="NO")])
lines(smooth.spline((1-wbe.missing.profile.model$p.missing[which(complete.cases(wbe.missing.profile.model$p.missing) & wbe.missing.profile.model$facility=="NO")])~wbe.missing.profile.model$cases.symptom.onset.7dma[which(complete.cases(wbe.missing.profile.model$p.missing) & wbe.missing.profile.model$facility=="NO")], spar = 0.75))

points((1-wbe.missing.profile.model$p.missing[which(complete.cases(wbe.missing.profile.model$p.missing) & wbe.missing.profile.model$facility=="MI")])~wbe.missing.profile.model$cases.symptom.onset.7dma[which(complete.cases(wbe.missing.profile.model$p.missing) & wbe.missing.profile.model$facility=="MI")], pch = 16, fill = "red")
lines(smooth.spline((1-wbe.missing.profile.model$p.missing[which(complete.cases(wbe.missing.profile.model$p.missing) & wbe.missing.profile.model$facility=="MI")])~wbe.missing.profile.model$cases.symptom.onset.7dma[which(complete.cases(wbe.missing.profile.model$p.missing) & wbe.missing.profile.model$facility=="MI")], spar = 0.75), lty = 3, col = "red")



points((1-wbe.missing.profile.model$p.missing[which(complete.cases(wbe.missing.profile.model$p.missing) & wbe.missing.profile.model$facility=="CC")])~wbe.missing.profile.model$cases.symptom.onset.7dma[which(complete.cases(wbe.missing.profile.model$p.missing) & wbe.missing.profile.model$facility=="CC")], pch = 5, col = "blue")
lines(smooth.spline((1-wbe.missing.profile.model$p.missing[which(complete.cases(wbe.missing.profile.model$p.missing) & wbe.missing.profile.model$facility=="CC")])~wbe.missing.profile.model$cases.symptom.onset.7dma[which(complete.cases(wbe.missing.profile.model$p.missing) & wbe.missing.profile.model$facility=="CC")], spar = 0.75), lty = 5, col = "blue")


# athens pop
## 129025

# 75/129025*100000


plot(wbe.missing.profile.model$p.missing~wbe.missing.profile.model$pcr.pos.7dma, pch = as.numeric(factor(wbe.missing.profile$target)))


```



## Ramblings
### Concepts of Sensitivity

Similar to Bayesian Framework discussed by Kyle Curtis during the Wastewater-based Epidemiology Researchers Collaboration Network Webinar on 31 March 2021.

$$Pr(+Sample~|~+RT-qPCR) = \frac{Pr(+RT-qPCR~|~+Sample) \times Pr(+Sample)}{[Pr(+RT-qPCR~|~+Sample) \times Pr(+Sample)] + [Pr(+Rt-qPCR~|~-Sample) \times Pr(-Sample)]}$$


It is important that we consider the sources of these water samples and how heterogeneity in these sources may impact downstream analyses. The wastewater reclamation facilities, as sources of water samples, may differ in  at least two potentially meaningful ways: the facility itself (e.g., size, processing, infrastructure, ...) and the origin of the water / sewage that is processed. Admittedly, the demarcation between facility and water characteristics may be impertinent as they are likely interrelated (e.g., size may reflect volume totals of influent water / sewage). However, considerations may still be warranted for their potential impacts and artifacts on data.
  
Ultimately, what characteristics of the wastewater reclamation facilities and service areas ***could*** affect water samples and, subsequently, their analysis?

Well, before considering the differences among facilities, let us first explore the sampling of water / sewage. Ideally, the collected samples would be representative of the water / sewage as a whole. What could prevent a sample from being representative? Using sparse chemistry knowledge and definitions, the water / sewage is likely more a mixture than a solution and, as such, the mixture may not be homogeneous (i.e., well mixed). So, a small sample may not be representative of the whole. The samples are, however, composites from a 24-hour period which may alleviate this as well as concerns from a cross-sectional perspective. 

With these sample and procedural impacts highlighted, the potential differences among facilities could potentially exacerbate these differences. For example, if Facility A collects larger volumes of water / sewage than Facility B, then we may expect the samples from Facility A to be drawn from a more homogeneous (well-mixed) source (I'm picturing a river with a waterfall versus a creek). There may be additional sources of heterogeneity due to a facility itself (e.g., water processing and point at which samples are taken), but at this point it is more of a thought experiment than empirical. 

Perhaps more profound in their effects on sample heterogeneity, these facilities have different service areas. The facilities effectively divide the county in three with respect to the service boundaries and the sewer networks. It is not much of a stretch to assume that the service areas differ in served population size and characteristics. So, perhaps the water samples are representative of these "subpopulations" and data analysis should consider them explicitly. Furthermore, due to potential differences in landscapes and built environments of service areas, weather events such as rainfall may be important to consider as sources of "error" in measurement. 




